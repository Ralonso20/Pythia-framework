{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#pythia-framework","title":"Pythia Framework","text":"<p>Modern Python Worker Framework for All Types of Background Processing</p> <p>Pythia is a comprehensive, high-performance Python framework that simplifies creating scalable workers for message processing, background jobs, HTTP polling, and more. Built with production-grade features and proven performance benchmarks.</p>"},{"location":"#worker-types-supported","title":"\ud83d\udd04 Worker Types Supported","text":""},{"location":"#1-message-based-workers","title":"1. Message-Based Workers","text":"<ul> <li>Kafka, RabbitMQ, Redis - Event streaming and queuing</li> <li>High-throughput message processing with proven benchmarks</li> <li>Advanced routing, durability, and error handling patterns</li> </ul>"},{"location":"#2-background-job-workers","title":"2. Background Job Workers","text":"<ul> <li>Async task processing with priority queues</li> <li>Scheduled jobs and cron-like functionality</li> <li>Retry logic with exponential backoff</li> <li>Built-in job status tracking and management</li> </ul>"},{"location":"#3-http-based-workers","title":"3. HTTP-Based Workers","text":"<ul> <li>API Pollers - Fetch data from REST APIs periodically</li> <li>Webhook Processors - Handle incoming HTTP callbacks</li> <li>Circuit breakers and retry policies for resilience</li> </ul>"},{"location":"#4-database-workers","title":"4. Database Workers","text":"<ul> <li>Change Data Capture (CDC) - Monitor database changes in real-time</li> <li>Data synchronization - Cross-database replication and sync</li> <li>PostgreSQL &amp; MySQL support - Native CDC and sync capabilities</li> </ul>"},{"location":"#key-features","title":"\u26a1 Key Features","text":"<ul> <li>\ud83d\ude80 High Performance: Optimized for throughput with async/await support</li> <li>\ud83d\udd04 Multiple Patterns: Message brokers, job queues, HTTP polling, webhooks</li> <li>\ud83d\udcca Built-in Monitoring: Comprehensive metrics and observability</li> <li>\u2699\ufe0f Zero Configuration: Smart auto-detection with manual override</li> <li>\ud83d\udee1\ufe0f Production Ready: Battle-tested with comprehensive error handling</li> <li>\ud83e\uddea Fully Tested: Complete test suite with performance benchmarks</li> </ul>"},{"location":"#performance-highlights","title":"\ud83d\udcc8 Performance Highlights","text":"<p>Based on our comprehensive benchmarking:</p> Broker Throughput P95 Latency CPU Usage Redis 3,304 msg/s 0.6ms 4.2% Kafka 1,872 msg/s 2.0ms 9.0% RabbitMQ 1,292 msg/s 0.0ms 6.8%"},{"location":"#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"#message-processing-worker","title":"Message Processing Worker","text":"<pre><code>from pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\n\nclass EventProcessor(Worker):\n    async def process_message(self, message: Message):\n        print(f\"Processing event: {message.body}\")\n        return {\"status\": \"processed\", \"event_id\": message.message_id}\n\nconfig = WorkerConfig(broker_type=\"redis\")\nworker = EventProcessor(config=config)\n</code></pre>"},{"location":"#background-job-worker","title":"Background Job Worker","text":"<pre><code>from pythia.jobs import BackgroundJobWorker, JobProcessor, JobResult\n\nclass EmailProcessor(JobProcessor):\n    async def process(self, job):\n        # Send email logic\n        await send_email(job.kwargs['to'], job.kwargs['subject'])\n        return JobResult(success=True, result=\"Email sent\")\n\nworker = BackgroundJobWorker(processor=EmailProcessor())\nawait worker.submit_job(\"send_welcome\", kwargs={\"to\": \"user@example.com\"})\n</code></pre>"},{"location":"#http-polling-worker","title":"HTTP Polling Worker","text":"<pre><code>from pythia.http.poller import HTTPPoller\n\npoller = HTTPPoller(\n    url=\"https://api.example.com/events\",\n    interval=60,  # Poll every minute\n    method=\"GET\"\n)\n\nasync for message in poller.consume():\n    data = json.loads(message.body)\n    print(f\"Received {len(data)} events from API\")\n</code></pre>"},{"location":"#database-cdc-worker","title":"Database CDC Worker","text":"<pre><code>from pythia.brokers.database import CDCWorker, DatabaseChange\n\nclass UserChangeProcessor(CDCWorker):\n    async def process_change(self, change: DatabaseChange):\n        if change.table == \"users\":\n            await self.send_welcome_email(change.new_data)\n        return {\"processed\": True, \"change_id\": change.primary_key}\n\nworker = UserChangeProcessor(\n    connection_string=\"postgresql://user:pass@localhost/db\",\n    tables=[\"users\", \"orders\"],\n    poll_interval=5.0\n)\n</code></pre>"},{"location":"#http-workers","title":"HTTP Workers","text":"<pre><code>from pythia.brokers.http import PollerWorker, WebhookSenderWorker\n\n# API Polling Worker\nclass PaymentStatusPoller(PollerWorker):\n    def __init__(self):\n        super().__init__(\n            url=\"https://api.payments.com/status\",\n            interval=30,  # Poll every 30 seconds\n            headers={\"Authorization\": \"Bearer your-token\"}\n        )\n\n    async def process_message(self, message):\n        if message.body.get(\"status\") == \"completed\":\n            await self.handle_payment_completed(message.body)\n\n# Webhook Sender Worker\nclass NotificationSender(WebhookSenderWorker):\n    def __init__(self):\n        super().__init__(base_url=\"https://hooks.example.com\")\n\n    async def notify_user_created(self, user_data):\n        await self.send_webhook(\n            endpoint=\"/notifications\",\n            data={\"event\": \"user_created\", \"user\": user_data}\n        )\n</code></pre>"},{"location":"#documentation-structure","title":"\ud83d\udcd6 Documentation Structure","text":"<ul> <li>Getting Started - Installation and first steps</li> <li>User Guide - Core concepts and patterns</li> <li>Broker Integration - Specific broker configurations</li> <li>Performance - Optimization and scaling guides</li> <li>API Reference - Complete class and method documentation</li> <li>Examples - Real-world implementation examples</li> </ul>"},{"location":"#why-choose-pythia","title":"\ud83c\udfaf Why Choose Pythia?","text":"<ul> <li>Developer Experience: Intuitive API with comprehensive documentation</li> <li>Performance: Proven performance with detailed benchmarks</li> <li>Flexibility: Support multiple message brokers with unified interface</li> <li>Monitoring: Built-in metrics and observability out of the box</li> <li>Community: Open source with active development and support</li> </ul>     **Ready to build high-performance workers?**    [Get started](getting-started/installation.md) in less than 5 minutes!"},{"location":"api/brokers/","title":"Broker Interfaces","text":"<p>This reference covers message broker interfaces and implementations in Pythia.</p>"},{"location":"api/brokers/#base-interfaces","title":"Base Interfaces","text":""},{"location":"api/brokers/#messagebroker","title":"MessageBroker","text":"<p>::: pythia.brokers.base.MessageBroker     Abstract base class for all message consumers.</p> <pre><code>from pythia.brokers.base import MessageBroker\nfrom typing import AsyncIterator\nfrom pythia.core.message import Message\n\nclass CustomConsumer(MessageBroker):\n    async def connect(self) -&gt; None:\n        \"\"\"Establish connection to broker\"\"\"\n        pass\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Close connection to broker\"\"\"\n        pass\n\n    async def consume(self) -&gt; AsyncIterator[Message]:\n        \"\"\"Consume messages from broker\"\"\"\n        async for message in self._fetch_messages():\n            yield message\n\n    async def health_check(self) -&gt; bool:\n        \"\"\"Check broker health\"\"\"\n        return True\n</code></pre>"},{"location":"api/brokers/#messageproducer","title":"MessageProducer","text":"<p>::: pythia.brokers.base.MessageProducer     Abstract base class for all message producers.</p> <pre><code>from pythia.brokers.base import MessageProducer\nfrom pythia.core.message import Message\n\nclass CustomProducer(MessageProducer):\n    async def connect(self) -&gt; None:\n        \"\"\"Establish connection to broker\"\"\"\n        pass\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Close connection to broker\"\"\"\n        pass\n\n    async def send(self, message: Message) -&gt; None:\n        \"\"\"Send message to broker\"\"\"\n        await self._publish_message(message)\n\n    async def health_check(self) -&gt; bool:\n        \"\"\"Check broker health\"\"\"\n        return True\n</code></pre>"},{"location":"api/brokers/#kafka-brokers","title":"Kafka Brokers","text":""},{"location":"api/brokers/#kafkaconsumer","title":"KafkaConsumer","text":"<p>::: pythia.brokers.kafka.consumer.KafkaConsumer     Kafka message consumer implementation.</p> <pre><code>from pythia.brokers.kafka import KafkaConsumer, KafkaConfig\n\n# Basic usage\nconsumer = KafkaConsumer(\n    topic=\"orders\",\n    consumer_group=\"order-processors\",\n    bootstrap_servers=\"localhost:9092\"\n)\n\n# With configuration\nconfig = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    session_timeout_ms=30000,\n    heartbeat_interval_ms=3000,\n    auto_offset_reset=\"earliest\"\n)\n\nconsumer = KafkaConsumer(\n    topic=\"orders\",\n    consumer_group=\"processors\",\n    config=config\n)\n</code></pre>"},{"location":"api/brokers/#constructor-parameters","title":"Constructor Parameters","text":"<ul> <li>topic: str - Kafka topic to consume from</li> <li>consumer_group: str - Consumer group ID</li> <li>bootstrap_servers: str - Kafka broker addresses (optional)</li> <li>config: KafkaConfig - Configuration object (optional)</li> <li>batch_size: int - Messages per batch (default: 100)</li> <li>auto_offset_reset: str - Offset reset strategy</li> </ul>"},{"location":"api/brokers/#methods","title":"Methods","text":"<pre><code>async def consume(self) -&gt; AsyncIterator[Message]:\n    \"\"\"Consume messages from Kafka topic\"\"\"\n    pass\n\nasync def commit_offsets(self) -&gt; None:\n    \"\"\"Manually commit current offsets\"\"\"\n    pass\n\nasync def seek_to_beginning(self) -&gt; None:\n    \"\"\"Seek to beginning of topic\"\"\"\n    pass\n\nasync def seek_to_end(self) -&gt; None:\n    \"\"\"Seek to end of topic\"\"\"\n    pass\n</code></pre>"},{"location":"api/brokers/#kafkaproducer","title":"KafkaProducer","text":"<p>::: pythia.brokers.kafka.producer.KafkaProducer     Kafka message producer implementation.</p> <pre><code>from pythia.brokers.kafka import KafkaProducer, KafkaConfig\n\n# Basic usage\nproducer = KafkaProducer(\n    topic=\"processed-orders\",\n    bootstrap_servers=\"localhost:9092\"\n)\n\n# With partitioning\nproducer = KafkaProducer(\n    topic=\"events\",\n    partition_key=lambda msg: msg.get(\"user_id\"),\n    bootstrap_servers=\"localhost:9092\"\n)\n</code></pre>"},{"location":"api/brokers/#constructor-parameters_1","title":"Constructor Parameters","text":"<ul> <li>topic: str - Kafka topic to produce to</li> <li>bootstrap_servers: str - Kafka broker addresses (optional)</li> <li>config: KafkaConfig - Configuration object (optional)</li> <li>partition_key: callable - Function to determine partition</li> <li>key_serializer: callable - Key serialization function</li> <li>value_serializer: callable - Value serialization function</li> </ul>"},{"location":"api/brokers/#methods_1","title":"Methods","text":"<pre><code>async def send(self, message: Message) -&gt; None:\n    \"\"\"Send message to Kafka topic\"\"\"\n    pass\n\nasync def send_batch(self, messages: List[Message]) -&gt; None:\n    \"\"\"Send multiple messages\"\"\"\n    pass\n\nasync def flush(self) -&gt; None:\n    \"\"\"Flush pending messages\"\"\"\n    pass\n</code></pre>"},{"location":"api/brokers/#redis-brokers","title":"Redis Brokers","text":""},{"location":"api/brokers/#redisstreamsconsumer","title":"RedisStreamsConsumer","text":"<p>::: pythia.brokers.redis.streams.RedisStreamsConsumer     Redis Streams consumer implementation.</p> <pre><code>from pythia.brokers.redis import RedisStreamsConsumer, RedisConfig\n\n# Basic usage\nconsumer = RedisStreamsConsumer(\n    stream=\"orders\",\n    consumer_group=\"processors\",\n    consumer_name=\"worker-1\"\n)\n\n# With configuration\nconfig = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    password=\"secret\",\n    max_connections=10\n)\n\nconsumer = RedisStreamsConsumer(\n    stream=\"events\",\n    consumer_group=\"handlers\",\n    consumer_name=\"handler-1\",\n    config=config\n)\n</code></pre>"},{"location":"api/brokers/#constructor-parameters_2","title":"Constructor Parameters","text":"<ul> <li>stream: str - Redis stream name</li> <li>consumer_group: str - Consumer group name</li> <li>consumer_name: str - Unique consumer name</li> <li>config: RedisConfig - Configuration object (optional)</li> <li>batch_size: int - Messages per batch</li> <li>block_time: int - Blocking timeout in milliseconds</li> <li>start_id: str - Starting message ID (default: \"&gt;\")</li> </ul>"},{"location":"api/brokers/#redisstreamsproducer","title":"RedisStreamsProducer","text":"<p>::: pythia.brokers.redis.streams.RedisStreamsProducer     Redis Streams producer implementation.</p> <pre><code>from pythia.brokers.redis import RedisStreamsProducer\n\nproducer = RedisStreamsProducer(\n    stream=\"orders\",\n    max_length=10000,  # Trim stream to max length\n    approximate=True   # Approximate trimming for performance\n)\n</code></pre>"},{"location":"api/brokers/#redispubsubconsumer","title":"RedisPubSubConsumer","text":"<p>::: pythia.brokers.redis.pubsub.RedisPubSubConsumer     Redis Pub/Sub consumer implementation.</p> <pre><code>from pythia.brokers.redis import RedisPubSubConsumer\n\n# Subscribe to channels\nconsumer = RedisPubSubConsumer(\n    channels=[\"orders\", \"events\"],\n    config=redis_config\n)\n\n# Subscribe to patterns\nconsumer = RedisPubSubConsumer(\n    patterns=[\"user.*\", \"order.*\"],\n    config=redis_config\n)\n</code></pre>"},{"location":"api/brokers/#redislistconsumer","title":"RedisListConsumer","text":"<p>::: pythia.brokers.redis.lists.RedisListConsumer     Redis List consumer (BRPOP/BLPOP) implementation.</p> <pre><code>from pythia.brokers.redis import RedisListConsumer\n\nconsumer = RedisListConsumer(\n    queue=\"task_queue\",\n    timeout=10,  # Block timeout in seconds\n    config=redis_config\n)\n</code></pre>"},{"location":"api/brokers/#rabbitmq-brokers","title":"RabbitMQ Brokers","text":""},{"location":"api/brokers/#rabbitmqconsumer","title":"RabbitMQConsumer","text":"<p>::: pythia.brokers.rabbitmq.consumer.RabbitMQConsumer     RabbitMQ message consumer implementation.</p> <pre><code>from pythia.brokers.rabbitmq import RabbitMQConsumer, RabbitMQConfig\n\n# Basic queue consumer\nconsumer = RabbitMQConsumer(\n    queue=\"orders\",\n    host=\"localhost\",\n    port=5672\n)\n\n# With exchange and routing\nconsumer = RabbitMQConsumer(\n    queue=\"order_processing\",\n    exchange=\"orders\",\n    routing_key=\"new_order\",\n    exchange_type=\"topic\"\n)\n\n# With configuration\nconfig = RabbitMQConfig(\n    host=\"rabbitmq.example.com\",\n    port=5672,\n    username=\"user\",\n    password=\"pass\",\n    virtual_host=\"/\",\n    heartbeat=600\n)\n\nconsumer = RabbitMQConsumer(\n    queue=\"tasks\",\n    config=config,\n    prefetch_count=100,\n    auto_ack=False\n)\n</code></pre>"},{"location":"api/brokers/#constructor-parameters_3","title":"Constructor Parameters","text":"<ul> <li>queue: str - Queue name to consume from</li> <li>exchange: str - Exchange name (optional)</li> <li>routing_key: str - Routing key (optional)</li> <li>exchange_type: str - Exchange type (direct, topic, fanout, headers)</li> <li>config: RabbitMQConfig - Configuration object (optional)</li> <li>prefetch_count: int - Prefetch message count</li> <li>auto_ack: bool - Automatic acknowledgment</li> <li>durable: bool - Durable queue declaration</li> </ul>"},{"location":"api/brokers/#rabbitmqproducer","title":"RabbitMQProducer","text":"<p>::: pythia.brokers.rabbitmq.producer.RabbitMQProducer     RabbitMQ message producer implementation.</p> <pre><code>from pythia.brokers.rabbitmq import RabbitMQProducer\n\n# Basic producer\nproducer = RabbitMQProducer(\n    exchange=\"orders\",\n    routing_key=\"new_order\"\n)\n\n# Queue producer\nproducer = RabbitMQProducer(\n    queue=\"direct_queue\"\n)\n</code></pre>"},{"location":"api/brokers/#http-brokers","title":"HTTP Brokers","text":""},{"location":"api/brokers/#pollerworker-consumer","title":"PollerWorker (Consumer)","text":"<p>::: pythia.brokers.http.poller.PollerWorker     HTTP polling consumer implementation.</p> <pre><code>from pythia.brokers.http import PollerWorker\n\npoller = PollerWorker(\n    url=\"https://api.example.com/events\",\n    poll_interval=30.0,  # Poll every 30 seconds\n    headers={\"Authorization\": \"Bearer token\"},\n    params={\"limit\": 100},\n    method=\"GET\"\n)\n</code></pre>"},{"location":"api/brokers/#webhooksenderworker-producer","title":"WebhookSenderWorker (Producer)","text":"<p>::: pythia.brokers.http.webhook.WebhookSenderWorker     HTTP webhook producer implementation.</p> <pre><code>from pythia.brokers.http import WebhookSenderWorker\n\nwebhook = WebhookSenderWorker(\n    webhook_urls=[\n        \"https://api.partner1.com/webhooks\",\n        \"https://api.partner2.com/webhooks\"\n    ],\n    headers={\"Content-Type\": \"application/json\"},\n    timeout=10.0,\n    max_retries=3\n)\n</code></pre>"},{"location":"api/brokers/#cloud-brokers","title":"Cloud Brokers","text":""},{"location":"api/brokers/#aws-sqssns","title":"AWS SQS/SNS","text":"<pre><code>from pythia.brokers.cloud.aws import SQSConsumer, SNSProducer\n\n# SQS Consumer\nsqs_consumer = SQSConsumer(\n    queue_url=\"https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\",\n    max_messages=10,\n    wait_time_seconds=20,  # Long polling\n    visibility_timeout_seconds=30\n)\n\n# SNS Producer\nsns_producer = SNSProducer(\n    topic_arn=\"arn:aws:sns:us-east-1:123456789012:my-topic\",\n    message_attributes={\n        \"event_type\": {\"DataType\": \"String\", \"StringValue\": \"order\"}\n    }\n)\n</code></pre>"},{"location":"api/brokers/#google-cloud-pubsub","title":"Google Cloud Pub/Sub","text":"<pre><code>from pythia.brokers.cloud.gcp import PubSubSubscriber, PubSubPublisher\n\n# Pub/Sub Subscriber\nsubscriber = PubSubSubscriber(\n    subscription_name=\"projects/my-project/subscriptions/my-subscription\",\n    max_messages=100,\n    ack_deadline_seconds=600\n)\n\n# Pub/Sub Publisher\npublisher = PubSubPublisher(\n    topic_name=\"projects/my-project/topics/events\",\n    ordering_key=\"user_id\",  # Message ordering\n    attributes={\"source\": \"web_app\"}\n)\n</code></pre>"},{"location":"api/brokers/#azure-service-bus","title":"Azure Service Bus","text":"<pre><code>from pythia.brokers.cloud.azure import (\n    ServiceBusConsumer,\n    ServiceBusProducer,\n    StorageQueueConsumer,\n    StorageQueueProducer\n)\n\n# Service Bus Consumer\nsb_consumer = ServiceBusConsumer(\n    queue_name=\"orders\",\n    max_messages=32,\n    max_wait_time=5\n)\n\n# Storage Queue Consumer\nstorage_consumer = StorageQueueConsumer(\n    queue_name=\"background_tasks\",\n    visibility_timeout=30,\n    message_count=32\n)\n</code></pre>"},{"location":"api/brokers/#database-brokers","title":"Database Brokers","text":""},{"location":"api/brokers/#cdcworker-consumer","title":"CDCWorker (Consumer)","text":"<p>::: pythia.brokers.database.cdc.CDCWorker     Change Data Capture consumer implementation.</p> <pre><code>from pythia.brokers.database import CDCWorker\n\ncdc = CDCWorker(\n    db_url=\"postgresql://user:pass@localhost/db\",\n    tables=[\"users\", \"orders\", \"products\"],\n    poll_interval=5.0,\n    batch_size=100\n)\n</code></pre>"},{"location":"api/brokers/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/brokers/#kafkaconfig","title":"KafkaConfig","text":"<pre><code>from pythia.config.kafka import KafkaConfig\n\nconfig = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_username=\"user\",\n    sasl_password=\"pass\",\n\n    # Consumer settings\n    session_timeout_ms=30000,\n    heartbeat_interval_ms=3000,\n    max_poll_interval_ms=300000,\n    auto_offset_reset=\"earliest\",\n\n    # Producer settings\n    acks=\"all\",\n    retries=3,\n    batch_size=16384,\n    linger_ms=10,\n    buffer_memory=33554432\n)\n</code></pre>"},{"location":"api/brokers/#redisconfig","title":"RedisConfig","text":"<pre><code>from pythia.config.redis import RedisConfig\n\nconfig = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    password=\"secret\",\n    db=0,\n\n    # Connection pooling\n    max_connections=20,\n    retry_on_timeout=True,\n    socket_keepalive=True,\n\n    # Timeouts\n    socket_connect_timeout=5,\n    socket_timeout=5,\n    response_timeout=5\n)\n</code></pre>"},{"location":"api/brokers/#rabbitmqconfig","title":"RabbitMQConfig","text":"<pre><code>from pythia.config.rabbitmq import RabbitMQConfig\n\nconfig = RabbitMQConfig(\n    host=\"localhost\",\n    port=5672,\n    username=\"guest\",\n    password=\"guest\",\n    virtual_host=\"/\",\n\n    # Connection settings\n    heartbeat=600,\n    connection_attempts=3,\n    retry_delay=5,\n\n    # SSL settings\n    ssl_enabled=False,\n    ssl_certfile=\"/path/to/cert.pem\",\n    ssl_keyfile=\"/path/to/key.pem\",\n    ssl_ca_certs=\"/path/to/ca.pem\"\n)\n</code></pre>"},{"location":"api/brokers/#httpconfig","title":"HTTPConfig","text":"<pre><code>from pythia.config.http import HTTPConfig\n\nconfig = HTTPConfig(\n    timeout=30.0,\n    max_retries=3,\n    retry_delay=1.0,\n\n    # Connection pooling\n    max_connections=100,\n    max_keepalive_connections=20,\n    keepalive_expiry=5.0,\n\n    # Circuit breaker\n    enable_circuit_breaker=True,\n    circuit_breaker_threshold=5,\n    circuit_breaker_timeout=60.0\n)\n</code></pre>"},{"location":"api/brokers/#cloudconfig-awsgcpazure","title":"CloudConfig (AWS/GCP/Azure)","text":"<pre><code>from pythia.config.cloud import AWSConfig, GCPConfig, AzureConfig\n\n# AWS Configuration\naws_config = AWSConfig(\n    region=\"us-east-1\",\n    access_key_id=\"AKIAIOSFODNN7EXAMPLE\",\n    secret_access_key=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n    timeout=30.0,\n    max_retries=3\n)\n\n# GCP Configuration\ngcp_config = GCPConfig(\n    project_id=\"my-project\",\n    credentials_path=\"/path/to/service-account.json\",\n    timeout=30.0,\n    max_retries=3\n)\n\n# Azure Configuration\nazure_config = AzureConfig(\n    connection_string=\"Endpoint=sb://...\",\n    timeout=30.0,\n    max_retries=3\n)\n</code></pre>"},{"location":"api/brokers/#error-handling","title":"Error Handling","text":""},{"location":"api/brokers/#brokererror","title":"BrokerError","text":"<pre><code>from pythia.exceptions import BrokerError, ConnectionError, MessageError\n\ntry:\n    await consumer.connect()\nexcept ConnectionError as e:\n    logger.error(f\"Failed to connect: {e}\")\nexcept BrokerError as e:\n    logger.error(f\"Broker error: {e}\")\n</code></pre>"},{"location":"api/brokers/#retry-mechanisms","title":"Retry Mechanisms","text":"<pre><code>from pythia.utils.retry import RetryConfig\n\nretry_config = RetryConfig(\n    max_retries=3,\n    initial_delay=1.0,\n    max_delay=60.0,\n    exponential_backoff=True,\n    jitter=True\n)\n\n# Apply to broker\nconsumer = KafkaConsumer(\n    topic=\"orders\",\n    retry_config=retry_config\n)\n</code></pre>"},{"location":"api/core/","title":"Core Classes","text":"<p>This reference covers the core classes of the Pythia framework.</p>"},{"location":"api/core/#worker","title":"Worker","text":"<p>::: pythia.core.worker.Worker     The main base class for creating message processing workers.</p>"},{"location":"api/core/#constructor","title":"Constructor","text":"<pre><code>from pythia import Worker\nfrom pythia.config import WorkerConfig, MetricsConfig\n\nworker = Worker(\n    config=WorkerConfig(worker_name=\"my-worker\"),\n    metrics_config=MetricsConfig(enabled=True)\n)\n</code></pre>"},{"location":"api/core/#class-attributes","title":"Class Attributes","text":"<ul> <li>source: Optional[MessageBroker] - Single message source</li> <li>sink: Optional[MessageProducer] - Single message sink</li> <li>sources: Optional[List[MessageBroker]] - Multiple message sources</li> <li>sinks: Optional[List[MessageProducer]] - Multiple message sinks</li> </ul>"},{"location":"api/core/#key-methods","title":"Key Methods","text":""},{"location":"api/core/#abstract-methods-must-implement","title":"Abstract Methods (Must Implement)","text":"<pre><code>async def process(self, message: Message) -&gt; Any:\n    \"\"\"Process a single message\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#lifecycle-methods","title":"Lifecycle Methods","text":"<pre><code>async def startup(self) -&gt; None:\n    \"\"\"Called when worker starts\"\"\"\n    pass\n\nasync def shutdown(self) -&gt; None:\n    \"\"\"Called when worker shuts down\"\"\"\n    pass\n\nasync def health_check(self) -&gt; bool:\n    \"\"\"Check if worker is healthy\"\"\"\n    return True\n</code></pre>"},{"location":"api/core/#message-handling","title":"Message Handling","text":"<pre><code>async def send_to_sink(self, data: Any, sink_index: int = 0) -&gt; None:\n    \"\"\"Send data to specific sink\"\"\"\n    pass\n\nasync def broadcast(self, data: Any) -&gt; None:\n    \"\"\"Send data to all sinks\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#runtime-methods","title":"Runtime Methods","text":"<pre><code>def run(self) -&gt; None:\n    \"\"\"Run worker synchronously\"\"\"\n    pass\n\nasync def run_async(self) -&gt; None:\n    \"\"\"Run worker asynchronously\"\"\"\n    pass\n\ndef stop(self) -&gt; None:\n    \"\"\"Stop the worker\"\"\"\n    pass\n\ndef get_stats(self) -&gt; Dict[str, Any]:\n    \"\"\"Get worker statistics\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#example-usage","title":"Example Usage","text":"<pre><code>from pythia import Worker\nfrom pythia.brokers.kafka import KafkaConsumer, KafkaProducer\n\nclass OrderProcessor(Worker):\n    source = KafkaConsumer(topic=\"orders\")\n    sink = KafkaProducer(topic=\"processed-orders\")\n\n    async def process(self, message):\n        order_data = message.body\n        processed_order = await self.process_order(order_data)\n        await self.send_to_sink(processed_order)\n        return processed_order\n\n    async def process_order(self, order):\n        # Business logic here\n        return {\"id\": order[\"id\"], \"status\": \"processed\"}\n\n# Run the worker\nif __name__ == \"__main__\":\n    worker = OrderProcessor()\n    worker.run()\n</code></pre>"},{"location":"api/core/#batchworker","title":"BatchWorker","text":"<p>::: pythia.core.worker.BatchWorker     Worker for processing messages in batches.</p> <pre><code>from pythia import BatchWorker\nfrom pythia.brokers.kafka import KafkaConsumer\n\nclass BatchOrderProcessor(BatchWorker):\n    source = KafkaConsumer(topic=\"orders\")\n    batch_size = 100\n    batch_timeout = 5.0  # seconds\n\n    async def process_batch(self, messages: List[Message]) -&gt; List[Any]:\n        \"\"\"Process multiple messages at once\"\"\"\n        orders = [msg.body for msg in messages]\n        processed = await self.bulk_process_orders(orders)\n        return processed\n</code></pre>"},{"location":"api/core/#simpleworker","title":"SimpleWorker","text":"<p>::: pythia.core.worker.SimpleWorker     Simplified worker created from functions.</p> <pre><code>from pythia import simple_worker\nfrom pythia.brokers.kafka import KafkaConsumer\n\n@simple_worker(source=KafkaConsumer(topic=\"events\"))\nasync def event_handler(message):\n    \"\"\"Simple function-based worker\"\"\"\n    print(f\"Received event: {message.body}\")\n    return {\"processed\": True}\n\n# Run it\nif __name__ == \"__main__\":\n    event_handler.run()\n</code></pre>"},{"location":"api/core/#message","title":"Message","text":"<p>::: pythia.core.message.Message     Universal message abstraction for all brokers.</p>"},{"location":"api/core/#constructor_1","title":"Constructor","text":"<pre><code>from pythia.core.message import Message\nfrom datetime import datetime\n\nmessage = Message(\n    body={\"user_id\": 123, \"action\": \"login\"},\n    headers={\"source\": \"auth-service\"},\n    message_id=\"msg-123\",\n    timestamp=datetime.utcnow(),\n    source_info={\"topic\": \"user-events\", \"partition\": 0}\n)\n</code></pre>"},{"location":"api/core/#attributes","title":"Attributes","text":"<ul> <li>body: Any - Message payload (dict, string, bytes, etc.)</li> <li>headers: Dict[str, Any] - Message headers/metadata</li> <li>message_id: str - Unique message identifier</li> <li>timestamp: datetime - Message timestamp</li> <li>source_info: Dict[str, Any] - Broker-specific information</li> <li>retry_count: int - Number of processing retries</li> <li>max_retries: int - Maximum allowed retries</li> </ul>"},{"location":"api/core/#factory-methods","title":"Factory Methods","text":"<pre><code># Create from Kafka message\nkafka_message = Message.from_kafka(kafka_record)\n\n# Create from RabbitMQ message\nrabbitmq_message = Message.from_rabbitmq(rabbitmq_message)\n\n# Create from Redis message\nredis_message = Message.from_redis(redis_data, stream_name)\n</code></pre>"},{"location":"api/core/#methods","title":"Methods","text":"<pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert message to dictionary\"\"\"\n    pass\n\ndef should_retry(self) -&gt; bool:\n    \"\"\"Check if message should be retried\"\"\"\n    pass\n\ndef increment_retry(self) -&gt; None:\n    \"\"\"Increment retry counter\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#example-usage_1","title":"Example Usage","text":"<pre><code>from pythia.core.message import Message\n\n# Create a message\nmessage = Message(\n    body={\"order_id\": 12345, \"amount\": 99.99},\n    headers={\"priority\": \"high\", \"source\": \"web\"},\n    message_id=\"order-12345\"\n)\n\n# Check if retryable\nif message.should_retry():\n    message.increment_retry()\n    await retry_processing(message)\n\n# Convert to dict for serialization\nmessage_dict = message.to_dict()\n</code></pre>"},{"location":"api/core/#lifecyclemanager","title":"LifecycleManager","text":"<p>::: pythia.core.lifecycle.LifecycleManager     Manages worker startup, shutdown, and signal handling.</p>"},{"location":"api/core/#constructor_2","title":"Constructor","text":"<pre><code>from pythia.core.lifecycle import LifecycleManager\n\nlifecycle = LifecycleManager(\n    startup_timeout=30.0,\n    shutdown_timeout=30.0,\n    graceful_shutdown=True\n)\n</code></pre>"},{"location":"api/core/#methods_1","title":"Methods","text":"<pre><code>async def startup(self, components: List[Any]) -&gt; None:\n    \"\"\"Start all components\"\"\"\n    pass\n\nasync def shutdown(self, components: List[Any]) -&gt; None:\n    \"\"\"Shutdown all components\"\"\"\n    pass\n\ndef setup_signal_handlers(self, stop_callback: Callable) -&gt; None:\n    \"\"\"Setup SIGINT/SIGTERM handlers\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/#example-usage_2","title":"Example Usage","text":"<pre><code>from pythia.core.lifecycle import LifecycleManager\n\nclass MyWorker(Worker):\n    def __init__(self):\n        super().__init__()\n        self.lifecycle = LifecycleManager()\n        self.lifecycle.setup_signal_handlers(self.stop)\n\n    async def startup(self):\n        await self.lifecycle.startup([self.source, self.sink])\n\n    async def shutdown(self):\n        await self.lifecycle.shutdown([self.source, self.sink])\n</code></pre>"},{"location":"api/core/#workerrunner","title":"WorkerRunner","text":"<p>::: pythia.core.lifecycle.WorkerRunner     Utility for running workers with proper lifecycle management.</p> <pre><code>from pythia.core.lifecycle import WorkerRunner\n\n# Run worker with proper lifecycle\nrunner = WorkerRunner(worker_instance)\nawait runner.run()\n\n# Or run synchronously\nrunner.run_sync()\n</code></pre>"},{"location":"api/core/#configuration-classes","title":"Configuration Classes","text":""},{"location":"api/core/#workerconfig","title":"WorkerConfig","text":"<pre><code>from pythia.config import WorkerConfig\n\nconfig = WorkerConfig(\n    worker_name=\"order-processor\",\n    worker_id=\"worker-1\",\n    batch_size=100,\n    batch_timeout=5.0,\n    max_retries=3,\n    retry_delay=1.0,\n    log_level=\"INFO\",\n    enable_metrics=True,\n    health_check_interval=30.0,\n)\n</code></pre>"},{"location":"api/core/#metricsconfig","title":"MetricsConfig","text":"<pre><code>from pythia.monitoring import MetricsConfig\n\nmetrics_config = MetricsConfig(\n    enabled=True,\n    port=8000,\n    path=\"/metrics\",\n    push_gateway_url=\"http://prometheus-pushgateway:9091\",\n    job_name=\"pythia-worker\"\n)\n</code></pre>"},{"location":"api/core/#error-handling","title":"Error Handling","text":""},{"location":"api/core/#pythiaerror","title":"PythiaError","text":"<pre><code>from pythia.exceptions import PythiaError, WorkerError, BrokerError\n\ntry:\n    await worker.process(message)\nexcept WorkerError as e:\n    logger.error(f\"Worker error: {e}\")\nexcept BrokerError as e:\n    logger.error(f\"Broker error: {e}\")\nexcept PythiaError as e:\n    logger.error(f\"Framework error: {e}\")\n</code></pre>"},{"location":"api/core/#retry-decorators","title":"Retry Decorators","text":"<pre><code>from pythia.utils.retry import retry_on_failure\n\nclass MyWorker(Worker):\n    @retry_on_failure(max_retries=3, delay=1.0)\n    async def process(self, message):\n        # This method will auto-retry on failure\n        result = await self.risky_operation(message.body)\n        return result\n</code></pre>"},{"location":"api/core/#testing-utilities","title":"Testing Utilities","text":""},{"location":"api/core/#workertestcase","title":"WorkerTestCase","text":"<pre><code>from pythia.testing import WorkerTestCase\n\nclass TestMyWorker(WorkerTestCase):\n    def setUp(self):\n        self.worker = MyWorker()\n\n    async def test_message_processing(self):\n        message = self.create_message({\"test\": \"data\"})\n        result = await self.worker.process(message)\n        self.assertEqual(result[\"status\"], \"processed\")\n\n    def test_worker_health(self):\n        self.assertTrue(self.worker.health_check())\n</code></pre>"},{"location":"api/core/#mockbrokers","title":"MockBrokers","text":"<pre><code>from pythia.testing.mocks import MockConsumer, MockProducer\n\n# Use in tests\nmock_consumer = MockConsumer(messages=[\n    {\"id\": 1, \"data\": \"test1\"},\n    {\"id\": 2, \"data\": \"test2\"},\n])\n\nworker = MyWorker()\nworker.source = mock_consumer\n</code></pre>"},{"location":"api/workers/","title":"Worker Classes","text":"<p>This reference covers specialized worker classes in the Pythia framework.</p>"},{"location":"api/workers/#http-workers","title":"HTTP Workers","text":""},{"location":"api/workers/#httpworker","title":"HTTPWorker","text":"<p>::: pythia.brokers.http.HTTPWorker     Base class for HTTP-based workers.</p> <pre><code>from pythia.brokers.http import HTTPWorker\nimport aiohttp\n\nclass APIWorker(HTTPWorker):\n    def __init__(self, base_url: str, **kwargs):\n        super().__init__(**kwargs)\n        self.base_url = base_url\n        self.session = None\n\n    async def startup(self):\n        await super().startup()\n        self.session = aiohttp.ClientSession()\n\n    async def shutdown(self):\n        if self.session:\n            await self.session.close()\n        await super().shutdown()\n</code></pre>"},{"location":"api/workers/#configuration","title":"Configuration","text":"<pre><code>from pythia.config.http import HTTPConfig\n\nhttp_config = HTTPConfig(\n    timeout=30.0,\n    max_retries=3,\n    retry_delay=1.0,\n    max_connections=100,\n    enable_circuit_breaker=True,\n    circuit_breaker_threshold=5,\n    circuit_breaker_timeout=60.0,\n)\n</code></pre>"},{"location":"api/workers/#pollerworker","title":"PollerWorker","text":"<p>::: pythia.brokers.http.PollerWorker     Worker that polls HTTP endpoints for data.</p> <pre><code>from pythia.brokers.http import PollerWorker\n\nclass APIPollerWorker(PollerWorker):\n    def __init__(self, api_url: str, poll_interval: float = 30.0):\n        super().__init__(\n            url=api_url,\n            poll_interval=poll_interval,\n            headers={\"Accept\": \"application/json\"}\n        )\n\n    async def process(self, message):\n        # Process data from API\n        data = message.body\n        processed = await self.transform_data(data)\n        return processed\n\n    async def transform_data(self, data):\n        # Transform API response\n        return {\"processed\": True, \"count\": len(data)}\n</code></pre>"},{"location":"api/workers/#configuration-options","title":"Configuration Options","text":"<ul> <li>url: str - API endpoint to poll</li> <li>poll_interval: float - Seconds between polls</li> <li>headers: Dict[str, str] - HTTP headers</li> <li>params: Dict[str, Any] - Query parameters</li> <li>timeout: float - Request timeout</li> <li>method: str - HTTP method (default: GET)</li> </ul>"},{"location":"api/workers/#webhooksenderworker","title":"WebhookSenderWorker","text":"<p>::: pythia.brokers.http.WebhookSenderWorker     Worker that sends webhooks to external services.</p> <pre><code>from pythia.brokers.http import WebhookSenderWorker\nfrom pythia.brokers.kafka import KafkaConsumer\n\nclass OrderWebhookWorker(WebhookSenderWorker):\n    source = KafkaConsumer(topic=\"orders\")\n\n    def __init__(self):\n        super().__init__(\n            webhook_urls=[\n                \"https://api.partner1.com/webhooks\",\n                \"https://api.partner2.com/webhooks\"\n            ],\n            headers={\"Content-Type\": \"application/json\"},\n            timeout=10.0\n        )\n\n    async def process(self, message):\n        order = message.body\n        webhook_payload = {\n            \"event\": \"order.created\",\n            \"data\": order,\n            \"timestamp\": message.timestamp.isoformat()\n        }\n\n        # Send to all webhooks\n        await self.broadcast_webhook(webhook_payload)\n        return {\"webhook_sent\": True}\n</code></pre>"},{"location":"api/workers/#methods","title":"Methods","text":"<pre><code>async def send_webhook(self, data: Any, url: str = None) -&gt; Dict:\n    \"\"\"Send webhook to specific URL\"\"\"\n    pass\n\nasync def broadcast_webhook(self, data: Any) -&gt; List[Dict]:\n    \"\"\"Send webhook to all configured URLs\"\"\"\n    pass\n\nasync def send_webhook_from_message(self, message: Message) -&gt; Dict:\n    \"\"\"Send webhook from message data\"\"\"\n    pass\n</code></pre>"},{"location":"api/workers/#database-workers","title":"Database Workers","text":""},{"location":"api/workers/#databaseworker","title":"DatabaseWorker","text":"<p>::: pythia.brokers.database.DatabaseWorker     Base class for database-based workers.</p> <pre><code>from pythia.brokers.database import DatabaseWorker\nfrom sqlalchemy.ext.asyncio import create_async_engine\n\nclass DBWorker(DatabaseWorker):\n    def __init__(self, db_url: str):\n        super().__init__()\n        self.engine = create_async_engine(db_url)\n\n    async def startup(self):\n        await super().startup()\n        # Initialize database connection\n\n    async def shutdown(self):\n        await self.engine.dispose()\n        await super().shutdown()\n</code></pre>"},{"location":"api/workers/#cdcworker-change-data-capture","title":"CDCWorker (Change Data Capture)","text":"<p>::: pythia.brokers.database.CDCWorker     Worker for monitoring database changes.</p> <pre><code>from pythia.brokers.database import CDCWorker\nfrom pythia.brokers.kafka import KafkaProducer\n\nclass UserCDCWorker(CDCWorker):\n    sink = KafkaProducer(topic=\"user-changes\")\n\n    def __init__(self, db_url: str):\n        super().__init__(\n            db_url=db_url,\n            tables=[\"users\", \"user_profiles\"],\n            poll_interval=5.0\n        )\n\n    async def process(self, message):\n        change = message.body\n\n        # Transform database change to event\n        event = {\n            \"table\": change[\"table\"],\n            \"operation\": change[\"operation\"],  # INSERT, UPDATE, DELETE\n            \"old_values\": change.get(\"old_values\"),\n            \"new_values\": change.get(\"new_values\"),\n            \"timestamp\": change[\"timestamp\"]\n        }\n\n        await self.send_to_sink(event)\n        return event\n</code></pre>"},{"location":"api/workers/#configuration_1","title":"Configuration","text":"<pre><code>from pythia.config.database import DatabaseConfig\n\ndb_config = DatabaseConfig(\n    url=\"postgresql+asyncpg://user:pass@localhost/db\",\n    tables=[\"orders\", \"users\"],\n    poll_interval=10.0,\n    batch_size=100,\n    enable_snapshots=True\n)\n</code></pre>"},{"location":"api/workers/#syncworker","title":"SyncWorker","text":"<p>::: pythia.brokers.database.SyncWorker     Worker for synchronizing data between databases.</p> <pre><code>from pythia.brokers.database import SyncWorker\n\nclass DatabaseSyncWorker(SyncWorker):\n    def __init__(self, source_url: str, target_url: str):\n        super().__init__(\n            source_db_url=source_url,\n            target_db_url=target_url,\n            sync_tables=[\"products\", \"inventory\"],\n            sync_interval=3600.0  # 1 hour\n        )\n\n    async def process(self, message):\n        sync_data = message.body\n\n        # Apply changes to target database\n        await self.apply_changes(sync_data)\n\n        return {\"synced_records\": len(sync_data[\"changes\"])}\n\n    async def apply_changes(self, sync_data):\n        # Custom sync logic\n        for change in sync_data[\"changes\"]:\n            await self.apply_single_change(change)\n</code></pre>"},{"location":"api/workers/#background-job-workers","title":"Background Job Workers","text":""},{"location":"api/workers/#jobworker","title":"JobWorker","text":"<p>::: pythia.jobs.worker.JobWorker     Worker for processing background jobs.</p> <pre><code>from pythia.jobs import JobWorker, Job\nfrom pythia.brokers.redis import RedisListConsumer\n\nclass BackgroundJobWorker(JobWorker):\n    source = RedisListConsumer(queue=\"background_jobs\")\n\n    async def process(self, message):\n        job_data = message.body\n        job = Job.from_dict(job_data)\n\n        # Execute the job\n        result = await self.execute_job(job)\n\n        # Update job status\n        await self.update_job_status(job.id, \"completed\", result)\n\n        return result\n\n    async def execute_job(self, job: Job):\n        if job.type == \"send_email\":\n            return await self.send_email(job.payload)\n        elif job.type == \"process_image\":\n            return await self.process_image(job.payload)\n        else:\n            raise ValueError(f\"Unknown job type: {job.type}\")\n</code></pre>"},{"location":"api/workers/#scheduledjobworker","title":"ScheduledJobWorker","text":"<p>::: pythia.jobs.scheduler.ScheduledJobWorker     Worker for scheduled/cron jobs.</p> <pre><code>from pythia.jobs import ScheduledJobWorker, CronSchedule\n\nclass CronWorker(ScheduledJobWorker):\n    def __init__(self):\n        super().__init__()\n\n        # Add scheduled jobs\n        self.add_job(\n            func=self.daily_cleanup,\n            schedule=CronSchedule(hour=2, minute=0),  # 2 AM daily\n            job_id=\"daily_cleanup\"\n        )\n\n        self.add_job(\n            func=self.send_reports,\n            schedule=CronSchedule(day_of_week=1, hour=9),  # Monday 9 AM\n            job_id=\"weekly_reports\"\n        )\n\n    async def daily_cleanup(self):\n        \"\"\"Daily cleanup task\"\"\"\n        await self.cleanup_old_files()\n        await self.cleanup_temp_data()\n        return {\"cleaned\": True}\n\n    async def send_reports(self):\n        \"\"\"Weekly reports task\"\"\"\n        report = await self.generate_weekly_report()\n        await self.send_email_report(report)\n        return {\"report_sent\": True}\n</code></pre>"},{"location":"api/workers/#cloud-workers","title":"Cloud Workers","text":""},{"location":"api/workers/#aws-workers","title":"AWS Workers","text":"<pre><code>from pythia.brokers.cloud.aws import SQSConsumer, SNSProducer\n\nclass AWSWorker(Worker):\n    source = SQSConsumer(\n        queue_url=\"https://sqs.us-east-1.amazonaws.com/123456789012/my-queue\"\n    )\n    sink = SNSProducer(\n        topic_arn=\"arn:aws:sns:us-east-1:123456789012:my-topic\"\n    )\n\n    async def process(self, message):\n        # Process AWS SQS message\n        data = message.body\n        processed = await self.handle_aws_message(data)\n\n        # Send result to SNS\n        await self.send_to_sink(processed)\n        return processed\n</code></pre>"},{"location":"api/workers/#gcp-workers","title":"GCP Workers","text":"<pre><code>from pythia.brokers.cloud.gcp import PubSubSubscriber, PubSubPublisher\n\nclass GCPWorker(Worker):\n    source = PubSubSubscriber(\n        subscription_name=\"projects/my-project/subscriptions/my-subscription\"\n    )\n    sink = PubSubPublisher(\n        topic_name=\"projects/my-project/topics/processed-events\"\n    )\n\n    async def process(self, message):\n        # Process GCP Pub/Sub message\n        event_data = message.body\n        result = await self.process_event(event_data)\n\n        await self.send_to_sink(result)\n        return result\n</code></pre>"},{"location":"api/workers/#azure-workers","title":"Azure Workers","text":"<pre><code>from pythia.brokers.cloud.azure import ServiceBusConsumer, ServiceBusProducer\n\nclass AzureWorker(Worker):\n    source = ServiceBusConsumer(queue_name=\"input-queue\")\n    sink = ServiceBusProducer(queue_name=\"output-queue\")\n\n    async def process(self, message):\n        # Process Azure Service Bus message\n        data = message.body\n        processed = await self.transform_data(data)\n\n        await self.send_to_sink(processed)\n        return processed\n</code></pre>"},{"location":"api/workers/#specialized-patterns","title":"Specialized Patterns","text":""},{"location":"api/workers/#pipeline-worker","title":"Pipeline Worker","text":"<pre><code>from pythia import Worker\nfrom typing import List, Any\n\nclass PipelineWorker(Worker):\n    \"\"\"Worker that processes messages through multiple stages\"\"\"\n\n    def __init__(self, stages: List[callable], **kwargs):\n        super().__init__(**kwargs)\n        self.stages = stages\n\n    async def process(self, message):\n        data = message.body\n\n        # Process through each stage\n        for i, stage in enumerate(self.stages):\n            try:\n                data = await self.run_stage(stage, data, i)\n            except Exception as e:\n                self.logger.error(f\"Stage {i} failed: {e}\")\n                raise\n\n        return data\n\n    async def run_stage(self, stage: callable, data: Any, stage_num: int):\n        self.logger.info(f\"Running stage {stage_num}\")\n\n        if asyncio.iscoroutinefunction(stage):\n            return await stage(data)\n        else:\n            return stage(data)\n\n# Usage\ndef validate_data(data):\n    # Validation logic\n    return data\n\nasync def enrich_data(data):\n    # Enrichment logic\n    return data\n\ndef transform_data(data):\n    # Transformation logic\n    return data\n\npipeline = PipelineWorker(stages=[validate_data, enrich_data, transform_data])\n</code></pre>"},{"location":"api/workers/#multi-source-worker","title":"Multi-Source Worker","text":"<pre><code>from pythia import Worker\nfrom pythia.brokers.kafka import KafkaConsumer\nfrom pythia.brokers.redis import RedisStreamsConsumer\n\nclass MultiSourceWorker(Worker):\n    \"\"\"Worker that consumes from multiple sources\"\"\"\n\n    sources = [\n        KafkaConsumer(topic=\"orders\"),\n        RedisStreamsConsumer(stream=\"events\"),\n    ]\n\n    async def process(self, message):\n        # Handle message based on source\n        source_type = message.source_info.get(\"broker_type\")\n\n        if source_type == \"kafka\":\n            return await self.handle_kafka_message(message)\n        elif source_type == \"redis\":\n            return await self.handle_redis_message(message)\n        else:\n            return await self.handle_generic_message(message)\n</code></pre>"},{"location":"api/workers/#circuit-breaker-worker","title":"Circuit Breaker Worker","text":"<pre><code>from pythia import Worker\nfrom pythia.utils.circuit_breaker import CircuitBreaker\n\nclass CircuitBreakerWorker(Worker):\n    \"\"\"Worker with circuit breaker pattern\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.circuit_breaker = CircuitBreaker(\n            failure_threshold=5,\n            recovery_timeout=60,\n            expected_exception=Exception\n        )\n\n    async def process(self, message):\n        async with self.circuit_breaker:\n            return await self.risky_operation(message)\n\n    async def risky_operation(self, message):\n        # Operation that might fail\n        result = await self.external_api_call(message.body)\n        return result\n</code></pre>"},{"location":"brokers/cloud/","title":"Cloud Message Queues","text":"<p>Pythia's Cloud Workers provide complete support for the main cloud messaging platforms: AWS, Google Cloud, and Azure. Implemented following the anti-over-engineering principle, they offer consistent APIs and simple configuration.</p>"},{"location":"brokers/cloud/#key-features","title":"\ud83c\udf1f Key Features","text":"<ul> <li>\ud83d\udce6 Modular Installation: Install only the providers you need</li> <li>\ud83d\udd04 Unified API: Same interface for all cloud providers</li> <li>\u26a1 Simple Configuration: Standard environment variables</li> <li>\ud83d\udee1\ufe0f Error Handling: Platform-specific error management</li> <li>\ud83d\udd27 Lazy Loading: No unnecessary dependencies</li> </ul>"},{"location":"brokers/cloud/#installation","title":"\ud83d\ude80 Installation","text":"<pre><code># AWS only\nuv add pythia[aws]\n\n# Google Cloud only\nuv add pythia[gcp]\n\n# Azure only\nuv add pythia[azure]\n\n# All cloud providers\nuv add pythia[cloud]\n</code></pre>"},{"location":"brokers/cloud/#amazon-web-services-aws","title":"\u2601\ufe0f Amazon Web Services (AWS)","text":"<p>Complete support for Amazon SQS (Simple Queue Service) and Amazon SNS (Simple Notification Service).</p>"},{"location":"brokers/cloud/#sqs-consumer","title":"\ud83d\udce5 SQS Consumer","text":""},{"location":"brokers/cloud/#basic-example","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.aws import SQSConsumer\nfrom pythia.config.cloud import AWSConfig\n\nclass OrderProcessor(SQSConsumer):\n    def __init__(self):\n        aws_config = AWSConfig(\n            region=\"us-east-1\",\n            queue_url=\"https://sqs.us-east-1.amazonaws.com/123456789/orders\"\n        )\n        super().__init__(\n            queue_url=\"https://sqs.us-east-1.amazonaws.com/123456789/orders\",\n            aws_config=aws_config\n        )\n\n    async def process_message(self, message):\n        \"\"\"Process SQS message\"\"\"\n        order_data = message.body\n        order_id = order_data.get('order_id')\n\n        # Your business logic\n        print(f\"Processing order: {order_id}\")\n        await self.process_order(order_data)\n\n        return {\"processed\": True, \"order_id\": order_id}\n\n    async def process_order(self, order_data):\n        # Simulate processing\n        await asyncio.sleep(0.1)\n        print(f\"Order {order_data['order_id']} completed\")\n\n# Execute\nif __name__ == \"__main__\":\n    OrderProcessor().run_sync()\n</code></pre>"},{"location":"brokers/cloud/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from pythia.config.cloud import AWSConfig\n\nclass AdvancedSQSProcessor(SQSConsumer):\n    def __init__(self):\n        aws_config = AWSConfig(\n            region=\"us-west-2\",\n            access_key_id=\"AKIA...\",  # Or use environment variables\n            secret_access_key=\"...\",\n            endpoint_url=\"http://localhost:4566\",  # For LocalStack\n            queue_url=\"https://sqs.us-west-2.amazonaws.com/123/priority-orders\",\n            visibility_timeout=60,  # 1 minute\n            wait_time_seconds=20,   # Long polling\n            max_messages=5          # Batch size\n        )\n\n        super().__init__(\n            queue_url=aws_config.queue_url,\n            aws_config=aws_config\n        )\n\n    async def process_message(self, message):\n        # Access to SQS metadata\n        message_id = message.headers.get('MessageId')\n        receipt_handle = message.headers.get('ReceiptHandle')\n        delivery_count = message.headers.get('MD5OfBody')\n\n        # Custom attributes\n        priority = message.headers.get('attr_priority', 'normal')\n        source = message.headers.get('attr_source', 'unknown')\n\n        self.logger.info(\n            f\"Processing SQS message {message_id}\",\n            priority=priority,\n            source=source\n        )\n\n        return {\n            \"message_id\": message_id,\n            \"priority\": priority,\n            \"processed\": True\n        }\n</code></pre>"},{"location":"brokers/cloud/#sns-producer","title":"\ud83d\udce4 SNS Producer","text":""},{"location":"brokers/cloud/#basic-example_1","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.aws import SNSProducer\n\nclass NotificationSender(SNSProducer):\n    def __init__(self):\n        super().__init__(\n            topic_arn=\"arn:aws:sns:us-east-1:123456789:user-notifications\"\n        )\n\n    async def send_welcome_notification(self, user_data):\n        \"\"\"Send welcome notification\"\"\"\n        message_data = {\n            \"event\": \"user_welcome\",\n            \"user_id\": user_data[\"id\"],\n            \"email\": user_data[\"email\"],\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        message_id = await self.publish_message(\n            message=message_data,\n            subject=\"Welcome to our platform\",\n            message_attributes={\n                \"event_type\": \"welcome\",\n                \"priority\": \"high\",\n                \"user_segment\": user_data.get(\"segment\", \"standard\")\n            }\n        )\n\n        if message_id:\n            self.logger.info(f\"Welcome notification sent: {message_id}\")\n        return message_id\n\n    async def send_from_pythia_message(self, pythia_message):\n        \"\"\"Send from Pythia message\"\"\"\n        return await self.publish_from_pythia_message(\n            message=pythia_message,\n            subject=\"System Alert\"\n        )\n</code></pre>"},{"location":"brokers/cloud/#aws-environment-variables","title":"AWS Environment Variables","text":"<pre><code># AWS Credentials\nAWS_REGION=us-east-1\nAWS_ACCESS_KEY_ID=AKIA...\nAWS_SECRET_ACCESS_KEY=...\nAWS_ENDPOINT_URL=http://localhost:4566  # For LocalStack\n\n# SQS\nSQS_QUEUE_URL=https://sqs.us-east-1.amazonaws.com/123/queue-name\n\n# SNS\nSNS_TOPIC_ARN=arn:aws:sns:us-east-1:123:topic-name\n</code></pre>"},{"location":"brokers/cloud/#google-cloud-platform-gcp","title":"\ud83c\udfd7\ufe0f Google Cloud Platform (GCP)","text":"<p>Complete support for Google Cloud Pub/Sub with subscriber and publisher.</p>"},{"location":"brokers/cloud/#pubsub-subscriber","title":"\ud83d\udce5 Pub/Sub Subscriber","text":""},{"location":"brokers/cloud/#basic-example_2","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.gcp import PubSubSubscriber\nfrom pythia.config.cloud import GCPConfig\n\nclass EventProcessor(PubSubSubscriber):\n    def __init__(self):\n        gcp_config = GCPConfig(\n            project_id=\"my-gcp-project\",\n            subscription_name=\"event-processor-sub\"\n        )\n        super().__init__(\n            subscription_path=\"projects/my-gcp-project/subscriptions/event-processor-sub\",\n            gcp_config=gcp_config\n        )\n\n    async def process_message(self, message):\n        \"\"\"Process Pub/Sub message\"\"\"\n        event_data = message.body\n        event_type = event_data.get('type', 'unknown')\n\n        # Message metadata\n        message_id = message.headers.get('message_id')\n        publish_time = message.headers.get('publish_time')\n        ordering_key = message.headers.get('ordering_key')\n\n        self.logger.info(\n            f\"Processing Pub/Sub event: {event_type}\",\n            message_id=message_id,\n            ordering_key=ordering_key\n        )\n\n        # Routing by event type\n        if event_type == 'user_signup':\n            await self.handle_user_signup(event_data)\n        elif event_type == 'order_placed':\n            await self.handle_order_placed(event_data)\n        else:\n            self.logger.warning(f\"Unknown event type: {event_type}\")\n\n        return {\n            \"event_type\": event_type,\n            \"processed\": True,\n            \"message_id\": message_id\n        }\n\n    async def handle_user_signup(self, event_data):\n        # Specific logic for signup\n        user_id = event_data.get('user_id')\n        print(f\"New user signup: {user_id}\")\n\n    async def handle_order_placed(self, event_data):\n        # Specific logic for orders\n        order_id = event_data.get('order_id')\n        print(f\"New order placed: {order_id}\")\n</code></pre>"},{"location":"brokers/cloud/#automatic-project-id-extraction","title":"Automatic Project ID Extraction","text":"<pre><code># The project_id is automatically extracted from the path\nsubscriber = PubSubSubscriber(\n    subscription_path=\"projects/my-project/subscriptions/my-sub\"\n    # project_id is automatically detected as \"my-project\"\n)\n</code></pre>"},{"location":"brokers/cloud/#pubsub-publisher","title":"\ud83d\udce4 Pub/Sub Publisher","text":""},{"location":"brokers/cloud/#basic-example_3","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.gcp import PubSubPublisher\n\nclass EventPublisher(PubSubPublisher):\n    def __init__(self):\n        super().__init__(\n            topic_path=\"projects/my-gcp-project/topics/user-events\"\n        )\n\n    async def publish_user_event(self, user_id, event_type, event_data):\n        \"\"\"Publish user event with ordering\"\"\"\n        message_data = {\n            \"user_id\": user_id,\n            \"event_type\": event_type,\n            \"data\": event_data,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        message_id = await self.publish_message(\n            message=message_data,\n            attributes={\n                \"event_type\": event_type,\n                \"user_segment\": event_data.get(\"segment\", \"standard\"),\n                \"source\": \"user-service\",\n                \"version\": \"1.0\"\n            },\n            ordering_key=f\"user-{user_id}\"  # Guarantees order per user\n        )\n\n        if message_id:\n            self.logger.info(f\"User event published: {message_id}\")\n        return message_id\n\n    async def broadcast_system_alert(self, alert_data):\n        \"\"\"Broadcast without ordering key\"\"\"\n        return await self.publish_message(\n            message={\n                \"alert_type\": alert_data[\"type\"],\n                \"severity\": alert_data[\"severity\"],\n                \"message\": alert_data[\"message\"],\n                \"timestamp\": datetime.now().isoformat()\n            },\n            attributes={\n                \"alert_type\": alert_data[\"type\"],\n                \"severity\": alert_data[\"severity\"]\n            }\n            # No ordering_key for broadcast\n        )\n</code></pre>"},{"location":"brokers/cloud/#gcp-environment-variables","title":"GCP Environment Variables","text":"<pre><code># Authentication\nGCP_PROJECT_ID=my-gcp-project\nGOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n\n# Pub/Sub\nPUBSUB_SUBSCRIPTION=projects/my-project/subscriptions/my-subscription\nPUBSUB_TOPIC=projects/my-project/topics/my-topic\n</code></pre>"},{"location":"brokers/cloud/#microsoft-azure","title":"\ud83d\udd37 Microsoft Azure","text":"<p>Complete support for Azure Service Bus and Azure Storage Queues.</p>"},{"location":"brokers/cloud/#service-bus-consumer","title":"\ud83d\udce5 Service Bus Consumer","text":""},{"location":"brokers/cloud/#basic-example_4","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.azure import ServiceBusConsumer\nfrom pythia.config.cloud import AzureConfig\n\nclass TaskProcessor(ServiceBusConsumer):\n    def __init__(self):\n        azure_config = AzureConfig(\n            service_bus_connection_string=\"Endpoint=sb://my-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=...\",\n            service_bus_queue_name=\"task-queue\"\n        )\n        super().__init__(\n            queue_name=\"task-queue\",\n            azure_config=azure_config\n        )\n\n    async def process_message(self, message):\n        \"\"\"Process Service Bus message\"\"\"\n        task_data = message.body\n\n        # Message metadata\n        message_id = message.headers.get('message_id')\n        correlation_id = message.headers.get('correlation_id')\n        content_type = message.headers.get('content_type')\n        delivery_count = message.headers.get('delivery_count')\n\n        # Custom properties\n        priority = message.headers.get('prop_priority', 'normal')\n        task_type = message.headers.get('prop_task_type', 'generic')\n\n        self.logger.info(\n            f\"Processing Service Bus task: {task_type}\",\n            message_id=message_id,\n            correlation_id=correlation_id,\n            priority=priority,\n            delivery_count=delivery_count\n        )\n\n        # Type-based processing\n        if task_type == 'image_processing':\n            await self.process_image_task(task_data)\n        elif task_type == 'email_send':\n            await self.process_email_task(task_data)\n        else:\n            await self.process_generic_task(task_data)\n\n        return {\n            \"task_type\": task_type,\n            \"processed\": True,\n            \"correlation_id\": correlation_id\n        }\n\n    async def process_image_task(self, task_data):\n        # Specific logic for image processing\n        image_id = task_data.get('image_id')\n        print(f\"Processing image: {image_id}\")\n\n    async def process_email_task(self, task_data):\n        # Specific logic for email sending\n        email_id = task_data.get('email_id')\n        print(f\"Sending email: {email_id}\")\n\n    async def process_generic_task(self, task_data):\n        # Generic logic\n        print(f\"Processing generic task: {task_data}\")\n</code></pre>"},{"location":"brokers/cloud/#service-bus-producer","title":"\ud83d\udce4 Service Bus Producer","text":"<pre><code>from pythia.brokers.cloud.azure import ServiceBusProducer\n\nclass TaskScheduler(ServiceBusProducer):\n    def __init__(self):\n        super().__init__(\n            queue_name=\"task-queue\",\n            connection_string=\"Endpoint=sb://my-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=...\"\n        )\n\n    async def schedule_image_processing(self, image_data):\n        \"\"\"Schedule image processing\"\"\"\n        task_data = {\n            \"task_type\": \"image_processing\",\n            \"image_id\": image_data[\"id\"],\n            \"image_url\": image_data[\"url\"],\n            \"processing_options\": image_data.get(\"options\", {})\n        }\n\n        success = await self.send_message(\n            message=task_data,\n            properties={\n                \"task_type\": \"image_processing\",\n                \"priority\": \"high\",\n                \"source\": \"upload-service\"\n            },\n            correlation_id=f\"img-{image_data['id']}\",\n            content_type=\"application/json\"\n        )\n\n        if success:\n            self.logger.info(f\"Image processing task scheduled: {image_data['id']}\")\n        return success\n</code></pre>"},{"location":"brokers/cloud/#storage-queue-consumer","title":"\ud83d\udce5 Storage Queue Consumer","text":""},{"location":"brokers/cloud/#basic-example_5","title":"Basic Example","text":"<pre><code>from pythia.brokers.cloud.azure import StorageQueueConsumer\n\nclass SimpleTaskProcessor(StorageQueueConsumer):\n    def __init__(self):\n        super().__init__(\n            queue_name=\"simple-tasks\",\n            connection_string=\"DefaultEndpointsProtocol=https;AccountName=mystorage;AccountKey=...;EndpointSuffix=core.windows.net\"\n        )\n\n    async def process_message(self, message):\n        \"\"\"Process Storage Queue message\"\"\"\n        task_data = message.body\n\n        # Message metadata\n        message_id = message.headers.get('message_id')\n        dequeue_count = message.headers.get('dequeue_count')\n        insertion_time = message.headers.get('insertion_time')\n\n        self.logger.info(\n            f\"Processing Storage Queue task\",\n            message_id=message_id,\n            dequeue_count=dequeue_count,\n            insertion_time=insertion_time\n        )\n\n        # Process simple task\n        task_type = task_data.get('type', 'unknown')\n        if task_type == 'cleanup':\n            await self.perform_cleanup(task_data)\n        elif task_type == 'backup':\n            await self.perform_backup(task_data)\n\n        return {\n            \"task_type\": task_type,\n            \"processed\": True,\n            \"message_id\": message_id\n        }\n\n    async def perform_cleanup(self, task_data):\n        # Cleanup logic\n        print(f\"Performing cleanup: {task_data}\")\n\n    async def perform_backup(self, task_data):\n        # Backup logic\n        print(f\"Performing backup: {task_data}\")\n</code></pre>"},{"location":"brokers/cloud/#storage-queue-producer","title":"\ud83d\udce4 Storage Queue Producer","text":"<pre><code>from pythia.brokers.cloud.azure import StorageQueueProducer\n\nclass MaintenanceScheduler(StorageQueueProducer):\n    def __init__(self):\n        super().__init__(\n            queue_name=\"maintenance-tasks\",\n            connection_string=\"DefaultEndpointsProtocol=https;AccountName=mystorage;AccountKey=...;EndpointSuffix=core.windows.net\"\n        )\n\n    async def schedule_cleanup_task(self, cleanup_data):\n        \"\"\"Schedule cleanup task\"\"\"\n        task_data = {\n            \"type\": \"cleanup\",\n            \"target\": cleanup_data[\"target\"],\n            \"options\": cleanup_data.get(\"options\", {}),\n            \"scheduled_time\": datetime.now().isoformat()\n        }\n\n        message_id = await self.send_message(\n            message=task_data,\n            visibility_timeout=300,  # 5 minutes invisible initially\n            time_to_live=86400      # TTL of 24 hours\n        )\n\n        if message_id:\n            self.logger.info(f\"Cleanup task scheduled: {message_id}\")\n        return message_id\n\n    async def schedule_backup_task(self, backup_data):\n        \"\"\"Schedule backup task\"\"\"\n        return await self.send_message(\n            message={\n                \"type\": \"backup\",\n                \"database\": backup_data[\"database\"],\n                \"backup_type\": backup_data.get(\"type\", \"full\")\n            },\n            time_to_live=172800  # TTL of 48 hours for backups\n        )\n</code></pre>"},{"location":"brokers/cloud/#azure-environment-variables","title":"Azure Environment Variables","text":"<pre><code># Service Bus\nAZURE_SERVICE_BUS_CONNECTION_STRING=Endpoint=sb://my-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=...\nSERVICE_BUS_QUEUE_NAME=task-queue\n\n# Storage Queue\nAZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;AccountName=mystorage;AccountKey=...;EndpointSuffix=core.windows.net\nSTORAGE_QUEUE_NAME=simple-tasks\n</code></pre>"},{"location":"brokers/cloud/#advanced-configuration_1","title":"\ud83d\udd27 Advanced Configuration","text":""},{"location":"brokers/cloud/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from pythia.config.cloud import AWSConfig, GCPConfig, AzureConfig, CloudProviderConfig\n\n# Unified configuration\ncloud_config = CloudProviderConfig.from_env()\n\n# Provider-specific configuration\naws_config = AWSConfig(\n    region=\"us-east-1\",\n    queue_url=\"https://sqs.us-east-1.amazonaws.com/123/queue\",\n    visibility_timeout=60,\n    max_messages=10\n)\n\ngcp_config = GCPConfig(\n    project_id=\"my-project\",\n    subscription_name=\"my-subscription\",\n    max_messages=100,\n    ack_deadline_seconds=600\n)\n\nazure_config = AzureConfig(\n    service_bus_connection_string=\"Endpoint=sb://...\",\n    service_bus_queue_name=\"my-queue\",\n    max_messages=32,\n    visibility_timeout=30\n)\n</code></pre>"},{"location":"brokers/cloud/#multi-cloud-worker-pattern","title":"Multi-Cloud Worker Pattern","text":"<pre><code>from pythia.brokers.cloud.aws import SQSConsumer\nfrom pythia.brokers.cloud.gcp import PubSubSubscriber\nfrom pythia.brokers.cloud.azure import ServiceBusConsumer\n\nclass MultiCloudProcessor:\n    def __init__(self):\n        # Configure multiple consumers\n        self.aws_consumer = SQSConsumer(\n            queue_url=\"https://sqs.us-east-1.amazonaws.com/123/aws-tasks\"\n        )\n        self.gcp_subscriber = PubSubSubscriber(\n            subscription_path=\"projects/my-project/subscriptions/gcp-tasks\"\n        )\n        self.azure_consumer = ServiceBusConsumer(\n            queue_name=\"azure-tasks\",\n            connection_string=\"Endpoint=sb://...\"\n        )\n\n    async def run_all(self):\n        \"\"\"Run all consumers concurrently\"\"\"\n        await asyncio.gather(\n            self.aws_consumer.run(),\n            self.gcp_subscriber.run(),\n            self.azure_consumer.run()\n        )\n</code></pre>"},{"location":"brokers/cloud/#error-handling-and-retries","title":"\ud83d\udee1\ufe0f Error Handling and Retries","text":""},{"location":"brokers/cloud/#aws-sqs-visibility-timeout","title":"AWS SQS - Visibility Timeout","text":"<pre><code>class ResilientSQSProcessor(SQSConsumer):\n    async def process_message(self, message):\n        try:\n            # Your logic here\n            await self.complex_processing(message.body)\n            return {\"processed\": True}\n        except TemporaryError as e:\n            # Don't return result -&gt; message will be visible again\n            self.logger.warning(f\"Temporary error, will retry: {e}\")\n            return None  # This causes the message to be retried\n        except PermanentError as e:\n            # Return False to avoid infinite retries\n            self.logger.error(f\"Permanent error: {e}\")\n            await self.send_to_dlq(message)\n            return {\"processed\": False, \"error\": str(e)}\n</code></pre>"},{"location":"brokers/cloud/#gcp-pubsub-acknowledgment-control","title":"GCP Pub/Sub - Acknowledgment Control","text":"<pre><code>class ResilientPubSubProcessor(PubSubSubscriber):\n    async def process_message(self, message):\n        try:\n            await self.process_event(message.body)\n            return {\"processed\": True}  # Message will be acknowledged\n        except Exception as e:\n            self.logger.error(f\"Processing failed: {e}\")\n            # Don't return anything -&gt; message will NOT be acknowledged\n            # Will be retried automatically\n</code></pre>"},{"location":"brokers/cloud/#azure-service-bus-complete-vs-abandon","title":"Azure Service Bus - Complete vs Abandon","text":"<pre><code>class ResilientServiceBusProcessor(ServiceBusConsumer):\n    async def process_message(self, message):\n        try:\n            result = await self.process_task(message.body)\n            return result  # Complete message\n        except RetriableError as e:\n            # The worker automatically does abandon_message\n            # when there's an exception\n            raise  # This causes abandon and retry\n        except NonRetriableError as e:\n            # Return result to complete and avoid retries\n            await self.handle_poison_message(message)\n            return {\"processed\": False, \"error\": \"poison_message\"}\n</code></pre>"},{"location":"brokers/cloud/#monitoring-and-observability","title":"\ud83d\udcca Monitoring and Observability","text":""},{"location":"brokers/cloud/#structured-logging","title":"Structured Logging","text":"<pre><code>class MonitoredCloudProcessor(SQSConsumer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.metrics = {\n            \"messages_processed\": 0,\n            \"errors_count\": 0,\n            \"processing_times\": []\n        }\n\n    async def process_message(self, message):\n        start_time = time.time()\n\n        try:\n            # Your processing\n            result = await self.handle_message(message.body)\n\n            # Success metrics\n            processing_time = time.time() - start_time\n            self.metrics[\"messages_processed\"] += 1\n            self.metrics[\"processing_times\"].append(processing_time)\n\n            self.logger.info(\n                \"Message processed successfully\",\n                message_id=message.headers.get('MessageId'),\n                processing_time_ms=processing_time * 1000,\n                queue_name=self.queue_name,\n                metrics=self.metrics\n            )\n\n            return result\n\n        except Exception as e:\n            self.metrics[\"errors_count\"] += 1\n            self.logger.error(\n                \"Message processing failed\",\n                message_id=message.headers.get('MessageId'),\n                error=str(e),\n                error_type=type(e).__name__,\n                queue_name=self.queue_name,\n                metrics=self.metrics,\n                exc_info=True\n            )\n            raise\n</code></pre>"},{"location":"brokers/cloud/#common-use-cases","title":"\ud83c\udfaf Common Use Cases","text":""},{"location":"brokers/cloud/#1-image-processing-pipeline","title":"1. Image Processing Pipeline","text":"<pre><code># AWS SQS to receive tasks\nclass ImageProcessor(SQSConsumer):\n    async def process_message(self, message):\n        image_data = message.body\n\n        # Process image\n        processed_image = await self.process_image(image_data)\n\n        # Notify completion via SNS\n        notification_sender = SNSProducer(topic_arn=\"arn:aws:sns:...\")\n        await notification_sender.publish_message({\n            \"event\": \"image_processed\",\n            \"image_id\": image_data[\"id\"],\n            \"result_url\": processed_image[\"url\"]\n        })\n\n        return {\"processed\": True}\n</code></pre>"},{"location":"brokers/cloud/#2-multi-channel-notification-system","title":"2. Multi-Channel Notification System","text":"<pre><code># GCP Pub/Sub for user events\nclass NotificationDispatcher(PubSubSubscriber):\n    async def process_message(self, message):\n        event = message.body\n        user_id = event[\"user_id\"]\n\n        # Send to Azure Service Bus for processing\n        service_bus_producer = ServiceBusProducer(queue_name=\"notifications\")\n        await service_bus_producer.send_message(\n            message={\n                \"user_id\": user_id,\n                \"notification_type\": event[\"type\"],\n                \"channels\": [\"email\", \"push\", \"sms\"]\n            },\n            correlation_id=f\"user-{user_id}\"\n        )\n\n        return {\"dispatched\": True}\n</code></pre>"},{"location":"brokers/cloud/#3-backup-and-maintenance-system","title":"3. Backup and Maintenance System","text":"<pre><code># Azure Storage Queue for maintenance tasks\nclass MaintenanceWorker(StorageQueueConsumer):\n    async def process_message(self, message):\n        task = message.body\n\n        if task[\"type\"] == \"backup\":\n            await self.perform_backup(task[\"database\"])\n        elif task[\"type\"] == \"cleanup\":\n            await self.cleanup_old_files(task[\"path\"])\n        elif task[\"type\"] == \"health_check\":\n            health_status = await self.check_system_health()\n\n            # Report to AWS SNS if there are issues\n            if not health_status[\"healthy\"]:\n                sns_producer = SNSProducer(topic_arn=\"arn:aws:sns:...\")\n                await sns_producer.publish_message(\n                    message=health_status,\n                    subject=\"System Health Alert\"\n                )\n\n        return {\"completed\": True}\n</code></pre> <p>Pythia's Cloud Workers provide a unified and powerful experience for working with the main cloud messaging platforms, maintaining the simplicity and elegance that characterizes the framework.</p> <p>Start building your cloud-native workers today! \ud83d\ude80</p>"},{"location":"brokers/database/","title":"Database Workers","text":"<p>Database workers in Pythia provide simple yet powerful capabilities for monitoring database changes and synchronizing data using SQLAlchemy.</p>"},{"location":"brokers/database/#overview","title":"Overview","text":"<p>Pythia supports two main types of database workers:</p> <ol> <li>CDC (Change Data Capture) Workers - Monitor database changes using polling</li> <li>Sync Workers - Synchronize data between databases</li> </ol> <p>All database workers use SQLAlchemy for database abstraction, supporting PostgreSQL, MySQL, SQLite, and other SQLAlchemy-compatible databases.</p>"},{"location":"brokers/database/#change-data-capture-cdc-workers","title":"Change Data Capture (CDC) Workers","text":"<p>CDC workers monitor database changes by polling tables for new or updated records based on timestamp columns.</p>"},{"location":"brokers/database/#basic-cdc-worker","title":"Basic CDC Worker","text":"<pre><code>from pythia.brokers.database import CDCWorker, DatabaseChange\n\nclass OrderProcessor(CDCWorker):\n    def __init__(self):\n        super().__init__(\n            connection_string=\"postgresql://user:pass@localhost:5432/ecommerce\",\n            tables=[\"orders\", \"order_items\"],\n            poll_interval=5.0,  # Check every 5 seconds\n            timestamp_column=\"updated_at\"\n        )\n\n    async def process_change(self, change: DatabaseChange):\n        if change.table == \"orders\":\n            # Process order changes\n            await self.send_order_confirmation(change.new_data)\n            await self.update_inventory(change.new_data)\n\n        return {\"processed\": True, \"order_id\": change.primary_key}\n\n# Usage\nworker = OrderProcessor()\nasync with worker:\n    await worker.start_cdc()\n\n    # Process changes as they come\n    async for change in worker.consume_changes():\n        result = await worker.process_change(change)\n        print(f\"Processed: {result}\")\n</code></pre>"},{"location":"brokers/database/#multi-database-cdc-worker","title":"Multi-Database CDC Worker","text":"<pre><code># Works with any SQLAlchemy-supported database\nclass UserActivityProcessor(CDCWorker):\n    def __init__(self):\n        super().__init__(\n            connection_string=\"mysql://user:pass@localhost:3306/analytics\",\n            tables=[\"user_events\", \"user_sessions\"],\n            poll_interval=2.0,\n            timestamp_column=\"created_at\"\n        )\n\n    async def process_change(self, change: DatabaseChange):\n        # Real-time analytics processing\n        await self.update_user_metrics(change.new_data)\n        return {\"event_processed\": True}\n</code></pre>"},{"location":"brokers/database/#database-sync-workers","title":"Database Sync Workers","text":"<p>Sync workers synchronize data between different database systems.</p>"},{"location":"brokers/database/#basic-sync-worker","title":"Basic Sync Worker","text":"<pre><code>from pythia.brokers.database import SyncWorker\n\nclass DataReplicationWorker(SyncWorker):\n    def __init__(self):\n        sync_config = {\n            'batch_size': 1000,\n            'mode': 'incremental',  # or 'full'\n            'conflict_resolution': 'source_wins',\n            'timestamp_column': 'updated_at'\n        }\n\n        super().__init__(\n            source_connection=\"postgresql://user:pass@source:5432/prod_db\",\n            target_connection=\"mysql://user:pass@target:3306/analytics_db\",\n            sync_config=sync_config\n        )\n\n# Sync specific tables\nworker = DataReplicationWorker()\nasync with worker:\n    result = await worker.sync_table(\"users\")\n    print(f\"Synced {result['rows_synced']} rows\")\n\n    # Validate sync\n    validation = await worker.validate_sync(\"users\")\n    if validation['is_valid']:\n        print(\"\u2713 Sync validated successfully\")\n</code></pre>"},{"location":"brokers/database/#cross-database-sync","title":"Cross-Database Sync","text":"<pre><code># PostgreSQL \u2192 MySQL sync\nasync def postgres_to_mysql_sync():\n    worker = SyncWorker(\n        source_connection=\"postgresql://user:pass@pg-host:5432/source\",\n        target_connection=\"mysql://user:pass@mysql-host:3306/target\"\n    )\n\n    async with worker:\n        # Sync specific tables\n        result = await worker.sync_table(\"users\")\n        print(f\"Synced {result['rows_synced']} rows\")\n\n# MySQL \u2192 PostgreSQL sync\nasync def mysql_to_postgres_sync():\n    worker = SyncWorker(\n        source_connection=\"mysql://user:pass@mysql-host:3306/source\",\n        target_connection=\"postgresql://user:pass@pg-host:5432/target\"\n    )\n\n    async with worker:\n        # Sync and validate\n        result = await worker.sync_table(\"products\")\n        validation = await worker.validate_sync(\"products\")\n        print(f\"Sync valid: {validation['is_valid']}\")\n</code></pre>"},{"location":"brokers/database/#requirements","title":"Requirements","text":""},{"location":"brokers/database/#python-dependencies","title":"Python Dependencies","text":"<pre><code># Core requirement\npip install sqlalchemy[asyncio]\n\n# Database-specific drivers (choose what you need)\npip install asyncpg          # PostgreSQL\npip install aiomysql         # MySQL\npip install aiosqlite        # SQLite\n</code></pre>"},{"location":"brokers/database/#database-setup","title":"Database Setup","text":"<p>All database workers require:</p> <ol> <li> <p>Timestamp columns on tables you want to monitor:    <pre><code>-- PostgreSQL\nALTER TABLE your_table ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP;\n\n-- MySQL\nALTER TABLE your_table ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;\n</code></pre></p> </li> <li> <p>Read permissions for CDC workers:    <pre><code>GRANT SELECT ON database.* TO 'cdc_user'@'%';\n</code></pre></p> </li> <li> <p>Write permissions for sync workers:    <pre><code>GRANT SELECT, INSERT, UPDATE, DELETE ON target_database.* TO 'sync_user'@'%';\n</code></pre></p> </li> </ol>"},{"location":"brokers/database/#configuration-options","title":"Configuration Options","text":""},{"location":"brokers/database/#cdc-configuration","title":"CDC Configuration","text":"<pre><code># Basic CDC configuration\nworker = CDCWorker(\n    connection_string=\"postgresql://...\",  # Any SQLAlchemy URL\n    tables=[\"table1\", \"table2\"],           # Tables to monitor\n    poll_interval=5.0,                     # Check every 5 seconds\n    timestamp_column=\"updated_at\"          # Column to track changes\n)\n</code></pre>"},{"location":"brokers/database/#sync-configuration","title":"Sync Configuration","text":"<pre><code>sync_config = {\n    'batch_size': 1000,                    # Rows per batch\n    'mode': 'incremental',                 # 'full' or 'incremental'\n    'conflict_resolution': 'source_wins',  # How to handle conflicts\n    'timestamp_column': 'updated_at',      # Column for incremental sync\n    'truncate_target': False               # Truncate before full sync\n}\n</code></pre>"},{"location":"brokers/database/#error-handling-and-monitoring","title":"Error Handling and Monitoring","text":""},{"location":"brokers/database/#built-in-error-handling","title":"Built-in Error Handling","text":"<pre><code>class RobustCDCWorker(PostgreSQLCDCWorker):\n    async def process_change(self, change: DatabaseChange):\n        try:\n            # Your processing logic\n            result = await self.process_business_logic(change)\n            return result\n        except Exception as e:\n            # Log error but don't stop worker\n            self.logger.error(f\"Error processing change: {e}\")\n            # Return error info for monitoring\n            return {\"error\": str(e), \"change_id\": change.primary_key}\n</code></pre>"},{"location":"brokers/database/#monitoring-and-metrics","title":"Monitoring and Metrics","text":"<pre><code># CDC monitoring\nasync with worker:\n    await worker.start_cdc()\n\n    change_count = 0\n    error_count = 0\n\n    async for change in worker.consume_changes():\n        try:\n            result = await worker.process_change(change)\n            change_count += 1\n\n            if change_count % 1000 == 0:\n                self.logger.info(f\"Processed {change_count} changes\")\n\n        except Exception as e:\n            error_count += 1\n            self.logger.error(f\"Processing error: {e}\")\n</code></pre>"},{"location":"brokers/database/#use-cases","title":"Use Cases","text":""},{"location":"brokers/database/#real-time-analytics","title":"Real-time Analytics","text":"<pre><code>class AnalyticsCDC(PostgreSQLCDCWorker):\n    async def process_change(self, change: DatabaseChange):\n        if change.table == \"user_events\":\n            await self.update_real_time_dashboard(change.new_data)\n            await self.trigger_alerts_if_needed(change.new_data)\n</code></pre>"},{"location":"brokers/database/#data-pipeline-triggers","title":"Data Pipeline Triggers","text":"<pre><code>class PipelineTrigger(MySQLCDCWorker):\n    async def process_change(self, change: DatabaseChange):\n        if change.table == \"raw_data\" and change.change_type.value == \"INSERT\":\n            await self.trigger_etl_pipeline(change.new_data)\n</code></pre>"},{"location":"brokers/database/#cross-system-synchronization","title":"Cross-System Synchronization","text":"<pre><code>class ECommerceCDC(PostgreSQLCDCWorker):\n    async def process_change(self, change: DatabaseChange):\n        if change.table == \"products\" and change.change_type.value == \"UPDATE\":\n            # Update search index\n            await self.update_elasticsearch(change.new_data)\n            # Update cache\n            await self.invalidate_cache(change.primary_key)\n            # Sync to warehouse\n            await self.sync_to_warehouse(change.new_data)\n</code></pre>"},{"location":"brokers/database/#best-practices","title":"Best Practices","text":"<ol> <li>Performance:</li> <li>Use appropriate batch sizes for sync operations</li> <li>Filter CDC events by relevant tables/schemas only</li> <li> <p>Monitor replication lag in production</p> </li> <li> <p>Reliability:</p> </li> <li>Implement proper error handling</li> <li>Use idempotent processing logic</li> <li> <p>Monitor worker health and restart if needed</p> </li> <li> <p>Security:</p> </li> <li>Use dedicated database users with minimal privileges</li> <li>Encrypt connections in production</li> <li> <p>Store credentials securely</p> </li> <li> <p>Operations:</p> </li> <li>Monitor replication slot usage (PostgreSQL)</li> <li>Monitor binary log disk usage (MySQL)</li> <li>Set up alerting for worker failures</li> </ol>"},{"location":"brokers/database/#creating-database-workers","title":"Creating Database Workers","text":"<p>Use Pythia's templates to quickly create database workers:</p> <pre><code># Create CDC worker\npythia create database-cdc my_cdc_worker --database-type postgresql\n\n# Create sync worker\npythia create database-sync my_sync_worker\n</code></pre> <p>This will generate a complete worker with configuration examples and best practices built-in.</p>"},{"location":"brokers/http/","title":"HTTP Workers","text":"<p>Pythia's HTTP Workers enable efficient HTTP-based tasks like API polling and webhook sending with built-in resilience patterns.</p>"},{"location":"brokers/http/#http-worker-types","title":"\ud83d\udd0d HTTP Worker Types","text":""},{"location":"brokers/http/#1-pollerworker-api-polling","title":"1. PollerWorker - API Polling","text":"<p>Worker that performs HTTP polling to external APIs at regular intervals.</p>"},{"location":"brokers/http/#2-webhooksenderworker-webhook-sending","title":"2. WebhookSenderWorker - Webhook Sending","text":"<p>Worker that sends HTTP webhooks with automatic retry and broadcast support.</p>"},{"location":"brokers/http/#3-httpworker-base-class","title":"3. HTTPWorker - Base Class","text":"<p>Base class that provides common HTTP functionality for all workers.</p>"},{"location":"brokers/http/#pollerworker","title":"\ud83d\ude80 PollerWorker","text":""},{"location":"brokers/http/#basic-use-case","title":"Basic Use Case","text":"<pre><code>from pythia.brokers.http import PollerWorker\nfrom pythia.config.http import PollerConfig\n\nclass PaymentStatusPoller(PollerWorker):\n    def __init__(self):\n        super().__init__(\n            url=\"https://api.payments.com/status\",\n            interval=30,  # Poll every 30 seconds\n            method=\"GET\",\n            headers={\"Authorization\": \"Bearer your-token\"},\n            params={\"status\": \"pending\"}\n        )\n\n    async def process_message(self, message):\n        \"\"\"Process HTTP response from polling\"\"\"\n        data = message.body\n\n        # Handle different response types\n        if isinstance(data, list):\n            for item in data:\n                await self.process_payment_update(item)\n        else:\n            await self.process_payment_update(data)\n\n        return {\"processed\": True, \"count\": len(data) if isinstance(data, list) else 1}\n\n    async def process_payment_update(self, payment_data):\n        \"\"\"Process individual payment update\"\"\"\n        payment_id = payment_data.get(\"id\")\n        status = payment_data.get(\"status\")\n\n        if status == \"completed\":\n            await self.handle_payment_completed(payment_id)\n        elif status == \"failed\":\n            await self.handle_payment_failed(payment_id, payment_data.get(\"error\"))\n\n        self.logger.info(f\"Processed payment {payment_id} with status {status}\")\n\n# Run the worker\nif __name__ == \"__main__\":\n    poller = PaymentStatusPoller()\n    poller.run_sync()\n</code></pre>"},{"location":"brokers/http/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from pythia.config.http import PollerConfig\n\nclass AdvancedAPIPoller(PollerWorker):\n    def __init__(self):\n        # Advanced poller configuration\n        poller_config = PollerConfig(\n            base_url=\"https://api.example.com\",\n            url=\"https://api.example.com/events\",\n            interval=60,\n            method=\"POST\",\n            connect_timeout=10.0,\n            read_timeout=30.0,\n            max_connections=50,\n            verify_ssl=True,\n            follow_redirects=True,\n            user_agent=\"MyApp/1.0\"\n        )\n\n        super().__init__(\n            url=\"https://api.example.com/events\",\n            interval=60,\n            method=\"POST\",\n            headers={\n                \"Authorization\": \"Bearer your-token\",\n                \"Content-Type\": \"application/json\"\n            },\n            poller_config=poller_config\n        )\n\n    async def process_message(self, message):\n        # Process response with metadata\n        response_data = message.body\n        status_code = message.headers.get(\"status_code\")\n        timestamp = message.timestamp\n\n        self.logger.info(\n            f\"Received response with status {status_code} at {timestamp}\",\n            response_size=len(str(response_data))\n        )\n\n        return {\"status\": \"processed\", \"response_code\": status_code}\n</code></pre>"},{"location":"brokers/http/#custom-data-extraction","title":"Custom Data Extraction","text":"<pre><code>def extract_events(response_data):\n    \"\"\"Custom data extractor function\"\"\"\n    if \"events\" in response_data:\n        return response_data[\"events\"]  # Return list of events\n    return [response_data]  # Single event\n\nclass EventPoller(PollerWorker):\n    def __init__(self):\n        super().__init__(\n            url=\"https://api.events.com/feed\",\n            interval=120,\n            data_extractor=extract_events  # Custom data extraction\n        )\n\n    async def process_message(self, message):\n        \"\"\"Each message contains a single extracted event\"\"\"\n        event = message.body\n        event_type = event.get(\"type\")\n\n        if event_type == \"user_signup\":\n            await self.handle_user_signup(event)\n        elif event_type == \"purchase\":\n            await self.handle_purchase(event)\n\n        return {\"event_type\": event_type, \"processed\": True}\n</code></pre>"},{"location":"brokers/http/#webhooksenderworker","title":"\ud83d\udce4 WebhookSenderWorker","text":""},{"location":"brokers/http/#basic-use-case_1","title":"Basic Use Case","text":"<pre><code>from pythia.brokers.http import WebhookSenderWorker\nfrom pythia.config.http import WebhookConfig\n\nclass OrderNotificationSender(WebhookSenderWorker):\n    def __init__(self):\n        super().__init__(\n            base_url=\"https://webhooks.example.com\",\n            webhook_config=WebhookConfig(\n                base_url=\"https://webhooks.example.com\",\n                timeout=30,\n                max_retries=3,\n                retry_delay=2.0\n            )\n        )\n\n    async def process(self):\n        \"\"\"Main processing loop - override based on your needs\"\"\"\n        # This is where you'd consume from another source\n        # and send webhooks based on that data\n        pass\n\n    async def send_order_created(self, order_data):\n        \"\"\"Send order created webhook\"\"\"\n        success = await self.send_webhook(\n            endpoint=\"/orders/created\",\n            data={\n                \"event\": \"order_created\",\n                \"order_id\": order_data[\"id\"],\n                \"customer_id\": order_data[\"customer_id\"],\n                \"amount\": order_data[\"total\"],\n                \"timestamp\": order_data[\"created_at\"]\n            },\n            headers={\"X-Event-Source\": \"order-service\"}\n        )\n\n        if success:\n            self.logger.info(f\"Order created webhook sent for order {order_data['id']}\")\n        else:\n            self.logger.error(f\"Failed to send order created webhook for order {order_data['id']}\")\n\n        return success\n</code></pre>"},{"location":"brokers/http/#worker-that-consumes-and-sends-webhooks","title":"Worker that Consumes and Sends Webhooks","text":"<pre><code>from pythia.core.message import Message\nfrom pythia.brokers.redis import RedisListConsumer\n\nclass OrderWebhookRelay(WebhookSenderWorker):\n    def __init__(self):\n        super().__init__(base_url=\"https://partner-webhooks.com\")\n\n        # Source to consume messages\n        self.source = RedisListConsumer(queue=\"order_events\")\n\n    async def process(self):\n        \"\"\"Consume from Redis and send webhooks\"\"\"\n        async for message in self.source.consume():\n            await self.process_message(message)\n\n    async def process_message(self, message: Message):\n        \"\"\"Process message from Redis and send webhook\"\"\"\n        order_data = message.body\n        event_type = order_data.get(\"event_type\")\n\n        # Route to different webhook endpoints based on event type\n        if event_type == \"order_created\":\n            endpoint = \"/webhooks/orders/created\"\n        elif event_type == \"order_updated\":\n            endpoint = \"/webhooks/orders/updated\"\n        elif event_type == \"order_cancelled\":\n            endpoint = \"/webhooks/orders/cancelled\"\n        else:\n            self.logger.warning(f\"Unknown event type: {event_type}\")\n            return {\"error\": \"unknown_event_type\"}\n\n        # Send webhook\n        success = await self.send_webhook_from_message(\n            message=message,\n            endpoint=endpoint,\n            headers={\"X-Source\": \"order-service\", \"X-Event-Type\": event_type}\n        )\n\n        return {\"webhook_sent\": success, \"event_type\": event_type}\n</code></pre>"},{"location":"brokers/http/#broadcast-to-multiple-endpoints","title":"Broadcast to Multiple Endpoints","text":"<pre><code>class SystemAlertBroadcaster(WebhookSenderWorker):\n    def __init__(self):\n        super().__init__(base_url=\"https://alerts.example.com\")\n\n    async def broadcast_system_alert(self, alert_data, urgent=False):\n        \"\"\"Broadcast system alert to multiple endpoints\"\"\"\n\n        # Define endpoints based on urgency\n        if urgent:\n            endpoints = [\n                \"/critical/alerts\",\n                \"/slack/alerts\",\n                \"/email/alerts\",\n                \"/sms/alerts\"\n            ]\n            fail_fast = True  # Stop on first failure for urgent alerts\n        else:\n            endpoints = [\n                \"/general/alerts\",\n                \"/slack/alerts\"\n            ]\n            fail_fast = False  # Continue even if some fail\n\n        # Broadcast to all endpoints\n        results = await self.broadcast_webhook(\n            endpoints=endpoints,\n            data={\n                \"alert_type\": alert_data[\"type\"],\n                \"severity\": \"urgent\" if urgent else \"normal\",\n                \"message\": alert_data[\"message\"],\n                \"timestamp\": alert_data[\"timestamp\"],\n                \"source\": alert_data.get(\"source\", \"system\")\n            },\n            headers={\"X-Alert-Priority\": \"high\" if urgent else \"normal\"},\n            fail_fast=fail_fast\n        )\n\n        # Log results\n        successful = sum(results.values())\n        total = len(results)\n\n        self.logger.info(\n            f\"Alert broadcast completed: {successful}/{total} successful\",\n            results=results,\n            alert_type=alert_data[\"type\"]\n        )\n\n        return results\n</code></pre>"},{"location":"brokers/http/#configuration","title":"\ud83d\udd27 Configuration","text":""},{"location":"brokers/http/#environment-variables","title":"Environment Variables","text":"<pre><code># HTTP Client Configuration\nHTTP_BASE_URL=https://api.example.com\nHTTP_TIMEOUT=30\nHTTP_MAX_RETRIES=3\nHTTP_RETRY_DELAY=1.0\nHTTP_RETRY_BACKOFF=2.0\n\n# Authentication\nHTTP_AUTH_TYPE=bearer\nHTTP_BEARER_TOKEN=your-bearer-token\nHTTP_API_KEY=your-api-key\nHTTP_API_KEY_HEADER=X-API-Key\n\n# SSL/TLS\nHTTP_SSL_VERIFY=true\nHTTP_SSL_CERT_FILE=/path/to/cert.pem\nHTTP_SSL_KEY_FILE=/path/to/key.pem\n\n# Connection Settings\nHTTP_CONNECTION_POOL_SIZE=100\nHTTP_MAX_KEEPALIVE_CONNECTIONS=20\nHTTP_KEEPALIVE_EXPIRY=300\n\n# Poller Specific\nPOLLER_URL=https://api.example.com/events\nPOLLER_INTERVAL=60\nPOLLER_METHOD=GET\nPOLLER_CONNECT_TIMEOUT=10.0\nPOLLER_READ_TIMEOUT=30.0\n</code></pre>"},{"location":"brokers/http/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from pythia.http import HTTPClientConfig\nfrom pythia.config.http import PollerConfig, WebhookConfig\n\n# HTTP Client base config\nhttp_config = HTTPClientConfig(\n    max_connections=100,\n    connect_timeout=5.0,\n    read_timeout=30.0,\n    verify_ssl=True,\n    default_headers={\"User-Agent\": \"MyApp/1.0\"}\n)\n\n# Poller specific config\npoller_config = PollerConfig(\n    base_url=\"https://api.example.com\",\n    url=\"https://api.example.com/events\",\n    interval=60,\n    method=\"GET\",\n    connect_timeout=10.0,\n    max_connections=50\n)\n\n# Webhook specific config\nwebhook_config = WebhookConfig(\n    base_url=\"https://webhooks.example.com\",\n    timeout=30,\n    max_retries=5,\n    retry_delay=2.0,\n    retry_backoff=2.0,\n    endpoints={\n        \"orders\": \"/orders/webhook\",\n        \"users\": \"/users/webhook\",\n        \"alerts\": \"/system/alerts\"\n    }\n)\n</code></pre>"},{"location":"brokers/http/#resilience-patterns","title":"\ud83d\udee1\ufe0f Resilience Patterns","text":""},{"location":"brokers/http/#circuit-breaker","title":"Circuit Breaker","text":"<pre><code>from pythia.http import CircuitBreakerConfig\n\nclass ResilientAPIPoller(PollerWorker):\n    def __init__(self):\n        # Configure circuit breaker\n        circuit_config = CircuitBreakerConfig(\n            failure_threshold=5,    # Open after 5 failures\n            timeout_seconds=60,     # Stay open for 60 seconds\n            expected_exception=Exception\n        )\n\n        http_config = HTTPClientConfig(\n            circuit_breaker_config=circuit_config\n        )\n\n        super().__init__(\n            url=\"https://unreliable-api.com/data\",\n            interval=30,\n            http_config=http_config\n        )\n\n    async def process_message(self, message):\n        try:\n            # Process normally\n            return await self.handle_api_response(message.body)\n        except Exception as e:\n            # Circuit breaker will handle failures\n            self.logger.error(f\"API processing failed: {e}\")\n            return {\"error\": str(e), \"processed\": False}\n</code></pre>"},{"location":"brokers/http/#retry-policies","title":"Retry Policies","text":"<pre><code>from pythia.http import RetryConfig, RetryStrategy\n\nclass RetryableWebhookSender(WebhookSenderWorker):\n    def __init__(self):\n        # Configure custom retry policy\n        retry_config = RetryConfig(\n            strategy=RetryStrategy.EXPONENTIAL,\n            max_attempts=5,\n            initial_delay=1.0,\n            max_delay=60.0,\n            exponential_base=2.0,\n            jitter=True  # Add randomness to prevent thundering herd\n        )\n\n        http_config = HTTPClientConfig(\n            retry_config=retry_config\n        )\n\n        super().__init__(\n            base_url=\"https://webhooks.example.com\",\n            http_config=http_config\n        )\n\n    async def send_critical_webhook(self, data):\n        \"\"\"Send webhook with custom retry logic\"\"\"\n        success = await self.send_webhook(\n            endpoint=\"/critical/alerts\",\n            data=data,\n            headers={\"X-Priority\": \"critical\"}\n        )\n\n        if not success:\n            # Log failure after all retries exhausted\n            self.logger.critical(\n                \"Critical webhook failed after all retries\",\n                data=data\n            )\n\n        return success\n</code></pre>"},{"location":"brokers/http/#monitoring-and-logging","title":"\ud83d\udcca Monitoring and Logging","text":""},{"location":"brokers/http/#structured-logging","title":"Structured Logging","text":"<pre><code>class MonitoredPoller(PollerWorker):\n    def __init__(self):\n        super().__init__(\n            url=\"https://api.example.com/metrics\",\n            interval=60\n        )\n        self.metrics = {\n            \"requests_sent\": 0,\n            \"responses_received\": 0,\n            \"errors_count\": 0\n        }\n\n    async def process_message(self, message):\n        self.metrics[\"responses_received\"] += 1\n\n        # Log with context\n        self.logger.info(\n            \"Processing API response\",\n            url=self.url,\n            status_code=message.headers.get(\"status_code\"),\n            response_size=len(str(message.body)),\n            metrics=self.metrics\n        )\n\n        try:\n            result = await self.handle_response(message.body)\n            return result\n        except Exception as e:\n            self.metrics[\"errors_count\"] += 1\n            self.logger.error(\n                \"Error processing API response\",\n                error=str(e),\n                metrics=self.metrics,\n                exc_info=True\n            )\n            raise\n</code></pre>"},{"location":"brokers/http/#health-checks","title":"Health Checks","text":"<pre><code>class HealthCheckPoller(PollerWorker):\n    def __init__(self):\n        super().__init__(\n            url=\"https://api.example.com/health\",\n            interval=30\n        )\n        self.last_successful_poll = None\n        self.consecutive_failures = 0\n\n    async def process_message(self, message):\n        \"\"\"Monitor API health\"\"\"\n        from datetime import datetime\n\n        status_code = message.headers.get(\"status_code\", 0)\n\n        if 200 &lt;= status_code &lt; 300:\n            self.last_successful_poll = datetime.now()\n            self.consecutive_failures = 0\n\n            self.logger.info(\n                \"API health check successful\",\n                status_code=status_code,\n                response_time_ms=message.headers.get(\"response_time\", 0)\n            )\n        else:\n            self.consecutive_failures += 1\n\n            self.logger.warning(\n                \"API health check failed\",\n                status_code=status_code,\n                consecutive_failures=self.consecutive_failures\n            )\n\n            # Alert on multiple failures\n            if self.consecutive_failures &gt;= 3:\n                await self.send_health_alert()\n\n        return {\n            \"healthy\": 200 &lt;= status_code &lt; 300,\n            \"status_code\": status_code,\n            \"consecutive_failures\": self.consecutive_failures\n        }\n\n    async def send_health_alert(self):\n        \"\"\"Send alert when API is unhealthy\"\"\"\n        self.logger.critical(\n            f\"API health check failed {self.consecutive_failures} times consecutively\",\n            url=self.url\n        )\n</code></pre>"},{"location":"brokers/http/#common-use-cases","title":"\ud83c\udfaf Common Use Cases","text":""},{"location":"brokers/http/#1-payment-api-integration","title":"1. Payment API Integration","text":"<pre><code>class PaymentWebhookProcessor(WebhookSenderWorker):\n    async def process_payment_event(self, payment_data):\n        if payment_data[\"status\"] == \"completed\":\n            await self.send_webhook(\"/payments/completed\", payment_data)\n        elif payment_data[\"status\"] == \"failed\":\n            await self.send_webhook(\"/payments/failed\", payment_data)\n</code></pre>"},{"location":"brokers/http/#2-external-api-monitoring","title":"2. External API Monitoring","text":"<pre><code>class APIMonitor(PollerWorker):\n    def __init__(self):\n        super().__init__(url=\"https://status.example.com/api\", interval=60)\n\n    async def process_message(self, message):\n        if message.body.get(\"status\") != \"operational\":\n            await self.alert_ops_team(message.body)\n</code></pre>"},{"location":"brokers/http/#3-data-synchronization","title":"3. Data Synchronization","text":"<pre><code>class DataSyncPoller(PollerWorker):\n    async def process_message(self, message):\n        # Sync external data to local database\n        await self.sync_data_to_database(message.body)\n</code></pre> <p>HTTP Workers provide a robust and scalable way to work with HTTP APIs in Pythia, with built-in resilience patterns and flexible configuration to adapt to any use case.</p>"},{"location":"brokers/kafka/","title":"Kafka Integration","text":"<p>Complete guide to using Pythia with Apache Kafka using confluent-kafka library.</p>"},{"location":"brokers/kafka/#overview","title":"Overview","text":"<p>Pythia's Kafka integration uses confluent-kafka (not kafka-python) for superior performance and reliability. Our benchmarks achieved 1,872 msg/s with 2.0ms P95 latency.</p> <p>Library Choice</p> <p>Pythia uses <code>confluent-kafka</code> instead of <code>kafka-python</code> for better performance, more features, and active maintenance by Confluent.</p>"},{"location":"brokers/kafka/#quick-start","title":"Quick Start","text":"<pre><code>from pythia.core import Worker\nfrom pythia.config import WorkerConfig\nfrom pythia.config.kafka import KafkaConfig\n\n# Basic Kafka configuration\nkafka_config = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    group_id=\"my-consumer-group\",\n    topics=[\"my-topic\"]\n)\n\nconfig = WorkerConfig(broker_type=\"kafka\")\n</code></pre>"},{"location":"brokers/kafka/#configuration-options","title":"Configuration Options","text":""},{"location":"brokers/kafka/#basic-consumer-configuration","title":"Basic Consumer Configuration","text":"<pre><code>from pythia.config.kafka import KafkaConfig\n\nkafka_config = KafkaConfig(\n    # Connection\n    bootstrap_servers=\"localhost:9092\",\n\n    # Consumer settings\n    group_id=\"email-processors\",\n    topics=[\"email-events\", \"user-events\"],\n    auto_offset_reset=\"earliest\",        # or \"latest\"\n    enable_auto_commit=True,\n    auto_commit_interval_ms=5000,\n\n    # Performance\n    max_poll_records=500,\n    fetch_min_bytes=1024,\n    fetch_max_wait_ms=500\n)\n</code></pre>"},{"location":"brokers/kafka/#high-performance-configuration","title":"High-Performance Configuration","text":"<p>Based on our benchmarks (1,872 msg/s), optimal settings:</p> <pre><code>kafka_config = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    group_id=\"high-throughput-group\",\n    topics=[\"events\"],\n\n    # Optimized for throughput\n    max_poll_records=1000,              # Larger batches\n    fetch_min_bytes=50000,              # Wait for more data\n    fetch_max_wait_ms=100,              # But don't wait too long\n\n    # Session management\n    session_timeout_ms=30000,           # Longer session timeout\n    heartbeat_interval_ms=3000,         # Regular heartbeats\n    max_poll_interval_ms=600000,        # 10 minutes max processing\n\n    # Producer optimization\n    acks=\"1\",                           # Balance durability/speed\n    retries=3,\n    batch_size=16384,                   # 16KB batches\n    linger_ms=5,                        # Small delay for batching\n)\n</code></pre>"},{"location":"brokers/kafka/#security-configuration","title":"Security Configuration","text":""},{"location":"brokers/kafka/#saslplain-authentication","title":"SASL/PLAIN Authentication","text":"<pre><code>kafka_config = KafkaConfig(\n    bootstrap_servers=\"kafka.example.com:9093\",\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_username=\"your-username\",\n    sasl_password=\"your-password\",\n\n    # SSL settings\n    ssl_ca_location=\"/path/to/ca.crt\"\n)\n</code></pre>"},{"location":"brokers/kafka/#mutual-tls-mtls","title":"Mutual TLS (mTLS)","text":"<pre><code>kafka_config = KafkaConfig(\n    bootstrap_servers=\"kafka.example.com:9093\",\n    security_protocol=\"SSL\",\n    ssl_ca_location=\"/path/to/ca.crt\",\n    ssl_certificate_location=\"/path/to/client.crt\",\n    ssl_key_location=\"/path/to/client.key\",\n    ssl_key_password=\"key-password\"  # If key is encrypted\n)\n</code></pre>"},{"location":"brokers/kafka/#working-with-kafka","title":"Working with Kafka","text":""},{"location":"brokers/kafka/#basic-consumer-worker","title":"Basic Consumer Worker","text":"<pre><code>import json\nimport asyncio\nfrom typing import Any, Dict\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\nfrom pythia.config.kafka import KafkaConfig\n\nclass OrderProcessor(Worker):\n    \"\"\"Process order events from Kafka\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process a single order event\"\"\"\n        try:\n            # Parse Kafka message\n            order_data = json.loads(message.body)\n\n            self.logger.info(\n                f\"Processing order {order_data['order_id']}\",\n                extra={\n                    \"order_id\": order_data[\"order_id\"],\n                    \"topic\": message.topic,\n                    \"partition\": message.partition,\n                    \"offset\": message.offset\n                }\n            )\n\n            # Business logic\n            result = await self._process_order(order_data)\n\n            # Acknowledge message (auto-commit handles this)\n            return result\n\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Invalid JSON in message: {e}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Failed to process order: {e}\")\n            raise\n\n    async def _process_order(self, order_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process the order (business logic)\"\"\"\n        # Simulate processing\n        await asyncio.sleep(0.1)\n\n        return {\n            \"order_id\": order_data[\"order_id\"],\n            \"status\": \"processed\",\n            \"processed_by\": self.config.worker_id\n        }\n\n# Configuration\nkafka_config = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    group_id=\"order-processors\",\n    topics=[\"orders\", \"order-updates\"]\n)\n\nconfig = WorkerConfig(\n    worker_name=\"order-processor\",\n    broker_type=\"kafka\",\n    max_concurrent=10\n)\n\n# Run worker\nif __name__ == \"__main__\":\n    worker = OrderProcessor(config=config)\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"brokers/kafka/#producer-integration","title":"Producer Integration","text":"<pre><code>from pythia.brokers.kafka.producer import KafkaProducer\n\nclass OrderProcessorWithOutput(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup producer for output\n        producer_config = kafka_config.to_producer_config()\n        self.producer = KafkaProducer(producer_config)\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        order_data = json.loads(message.body)\n        result = await self._process_order(order_data)\n\n        # Send result to output topic\n        await self.producer.send_async(\n            topic=\"processed-orders\",\n            key=str(order_data[\"order_id\"]),\n            value=json.dumps(result)\n        )\n\n        return result\n\n    async def on_shutdown(self):\n        \"\"\"Cleanup producer on shutdown\"\"\"\n        await self.producer.close()\n        await super().on_shutdown()\n</code></pre>"},{"location":"brokers/kafka/#topic-management","title":"Topic Management","text":""},{"location":"brokers/kafka/#automatic-topic-creation","title":"Automatic Topic Creation","text":"<pre><code>from pythia.config.kafka import KafkaTopicConfig\n\n# Define topic configuration\ntopic_config = KafkaTopicConfig(\n    name=\"user-events\",\n    num_partitions=6,                    # Scale for throughput\n    replication_factor=3,                # High availability\n    cleanup_policy=\"delete\",             # or \"compact\"\n    retention_ms=7 * 24 * 3600 * 1000,  # 7 days\n    segment_ms=24 * 3600 * 1000,         # 1 day segments\n    max_message_bytes=1024 * 1024        # 1MB max message\n)\n\n# Worker can auto-create topics\nclass EventProcessor(Worker):\n    topic_configs = [topic_config]  # Auto-create on startup\n</code></pre>"},{"location":"brokers/kafka/#manual-topic-administration","title":"Manual Topic Administration","text":"<pre><code>from pythia.brokers.kafka.admin import KafkaAdminClient\n\nasync def setup_topics():\n    admin = KafkaAdminClient(kafka_config.to_confluent_config())\n\n    # Create topics\n    await admin.create_topics([\n        {\n            \"topic\": \"events\",\n            \"num_partitions\": 12,\n            \"replication_factor\": 3,\n            \"config\": {\n                \"cleanup.policy\": \"delete\",\n                \"retention.ms\": \"604800000\",  # 7 days\n                \"compression.type\": \"gzip\"\n            }\n        }\n    ])\n\n    # List topics\n    topics = await admin.list_topics()\n    print(f\"Available topics: {topics}\")\n</code></pre>"},{"location":"brokers/kafka/#error-handling-resilience","title":"Error Handling &amp; Resilience","text":""},{"location":"brokers/kafka/#retry-configuration","title":"Retry Configuration","text":"<pre><code>from pythia.config import ResilienceConfig\n\nresilience_config = ResilienceConfig(\n    max_retries=5,\n    retry_delay=1.0,\n    retry_backoff=2.0,              # Exponential backoff\n    retry_max_delay=60.0,\n\n    # Kafka-specific timeouts\n    processing_timeout=300,         # 5 minutes per message\n    connection_timeout=30\n)\n\nconfig = WorkerConfig(\n    broker_type=\"kafka\",\n    resilience=resilience_config\n)\n</code></pre>"},{"location":"brokers/kafka/#dead-letter-queue-pattern","title":"Dead Letter Queue Pattern","text":"<pre><code>class EventProcessorWithDLQ(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.dlq_producer = KafkaProducer(kafka_config.to_producer_config())\n\n    async def process_message(self, message: Message) -&gt; Any:\n        try:\n            return await self._process_event(message)\n        except Exception as e:\n            # Send to dead letter queue after max retries\n            if message.retry_count &gt;= self.config.max_retries:\n                await self._send_to_dlq(message, str(e))\n                return None\n            raise\n\n    async def _send_to_dlq(self, message: Message, error: str):\n        dlq_message = {\n            \"original_topic\": message.topic,\n            \"original_partition\": message.partition,\n            \"original_offset\": message.offset,\n            \"error\": error,\n            \"timestamp\": message.timestamp,\n            \"body\": message.body\n        }\n\n        await self.dlq_producer.send_async(\n            topic=f\"{message.topic}-dlq\",\n            value=json.dumps(dlq_message)\n        )\n</code></pre>"},{"location":"brokers/kafka/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":""},{"location":"brokers/kafka/#kafka-specific-metrics","title":"Kafka-Specific Metrics","text":"<pre><code>from pythia.config import MetricsConfig\n\nmetrics_config = MetricsConfig(\n    enabled=True,\n    prometheus_enabled=True,\n    custom_metrics={\n        \"kafka_consumer_lag\": True,        # Monitor lag\n        \"kafka_partition_assignment\": True, # Track partitions\n        \"kafka_commit_latency\": True,      # Monitor commits\n        \"kafka_fetch_latency\": True,       # Monitor fetch times\n        \"kafka_throughput\": True           # Messages/sec\n    }\n)\n</code></pre>"},{"location":"brokers/kafka/#consumer-lag-monitoring","title":"Consumer Lag Monitoring","text":"<pre><code>class MonitoredKafkaWorker(Worker):\n    async def on_startup(self):\n        # Setup lag monitoring\n        self.lag_monitor = KafkaLagMonitor(\n            kafka_config=kafka_config,\n            group_id=\"order-processors\"\n        )\n        await self.lag_monitor.start()\n\n    async def process_message(self, message: Message) -&gt; Any:\n        # Record processing metrics\n        with self.metrics.timer(\"kafka_message_processing_time\"):\n            result = await self._process_message(message)\n\n        # Update throughput counter\n        self.metrics.counter(\"kafka_messages_processed\").inc()\n\n        return result\n</code></pre>"},{"location":"brokers/kafka/#performance-optimization","title":"Performance Optimization","text":""},{"location":"brokers/kafka/#consumer-tuning","title":"Consumer Tuning","text":"<pre><code># High-throughput configuration\nkafka_config = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    group_id=\"high-perf-group\",\n\n    # Fetch optimization\n    fetch_min_bytes=50000,           # Wait for 50KB\n    fetch_max_wait_ms=100,           # Max 100ms wait\n    max_poll_records=2000,           # Large batches\n\n    # Connection optimization\n    session_timeout_ms=45000,        # Longer sessions\n    heartbeat_interval_ms=3000,      # Regular heartbeats\n\n    # Commit optimization\n    enable_auto_commit=False,        # Manual commits\n    auto_commit_interval_ms=1000     # If auto-commit enabled\n)\n</code></pre>"},{"location":"brokers/kafka/#producer-tuning","title":"Producer Tuning","text":"<pre><code># High-throughput producer\nproducer_config = kafka_config.to_producer_config()\nproducer_config.update({\n    \"acks\": \"1\",                     # Faster than \"all\"\n    \"batch.size\": 65536,             # 64KB batches\n    \"linger.ms\": 10,                 # Wait 10ms for batching\n    \"compression.type\": \"gzip\",      # Compress messages\n    \"buffer.memory\": 67108864,       # 64MB buffer\n    \"max.in.flight.requests.per.connection\": 5\n})\n</code></pre>"},{"location":"brokers/kafka/#testing","title":"Testing","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\nfrom confluent_kafka import Producer\n\nclass TestKafkaWorker(WorkerTestCase):\n    def setup_method(self):\n        self.producer = Producer({\n            'bootstrap.servers': 'localhost:9092'\n        })\n\n    async def test_order_processing(self):\n        # Send test message\n        test_order = {\n            \"order_id\": \"12345\",\n            \"amount\": 99.99,\n            \"customer_id\": \"cust_123\"\n        }\n\n        self.producer.produce(\n            topic=\"orders\",\n            key=\"12345\",\n            value=json.dumps(test_order)\n        )\n        self.producer.flush()\n\n        # Process message\n        message = await self.get_next_message()\n        result = await self.worker.process_message(message)\n\n        assert result[\"order_id\"] == \"12345\"\n        assert result[\"status\"] == \"processed\"\n</code></pre>"},{"location":"brokers/kafka/#production-deployment","title":"Production Deployment","text":""},{"location":"brokers/kafka/#docker-compose-with-kafka","title":"Docker Compose with Kafka","text":"<pre><code>version: '3.8'\nservices:\n  zookeeper:\n    image: confluentinc/cp-zookeeper:7.4.0\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 2181\n\n  kafka:\n    image: confluentinc/cp-kafka:7.4.0\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true\n\n  pythia-worker:\n    image: my-kafka-worker\n    depends_on:\n      - kafka\n    environment:\n      - PYTHIA_BROKER_TYPE=kafka\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092\n      - KAFKA_GROUP_ID=production-group\n      - PYTHIA_MAX_CONCURRENT=20\n    deploy:\n      replicas: 5\n</code></pre>"},{"location":"brokers/kafka/#production-kafka-configuration","title":"Production Kafka Configuration","text":"<pre><code># Production-optimized configuration\nproduction_kafka_config = KafkaConfig(\n    bootstrap_servers=\"kafka1:9092,kafka2:9092,kafka3:9092\",\n\n    # Consumer settings\n    group_id=\"production-workers\",\n    auto_offset_reset=\"earliest\",\n    enable_auto_commit=False,          # Manual commits for reliability\n\n    # Performance settings\n    max_poll_records=1000,\n    fetch_min_bytes=50000,\n    fetch_max_wait_ms=500,\n    session_timeout_ms=30000,\n    heartbeat_interval_ms=3000,\n\n    # Security\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_username=\"your-username\",\n    sasl_password=\"your-password\"\n)\n</code></pre>"},{"location":"brokers/kafka/#benchmark-results","title":"Benchmark Results","text":"<p>Our Kafka integration achieved solid performance:</p> Metric Value Throughput 1,872 msg/s P95 Latency 2.0ms P99 Latency 5.0ms CPU Usage 9.0% Memory Usage 7,728 MB Error Rate 0%"},{"location":"brokers/kafka/#troubleshooting","title":"Troubleshooting","text":""},{"location":"brokers/kafka/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Consumer lag <pre><code># Increase batch sizes\nkafka_config = KafkaConfig(\n    max_poll_records=2000,\n    fetch_min_bytes=100000\n)\n</code></pre></p> </li> <li> <p>Rebalancing issues <pre><code># Tune session timeouts\nkafka_config = KafkaConfig(\n    session_timeout_ms=45000,\n    heartbeat_interval_ms=3000,\n    max_poll_interval_ms=600000\n)\n</code></pre></p> </li> <li> <p>Connection timeouts <pre><code># Add retry logic\nresilience_config = ResilienceConfig(\n    connection_timeout=60,\n    max_retries=5\n)\n</code></pre></p> </li> </ol>"},{"location":"brokers/kafka/#migration-from-kafka-python","title":"Migration from kafka-python","text":"<p>If migrating from <code>kafka-python</code>, key differences:</p> kafka-python confluent-kafka Notes <code>KafkaConsumer</code> <code>Consumer</code> Different API <code>KafkaProducer</code> <code>Producer</code> Better performance <code>bootstrap_servers</code> <code>bootstrap.servers</code> Dot notation Python objects C library Much faster"},{"location":"brokers/kafka/#next-steps","title":"Next Steps","text":"<ul> <li>RabbitMQ Integration - Compare with RabbitMQ setup</li> <li>Performance Benchmarks - Detailed performance comparison</li> <li>Configuration Guide - Advanced configuration options</li> </ul>"},{"location":"brokers/rabbitmq/","title":"RabbitMQ Integration","text":"<p>Complete guide to using Pythia with RabbitMQ for advanced message routing and enterprise messaging patterns.</p>"},{"location":"brokers/rabbitmq/#overview","title":"Overview","text":"<p>RabbitMQ integration in Pythia provides enterprise-grade messaging with sophisticated routing, reliability guarantees, and management features. Our benchmarks achieved 1,292 msg/s with excellent resource efficiency (6.8% CPU usage).</p> <p>Enterprise Messaging</p> <p>RabbitMQ excels at complex routing scenarios, message durability, and enterprise integration patterns that Redis and Kafka don't natively support.</p>"},{"location":"brokers/rabbitmq/#quick-start","title":"Quick Start","text":"<pre><code>from pythia.core import Worker\nfrom pythia.config import WorkerConfig\nfrom pythia.config.rabbitmq import RabbitMQConfig\n\n# Basic RabbitMQ configuration\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://guest:guest@localhost:5672/\",\n    queue=\"task-queue\",\n    exchange=\"tasks-exchange\",\n    routing_key=\"tasks\"\n)\n\nconfig = WorkerConfig(broker_type=\"rabbitmq\")\n</code></pre>"},{"location":"brokers/rabbitmq/#configuration-options","title":"Configuration Options","text":""},{"location":"brokers/rabbitmq/#basic-queue-configuration","title":"Basic Queue Configuration","text":"<pre><code>from pythia.config.rabbitmq import RabbitMQConfig\n\n# Simple work queue pattern\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://user:password@localhost:5672/vhost\",\n    queue=\"work-queue\",\n    durable=True,                # Survive broker restarts\n    auto_ack=False,             # Manual acknowledgment\n    prefetch_count=10           # Fair dispatch\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#advanced-exchange-patterns","title":"Advanced Exchange Patterns","text":""},{"location":"brokers/rabbitmq/#direct-exchange-point-to-point","title":"Direct Exchange (Point-to-Point)","text":"<pre><code>rabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"orders-queue\",\n    exchange=\"orders-exchange\",\n    routing_key=\"order.created\",\n    durable=True\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#topic-exchange-pattern-matching","title":"Topic Exchange (Pattern Matching)","text":"<pre><code>rabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"notifications-queue\",\n    exchange=\"events-exchange\",\n    routing_key=\"user.*.created\",    # Wildcard routing\n    durable=True\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#fanout-exchange-broadcast","title":"Fanout Exchange (Broadcast)","text":"<pre><code>rabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"audit-queue\",\n    exchange=\"broadcast-exchange\",\n    routing_key=\"\",                  # Ignored in fanout\n    durable=True\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#high-performance-configuration","title":"High-Performance Configuration","text":"<p>Based on our benchmarks (1,292 msg/s), optimized settings:</p> <pre><code>rabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"high-throughput-queue\",\n    exchange=\"performance-exchange\",\n    routing_key=\"perf\",\n\n    # Performance optimization\n    durable=True,                    # Persistence for reliability\n    auto_ack=False,                 # Manual ack for safety\n    prefetch_count=100,             # Higher prefetch for throughput\n\n    # Connection optimization\n    heartbeat=600,                  # 10 minutes heartbeat\n    connection_attempts=5,          # More retry attempts\n    retry_delay=1.0                # Faster reconnection\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#working-with-rabbitmq","title":"Working with RabbitMQ","text":""},{"location":"brokers/rabbitmq/#basic-consumer-worker","title":"Basic Consumer Worker","text":"<pre><code>import json\nimport asyncio\nfrom typing import Any, Dict\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\nfrom pythia.config.rabbitmq import RabbitMQConfig\n\nclass EmailNotificationWorker(Worker):\n    \"\"\"Process email notifications from RabbitMQ\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process an email notification\"\"\"\n        try:\n            # Parse RabbitMQ message\n            notification_data = json.loads(message.body)\n\n            self.logger.info(\n                f\"Processing notification {notification_data.get('id')}\",\n                extra={\n                    \"notification_type\": notification_data.get(\"type\"),\n                    \"exchange\": message.exchange,\n                    \"routing_key\": message.routing_key,\n                    \"delivery_tag\": message.delivery_tag\n                }\n            )\n\n            # Business logic\n            result = await self._send_notification(notification_data)\n\n            # Manual acknowledgment (important for reliability)\n            await message.ack()\n\n            return result\n\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Invalid JSON in message: {e}\")\n            # Reject message and don't requeue\n            await message.nack(requeue=False)\n            raise\n\n        except Exception as e:\n            self.logger.error(f\"Failed to process notification: {e}\")\n            # Reject and requeue for retry\n            await message.nack(requeue=True)\n            raise\n\n    async def _send_notification(self, notification_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Send the notification (business logic)\"\"\"\n        # Simulate notification sending\n        await asyncio.sleep(0.1)\n\n        notification_type = notification_data.get(\"type\")\n        recipient = notification_data.get(\"recipient\")\n\n        # Different handling based on type\n        if notification_type == \"email\":\n            await self._send_email(recipient, notification_data)\n        elif notification_type == \"sms\":\n            await self._send_sms(recipient, notification_data)\n        elif notification_type == \"push\":\n            await self._send_push(recipient, notification_data)\n\n        return {\n            \"status\": \"sent\",\n            \"type\": notification_type,\n            \"recipient\": recipient,\n            \"processed_by\": self.config.worker_id\n        }\n\n    async def _send_email(self, recipient: str, data: Dict):\n        \"\"\"Send email notification\"\"\"\n        self.logger.info(f\"Sending email to {recipient}\")\n        # Email sending logic here\n\n    async def _send_sms(self, recipient: str, data: Dict):\n        \"\"\"Send SMS notification\"\"\"\n        self.logger.info(f\"Sending SMS to {recipient}\")\n        # SMS sending logic here\n\n    async def _send_push(self, recipient: str, data: Dict):\n        \"\"\"Send push notification\"\"\"\n        self.logger.info(f\"Sending push notification to {recipient}\")\n        # Push notification logic here\n\n# Configuration\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"notifications-queue\",\n    exchange=\"notifications-exchange\",\n    routing_key=\"notification.*\",\n    prefetch_count=20\n)\n\nconfig = WorkerConfig(\n    worker_name=\"notification-worker\",\n    broker_type=\"rabbitmq\",\n    max_concurrent=10\n)\n\n# Run worker\nif __name__ == \"__main__\":\n    worker = EmailNotificationWorker(config=config)\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"brokers/rabbitmq/#publisher-integration","title":"Publisher Integration","text":"<pre><code>from pythia.brokers.rabbitmq.producer import RabbitMQProducer\n\nclass OrderProcessorWithNotifications(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup publisher for notifications\n        self.publisher = RabbitMQProducer(rabbitmq_config)\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        order_data = json.loads(message.body)\n        result = await self._process_order(order_data)\n\n        # Publish notification based on result\n        await self._publish_notifications(order_data, result)\n\n        return result\n\n    async def _publish_notifications(self, order_data: Dict, result: Dict):\n        \"\"\"Publish different notifications based on processing result\"\"\"\n\n        # Email notification\n        email_notification = {\n            \"type\": \"email\",\n            \"recipient\": order_data[\"customer_email\"],\n            \"template\": \"order_confirmation\",\n            \"data\": result\n        }\n\n        await self.publisher.publish(\n            message=json.dumps(email_notification),\n            exchange=\"notifications-exchange\",\n            routing_key=\"notification.email\"\n        )\n\n        # SMS for high-value orders\n        if order_data.get(\"amount\", 0) &gt; 1000:\n            sms_notification = {\n                \"type\": \"sms\",\n                \"recipient\": order_data[\"customer_phone\"],\n                \"template\": \"high_value_order\",\n                \"data\": result\n            }\n\n            await self.publisher.publish(\n                message=json.dumps(sms_notification),\n                exchange=\"notifications-exchange\",\n                routing_key=\"notification.sms.priority\"\n            )\n\n        # Internal audit notification\n        audit_notification = {\n            \"type\": \"audit\",\n            \"order_id\": order_data[\"order_id\"],\n            \"amount\": order_data[\"amount\"],\n            \"processed_by\": self.config.worker_id\n        }\n\n        await self.publisher.publish(\n            message=json.dumps(audit_notification),\n            exchange=\"audit-exchange\",  # Different exchange\n            routing_key=\"order.processed\"\n        )\n\n    async def on_shutdown(self):\n        \"\"\"Cleanup publisher on shutdown\"\"\"\n        await self.publisher.close()\n        await super().on_shutdown()\n</code></pre>"},{"location":"brokers/rabbitmq/#exchange-types-routing-patterns","title":"Exchange Types &amp; Routing Patterns","text":""},{"location":"brokers/rabbitmq/#1-direct-exchange-exact-matching","title":"1. Direct Exchange (Exact Matching)","text":"<pre><code># Producer\nawait publisher.publish(\n    message=json.dumps({\"task\": \"send_email\"}),\n    exchange=\"tasks-direct\",\n    routing_key=\"email.send\"\n)\n\n# Consumer\nrabbitmq_config = RabbitMQConfig(\n    queue=\"email-queue\",\n    exchange=\"tasks-direct\",\n    routing_key=\"email.send\"  # Exact match required\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#2-topic-exchange-pattern-matching","title":"2. Topic Exchange (Pattern Matching)","text":"<pre><code># Producer - multiple routing keys\nawait publisher.publish(\n    message=json.dumps({\"event\": \"user_registered\"}),\n    exchange=\"events-topic\",\n    routing_key=\"user.registered.premium\"\n)\n\n# Consumer - pattern matching\nrabbitmq_config = RabbitMQConfig(\n    queue=\"user-events-queue\",\n    exchange=\"events-topic\",\n    routing_key=\"user.*\"        # Matches user.registered, user.updated, etc.\n)\n\n# More specific pattern\nrabbitmq_config = RabbitMQConfig(\n    queue=\"premium-events-queue\",\n    exchange=\"events-topic\",\n    routing_key=\"*.*.premium\"   # Matches any.any.premium\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#3-fanout-exchange-broadcast","title":"3. Fanout Exchange (Broadcast)","text":"<pre><code># Producer\nawait publisher.publish(\n    message=json.dumps({\"alert\": \"system_maintenance\"}),\n    exchange=\"broadcast-fanout\",\n    routing_key=\"\"  # Ignored in fanout\n)\n\n# Multiple consumers receive the same message\nconsumer1_config = RabbitMQConfig(\n    queue=\"alerts-email-queue\",\n    exchange=\"broadcast-fanout\"\n)\n\nconsumer2_config = RabbitMQConfig(\n    queue=\"alerts-sms-queue\",\n    exchange=\"broadcast-fanout\"\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#4-headers-exchange-attribute-matching","title":"4. Headers Exchange (Attribute Matching)","text":"<pre><code># Producer with headers\nawait publisher.publish(\n    message=json.dumps({\"task\": \"process\"}),\n    exchange=\"tasks-headers\",\n    routing_key=\"\",\n    headers={\n        \"priority\": \"high\",\n        \"department\": \"finance\",\n        \"region\": \"us-east\"\n    }\n)\n\n# Consumer matching headers\nrabbitmq_config = RabbitMQConfig(\n    queue=\"priority-queue\",\n    exchange=\"tasks-headers\",\n    headers_match={\n        \"priority\": \"high\",\n        \"x-match\": \"any\"  # Match any header\n    }\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#message-durability-reliability","title":"Message Durability &amp; Reliability","text":""},{"location":"brokers/rabbitmq/#durable-queues-messages","title":"Durable Queues &amp; Messages","text":"<pre><code># Durable configuration for reliability\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n    queue=\"critical-tasks\",\n    exchange=\"critical-exchange\",\n    routing_key=\"critical\",\n\n    # Durability settings\n    durable=True,                    # Queue survives broker restart\n    auto_ack=False,                 # Manual acknowledgment required\n\n    # Message persistence (set by publisher)\n    delivery_mode=2                 # Persistent messages\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#dead-letter-exchange-dlq","title":"Dead Letter Exchange (DLQ)","text":"<pre><code>class TaskWorkerWithDLQ(Worker):\n    async def process_message(self, message: Message) -&gt; Any:\n        try:\n            return await self._process_task(message)\n        except Exception as e:\n            self.logger.error(f\"Task processing failed: {e}\")\n\n            # Check retry count from headers\n            retry_count = message.headers.get(\"x-retry-count\", 0)\n\n            if retry_count &gt;= 3:\n                # Send to DLQ by rejecting without requeue\n                await message.nack(requeue=False)\n                return None\n            else:\n                # Increment retry count and requeue\n                message.headers[\"x-retry-count\"] = retry_count + 1\n                await message.nack(requeue=True)\n                raise\n\n# Queue configuration with DLQ\nrabbitmq_config = RabbitMQConfig(\n    queue=\"tasks-queue\",\n    exchange=\"tasks-exchange\",\n    routing_key=\"task\",\n\n    # DLQ configuration\n    queue_arguments={\n        \"x-dead-letter-exchange\": \"dlq-exchange\",\n        \"x-dead-letter-routing-key\": \"failed-task\",\n        \"x-message-ttl\": 300000,  # 5 minutes TTL\n        \"x-max-retries\": 3\n    }\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#error-handling-resilience","title":"Error Handling &amp; Resilience","text":""},{"location":"brokers/rabbitmq/#connection-resilience","title":"Connection Resilience","text":"<pre><code>from pythia.config import ResilienceConfig\n\n# Robust connection handling\nresilience_config = ResilienceConfig(\n    max_retries=5,\n    retry_delay=2.0,\n    retry_backoff=1.5,              # Linear backoff for RabbitMQ\n    retry_max_delay=30.0,\n\n    # RabbitMQ-specific timeouts\n    connection_timeout=30,\n    processing_timeout=300\n)\n\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://localhost:5672/\",\n\n    # Connection resilience\n    connection_attempts=5,          # Retry connection\n    retry_delay=2.0,               # Delay between attempts\n    heartbeat=300,                 # 5 minute heartbeat\n\n    # Channel settings\n    prefetch_count=50,             # Balance throughput/memory\n    auto_ack=False                 # Manual ack for reliability\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#message-acknowledgment-patterns","title":"Message Acknowledgment Patterns","text":"<pre><code>class ReliableMessageProcessor(Worker):\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Reliable message processing with proper acknowledgment\"\"\"\n        try:\n            # Process the message\n            result = await self._process_business_logic(message)\n\n            # Only acknowledge after successful processing\n            await message.ack()\n\n            return result\n\n        except RecoverableError as e:\n            # Temporary error - requeue for retry\n            self.logger.warning(f\"Recoverable error, requeueing: {e}\")\n            await message.nack(requeue=True)\n\n        except PermanentError as e:\n            # Permanent error - don't requeue\n            self.logger.error(f\"Permanent error, discarding: {e}\")\n            await message.nack(requeue=False)\n\n        except Exception as e:\n            # Unknown error - be conservative and requeue\n            self.logger.error(f\"Unknown error, requeueing: {e}\")\n            await message.nack(requeue=True)\n            raise\n</code></pre>"},{"location":"brokers/rabbitmq/#monitoring-management","title":"Monitoring &amp; Management","text":""},{"location":"brokers/rabbitmq/#rabbitmq-management-integration","title":"RabbitMQ Management Integration","text":"<pre><code>from pythia.config import MetricsConfig\n\n# RabbitMQ-specific monitoring\nmetrics_config = MetricsConfig(\n    enabled=True,\n    prometheus_enabled=True,\n    custom_metrics={\n        \"rabbitmq_queue_depth\": True,        # Monitor queue lengths\n        \"rabbitmq_connection_count\": True,   # Active connections\n        \"rabbitmq_consumer_count\": True,     # Active consumers\n        \"rabbitmq_message_rates\": True,      # Publish/deliver rates\n        \"rabbitmq_memory_usage\": True        # Broker memory usage\n    }\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#queue-monitoring","title":"Queue Monitoring","text":"<pre><code>class MonitoredRabbitMQWorker(Worker):\n    async def on_startup(self):\n        # Setup queue monitoring\n        self.queue_monitor = RabbitMQQueueMonitor(\n            management_url=\"http://localhost:15672\",\n            username=\"guest\",\n            password=\"guest\",\n            queues=[\"notifications-queue\", \"tasks-queue\"]\n        )\n        await self.queue_monitor.start()\n\n    async def process_message(self, message: Message) -&gt; Any:\n        # Record processing metrics\n        with self.metrics.timer(\"rabbitmq_message_processing_time\"):\n            result = await self._process_message(message)\n\n        # Update counters\n        self.metrics.counter(\"rabbitmq_messages_processed\").inc()\n\n        # Monitor queue depth\n        queue_depth = await self.queue_monitor.get_queue_depth(\"notifications-queue\")\n        self.metrics.gauge(\"rabbitmq_queue_depth\").set(queue_depth)\n\n        return result\n</code></pre>"},{"location":"brokers/rabbitmq/#testing","title":"Testing","text":""},{"location":"brokers/rabbitmq/#unit-testing","title":"Unit Testing","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\nfrom unittest.mock import AsyncMock\n\nclass TestRabbitMQWorker(WorkerTestCase):\n    def setup_method(self):\n        # Mock RabbitMQ connections for testing\n        self.mock_connection = AsyncMock()\n        self.mock_channel = AsyncMock()\n\n    async def test_notification_processing(self):\n        # Create test message\n        notification_data = {\n            \"type\": \"email\",\n            \"recipient\": \"user@example.com\",\n            \"template\": \"welcome\",\n            \"data\": {\"name\": \"John\"}\n        }\n\n        message = self.create_test_message(\n            body=json.dumps(notification_data),\n            exchange=\"notifications-exchange\",\n            routing_key=\"notification.email\"\n        )\n\n        # Process message\n        result = await self.worker.process_message(message)\n\n        assert result[\"status\"] == \"sent\"\n        assert result[\"type\"] == \"email\"\n        assert result[\"recipient\"] == \"user@example.com\"\n\n    async def test_error_handling(self):\n        # Create invalid message\n        invalid_message = self.create_test_message(\n            body=\"invalid json\"\n        )\n\n        # Should handle gracefully\n        with pytest.raises(json.JSONDecodeError):\n            await self.worker.process_message(invalid_message)\n</code></pre>"},{"location":"brokers/rabbitmq/#integration-testing","title":"Integration Testing","text":"<pre><code>import pika\nimport json\n\nclass TestRabbitMQIntegration:\n    def setup_method(self):\n        # Setup test RabbitMQ connection\n        self.connection = pika.BlockingConnection(\n            pika.ConnectionParameters('localhost')\n        )\n        self.channel = self.connection.channel()\n\n        # Declare test queue\n        self.channel.queue_declare(queue='test-queue', durable=True)\n\n    def test_end_to_end_processing(self):\n        # Send test message\n        test_message = {\n            \"task\": \"test_task\",\n            \"data\": {\"key\": \"value\"}\n        }\n\n        self.channel.basic_publish(\n            exchange='',\n            routing_key='test-queue',\n            body=json.dumps(test_message),\n            properties=pika.BasicProperties(delivery_mode=2)  # Persistent\n        )\n\n        # Worker should process this message\n        # Add assertions based on expected side effects\n\n    def teardown_method(self):\n        self.channel.queue_delete(queue='test-queue')\n        self.connection.close()\n</code></pre>"},{"location":"brokers/rabbitmq/#production-deployment","title":"Production Deployment","text":""},{"location":"brokers/rabbitmq/#docker-compose-with-rabbitmq","title":"Docker Compose with RabbitMQ","text":"<pre><code>version: '3.8'\nservices:\n  rabbitmq:\n    image: rabbitmq:3.12-management-alpine\n    hostname: rabbitmq\n    ports:\n      - \"5672:5672\"      # AMQP port\n      - \"15672:15672\"    # Management UI\n    environment:\n      RABBITMQ_DEFAULT_USER: admin\n      RABBITMQ_DEFAULT_PASS: secure_password\n      RABBITMQ_DEFAULT_VHOST: production\n    volumes:\n      - rabbitmq_data:/var/lib/rabbitmq\n      - ./rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf\n\n  pythia-worker:\n    image: my-rabbitmq-worker\n    depends_on:\n      - rabbitmq\n    environment:\n      - PYTHIA_BROKER_TYPE=rabbitmq\n      - RABBITMQ_URL=amqp://admin:secure_password@rabbitmq:5672/production\n      - RABBITMQ_QUEUE=production-queue\n      - PYTHIA_MAX_CONCURRENT=20\n    deploy:\n      replicas: 3\n\nvolumes:\n  rabbitmq_data:\n</code></pre>"},{"location":"brokers/rabbitmq/#production-rabbitmq-configuration","title":"Production RabbitMQ Configuration","text":"<pre><code># rabbitmq.conf\n# Memory and disk thresholds\nvm_memory_high_watermark.relative = 0.6\ndisk_free_limit.relative = 1.0\n\n# Connection limits\nnum_acceptors.tcp = 10\nhandshake_timeout = 10000\n\n# Queue settings\nqueue_master_locator = min-masters\n</code></pre>"},{"location":"brokers/rabbitmq/#production-python-configuration","title":"Production Python Configuration","text":"<pre><code># Production-optimized RabbitMQ configuration\nproduction_rabbitmq_config = RabbitMQConfig(\n    url=\"amqp://admin:secure_password@rabbitmq-cluster:5672/production\",\n\n    # Queue settings\n    queue=\"production-tasks\",\n    exchange=\"production-exchange\",\n    routing_key=\"task\",\n    durable=True,\n\n    # Performance optimization\n    prefetch_count=100,             # Higher prefetch for throughput\n    auto_ack=False,                # Reliability over speed\n\n    # Connection optimization\n    heartbeat=300,                 # 5 minutes\n    connection_attempts=5,         # Retry connections\n    retry_delay=1.0,              # Quick retry\n\n    # Security\n    verify_ssl=True,\n    ca_cert_path=\"/etc/ssl/ca.crt\"\n)\n</code></pre>"},{"location":"brokers/rabbitmq/#benchmark-results","title":"Benchmark Results","text":"<p>Our RabbitMQ integration achieved solid performance with excellent resource efficiency:</p> Metric Value Throughput 1,292 msg/s P95 Latency 0.0ms* P99 Latency 0.0ms* CPU Usage 6.8% Memory Usage 7,893 MB Error Rate 0% <p>*Simplified latency measurement in benchmarks</p>"},{"location":"brokers/rabbitmq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"brokers/rabbitmq/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Connection drops <pre><code># Increase heartbeat interval\nrabbitmq_config = RabbitMQConfig(\n    heartbeat=600,\n    connection_attempts=10,\n    retry_delay=2.0\n)\n</code></pre></p> </li> <li> <p>Queue buildup <pre><code># Increase prefetch and workers\nrabbitmq_config = RabbitMQConfig(\n    prefetch_count=200,\n    # Scale workers horizontally\n)\n</code></pre></p> </li> <li> <p>Memory issues on broker <pre><code># Check RabbitMQ memory usage\nrabbitmqctl status\n\n# Set memory high watermark\nrabbitmqctl set_vm_memory_high_watermark 0.4\n</code></pre></p> </li> </ol>"},{"location":"brokers/rabbitmq/#rabbitmq-vs-other-brokers","title":"RabbitMQ vs Other Brokers","text":"Feature RabbitMQ Kafka Redis Routing \u2705 Advanced \u274c Topic-based \u274c Basic Durability \u2705 Excellent \u2705 Excellent \u26a0\ufe0f Limited Management \u2705 Built-in UI \u26a0\ufe0f External tools \u26a0\ufe0f CLI only Throughput \u26a0\ufe0f Medium \u2705 High \u2705 Highest Latency \u2705 Low \u26a0\ufe0f Medium \u2705 Lowest Complexity \u26a0\ufe0f High \u26a0\ufe0f High \u2705 Simple"},{"location":"brokers/rabbitmq/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Benchmarks - Complete broker comparison</li> <li>Configuration Guide - Advanced configuration patterns</li> <li>Examples - Real-world implementation examples</li> </ul>"},{"location":"brokers/redis/","title":"Redis Integration","text":"<p>Complete guide to using Pythia with Redis as your message broker.</p>"},{"location":"brokers/redis/#overview","title":"Overview","text":"<p>Redis integration in Pythia supports multiple patterns: - Lists - Simple queue-based messaging - Streams - Advanced event streaming with consumer groups - Pub/Sub - Publisher/subscriber messaging pattern</p>"},{"location":"brokers/redis/#quick-start","title":"Quick Start","text":"<pre><code>from pythia.core import Worker\nfrom pythia.config import WorkerConfig\nfrom pythia.config.redis import RedisConfig\n\n# Basic Redis configuration\nredis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    queue=\"my-queue\"  # For lists-based messaging\n)\n\nconfig = WorkerConfig(broker_type=\"redis\")\n</code></pre>"},{"location":"brokers/redis/#configuration-options","title":"Configuration Options","text":""},{"location":"brokers/redis/#basic-connection","title":"Basic Connection","text":"<pre><code>redis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    db=0,\n    password=None,  # Optional authentication\n)\n</code></pre>"},{"location":"brokers/redis/#lists-based-queues-recommended","title":"Lists-Based Queues (Recommended)","text":"<pre><code>redis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    queue=\"task-queue\",           # Queue name\n    batch_size=10,                # Process in batches\n    block_timeout_ms=1000         # Polling timeout\n)\n</code></pre>"},{"location":"brokers/redis/#streams-based-processing","title":"Streams-Based Processing","text":"<pre><code>redis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    stream=\"events-stream\",       # Stream name\n    consumer_group=\"workers\",     # Consumer group\n    batch_size=50,               # Larger batches for streams\n    max_stream_length=10000      # Limit stream size\n)\n</code></pre>"},{"location":"brokers/redis/#pubsub-pattern","title":"Pub/Sub Pattern","text":"<pre><code>redis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    channel=\"notifications\",      # Pub/Sub channel\n    batch_size=1                 # Process individually\n)\n</code></pre>"},{"location":"brokers/redis/#performance-optimizations","title":"Performance Optimizations","text":"<p>Based on our benchmark results (3,304 msg/s), here are the optimal settings:</p> <pre><code>from pythia.config.redis import RedisConfig\n\n# High-performance configuration\nredis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n\n    # Queue settings\n    queue=\"high-throughput-queue\",\n    batch_size=100,               # Larger batches for throughput\n    block_timeout_ms=100,         # Shorter polling for responsiveness\n\n    # Connection optimization\n    connection_pool_size=20,      # More connections\n    socket_keepalive=True,        # Keep connections alive\n    socket_timeout=5,             # Connection timeout\n    retry_on_timeout=True,        # Auto-retry on timeout\n\n    # Health monitoring\n    health_check_interval=30      # Regular health checks\n)\n</code></pre>"},{"location":"brokers/redis/#working-with-different-redis-patterns","title":"Working with Different Redis Patterns","text":""},{"location":"brokers/redis/#1-lists-lpushbrpop","title":"1. Lists (LPUSH/BRPOP)","text":"<p>Best for simple task queues:</p> <pre><code>class TaskWorker(Worker):\n    async def process_message(self, message):\n        # Process task from Redis list\n        task_data = json.loads(message.body)\n        result = await self.execute_task(task_data)\n        return result\n\n# Producer side\nredis_client.lpush(\"task-queue\", json.dumps(task_data))\n</code></pre>"},{"location":"brokers/redis/#2-streams-xreadxack","title":"2. Streams (XREAD/XACK)","text":"<p>Best for event processing with replay capability:</p> <pre><code>class EventWorker(Worker):\n    async def process_message(self, message):\n        # Process event from Redis stream\n        event_data = message.fields  # Stream fields\n        await self.handle_event(event_data)\n\n        # Acknowledge processing\n        await self.ack_message(message)\n\n# Producer side\nredis_client.xadd(\"events-stream\", {\"event\": \"user_registered\", \"user_id\": \"123\"})\n</code></pre>"},{"location":"brokers/redis/#3-pubsub-publishsubscribe","title":"3. Pub/Sub (PUBLISH/SUBSCRIBE)","text":"<p>Best for real-time notifications:</p> <pre><code>class NotificationWorker(Worker):\n    async def process_message(self, message):\n        # Process real-time notification\n        notification = json.loads(message.body)\n        await self.send_notification(notification)\n\n# Producer side\nredis_client.publish(\"notifications\", json.dumps(notification_data))\n</code></pre>"},{"location":"brokers/redis/#error-handling-resilience","title":"Error Handling &amp; Resilience","text":"<pre><code>from pythia.config import ResilienceConfig\n\nresilience_config = ResilienceConfig(\n    max_retries=5,                # Retry failed messages\n    retry_delay=1.0,             # Initial delay\n    retry_backoff=2.0,           # Exponential backoff\n    circuit_breaker_enabled=True, # Circuit breaker protection\n    processing_timeout=30        # Per-message timeout\n)\n\nconfig = WorkerConfig(\n    broker_type=\"redis\",\n    resilience=resilience_config\n)\n</code></pre>"},{"location":"brokers/redis/#monitoring-metrics","title":"Monitoring &amp; Metrics","text":"<p>Enable Redis-specific metrics:</p> <pre><code>from pythia.config import MetricsConfig\n\nmetrics_config = MetricsConfig(\n    enabled=True,\n    prometheus_enabled=True,\n    custom_metrics={\n        \"redis_queue_length\": True,      # Monitor queue depth\n        \"redis_connection_pool\": True,   # Monitor connections\n        \"redis_memory_usage\": True       # Monitor Redis memory\n    }\n)\n\nworker = TaskWorker(\n    config=config,\n    metrics_config=metrics_config\n)\n</code></pre>"},{"location":"brokers/redis/#production-deployment","title":"Production Deployment","text":""},{"location":"brokers/redis/#docker-compose-example","title":"Docker Compose Example","text":"<pre><code>version: '3.8'\nservices:\n  redis:\n    image: redis:7-alpine\n    command: redis-server --maxmemory 1gb --maxmemory-policy allkeys-lru\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  pythia-worker:\n    image: my-pythia-worker\n    depends_on:\n      - redis\n    environment:\n      - PYTHIA_BROKER_TYPE=redis\n      - REDIS_HOST=redis\n      - REDIS_QUEUE=production-queue\n      - PYTHIA_MAX_CONCURRENT=10\n    deploy:\n      replicas: 3\n\nvolumes:\n  redis_data:\n</code></pre>"},{"location":"brokers/redis/#production-redis-configuration","title":"Production Redis Configuration","text":"<pre><code># redis.conf\nmaxmemory 2gb\nmaxmemory-policy allkeys-lru\nsave 900 1\nsave 300 10\nsave 60 10000\nappendonly yes\nappendfsync everysec\n</code></pre>"},{"location":"brokers/redis/#testing","title":"Testing","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\nfrom redis import Redis\n\nclass TestRedisWorker(WorkerTestCase):\n    def setup_method(self):\n        self.redis = Redis(host='localhost', port=6379, decode_responses=True)\n        self.redis.flushdb()  # Clean test database\n\n    async def test_message_processing(self):\n        # Add test message\n        self.redis.lpush(\"test-queue\", '{\"task\": \"test\"}')\n\n        # Process message\n        message = await self.get_next_message()\n        result = await self.worker.process_message(message)\n\n        assert result is not None\n</code></pre>"},{"location":"brokers/redis/#benchmark-results","title":"Benchmark Results","text":"<p>Our Redis integration achieved exceptional performance:</p> Metric Value Throughput 3,304 msg/s P95 Latency 0.6ms P99 Latency 2.2ms CPU Usage 4.2% Memory Usage 7,877 MB Error Rate 0%"},{"location":"brokers/redis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"brokers/redis/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Connection timeouts <pre><code># Increase timeout values\nredis_config = RedisConfig(\n    socket_timeout=10,\n    connection_timeout=30\n)\n</code></pre></p> </li> <li> <p>Memory issues <pre><code># Use stream trimming\nredis_config = RedisConfig(\n    max_stream_length=50000,\n    trim_strategy=\"maxlen\"\n)\n</code></pre></p> </li> <li> <p>High CPU usage <pre><code># Increase batch sizes\nredis_config = RedisConfig(\n    batch_size=200,\n    block_timeout_ms=1000\n)\n</code></pre></p> </li> </ol>"},{"location":"brokers/redis/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Complete configuration reference</li> <li>Performance Benchmarks - Detailed performance analysis</li> <li>Kafka Integration - Compare with Kafka setup</li> </ul>"},{"location":"examples/aws-cloud/","title":"AWS SQS/SNS Example","text":"<p>This example demonstrates how to build a distributed order processing system using AWS SQS and SNS with Pythia.</p>"},{"location":"examples/aws-cloud/#architecture","title":"Architecture","text":"<pre><code>Order API \u2192 SNS Topic \u2192 SQS Queue \u2192 Order Processor\n                    \u2193\n                Email Queue \u2192 Email Service\n</code></pre>"},{"location":"examples/aws-cloud/#prerequisites","title":"Prerequisites","text":"<p>Install Pythia with AWS support:</p> <pre><code>pip install pythia[aws]\n</code></pre> <p>Set up AWS credentials and infrastructure:</p> <pre><code># AWS credentials\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\nexport AWS_REGION=\"us-east-1\"\n\n# SQS Queue URLs\nexport SQS_ORDERS_QUEUE_URL=\"https://sqs.us-east-1.amazonaws.com/123456789012/orders\"\nexport SQS_EMAILS_QUEUE_URL=\"https://sqs.us-east-1.amazonaws.com/123456789012/emails\"\n\n# SNS Topic ARN\nexport SNS_ORDER_EVENTS_TOPIC=\"arn:aws:sns:us-east-1:123456789012:order-events\"\n</code></pre>"},{"location":"examples/aws-cloud/#order-processing-worker","title":"Order Processing Worker","text":"<pre><code># order_processor.py\nimport asyncio\nimport json\nfrom pythia import Worker\nfrom pythia.brokers.cloud import SQSConsumer, SNSProducer\nfrom pythia.models import Message\n\nclass OrderProcessor(Worker):\n    \"\"\"Processes incoming orders from SQS and publishes events to SNS.\"\"\"\n\n    source = SQSConsumer(\n        queue_url=\"${SQS_ORDERS_QUEUE_URL}\",\n        max_messages=10,\n        wait_time=20  # Long polling\n    )\n\n    def __init__(self):\n        super().__init__()\n        # SNS producer for order events\n        self.event_publisher = SNSProducer(\n            topic_arn=\"${SNS_ORDER_EVENTS_TOPIC}\"\n        )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Process an order and publish events.\"\"\"\n        order_data = message.body\n        order_id = order_data.get(\"order_id\")\n\n        print(f\"Processing order {order_id}\")\n\n        try:\n            # Validate order\n            if not self._validate_order(order_data):\n                raise ValueError(f\"Invalid order data: {order_id}\")\n\n            # Process payment\n            payment_result = await self._process_payment(order_data)\n\n            # Update inventory\n            await self._update_inventory(order_data)\n\n            # Publish order processed event\n            await self.event_publisher.send(\n                message=Message(\n                    body={\n                        \"event_type\": \"order.processed\",\n                        \"order_id\": order_id,\n                        \"customer_id\": order_data.get(\"customer_id\"),\n                        \"total_amount\": order_data.get(\"total_amount\"),\n                        \"payment_id\": payment_result[\"payment_id\"]\n                    }\n                ),\n                subject=\"Order Processed\",\n                message_attributes={\n                    \"event_type\": {\"DataType\": \"String\", \"StringValue\": \"order.processed\"},\n                    \"order_id\": {\"DataType\": \"String\", \"StringValue\": str(order_id)}\n                }\n            )\n\n            return {\n                \"status\": \"processed\",\n                \"order_id\": order_id,\n                \"payment_id\": payment_result[\"payment_id\"]\n            }\n\n        except Exception as e:\n            print(f\"Error processing order {order_id}: {e}\")\n\n            # Publish order failed event\n            await self.event_publisher.send(\n                message=Message(\n                    body={\n                        \"event_type\": \"order.failed\",\n                        \"order_id\": order_id,\n                        \"error\": str(e)\n                    }\n                ),\n                subject=\"Order Failed\",\n                message_attributes={\n                    \"event_type\": {\"DataType\": \"String\", \"StringValue\": \"order.failed\"},\n                    \"order_id\": {\"DataType\": \"String\", \"StringValue\": str(order_id)}\n                }\n            )\n\n            # Re-raise to mark message as failed\n            raise\n\n    def _validate_order(self, order_data: dict) -&gt; bool:\n        \"\"\"Validate order data.\"\"\"\n        required_fields = [\"order_id\", \"customer_id\", \"items\", \"total_amount\"]\n        return all(field in order_data for field in required_fields)\n\n    async def _process_payment(self, order_data: dict) -&gt; dict:\n        \"\"\"Simulate payment processing.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate API call\n        return {\n            \"payment_id\": f\"pay_{order_data['order_id']}\",\n            \"status\": \"completed\"\n        }\n\n    async def _update_inventory(self, order_data: dict) -&gt; None:\n        \"\"\"Update inventory levels.\"\"\"\n        await asyncio.sleep(0.05)  # Simulate database update\n        print(f\"Inventory updated for order {order_data['order_id']}\")\n\n# Run the worker\nif __name__ == \"__main__\":\n    worker = OrderProcessor()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/aws-cloud/#email-notification-worker","title":"Email Notification Worker","text":"<pre><code># email_service.py\nimport asyncio\nfrom pythia import Worker\nfrom pythia.brokers.cloud import SQSConsumer\nfrom pythia.models import Message\n\nclass EmailService(Worker):\n    \"\"\"Sends email notifications based on order events.\"\"\"\n\n    source = SQSConsumer(\n        queue_url=\"${SQS_EMAILS_QUEUE_URL}\",\n        max_messages=5,\n        wait_time=10\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Send email notification.\"\"\"\n        event_data = message.body\n        event_type = event_data.get(\"event_type\")\n\n        if event_type == \"order.processed\":\n            return await self._send_order_confirmation(event_data)\n        elif event_type == \"order.failed\":\n            return await self._send_order_failure_notification(event_data)\n        else:\n            print(f\"Unknown event type: {event_type}\")\n            return {\"status\": \"skipped\"}\n\n    async def _send_order_confirmation(self, event_data: dict) -&gt; dict:\n        \"\"\"Send order confirmation email.\"\"\"\n        order_id = event_data[\"order_id\"]\n        customer_id = event_data[\"customer_id\"]\n\n        # Simulate email sending\n        await asyncio.sleep(0.2)\n\n        print(f\"\u2709\ufe0f Confirmation email sent for order {order_id} to customer {customer_id}\")\n\n        return {\n            \"status\": \"sent\",\n            \"email_type\": \"order_confirmation\",\n            \"order_id\": order_id\n        }\n\n    async def _send_order_failure_notification(self, event_data: dict) -&gt; dict:\n        \"\"\"Send order failure notification.\"\"\"\n        order_id = event_data[\"order_id\"]\n        error = event_data.get(\"error\", \"Unknown error\")\n\n        # Simulate email sending\n        await asyncio.sleep(0.1)\n\n        print(f\"\ud83d\udce7 Failure notification sent for order {order_id}: {error}\")\n\n        return {\n            \"status\": \"sent\",\n            \"email_type\": \"order_failure\",\n            \"order_id\": order_id\n        }\n\n# Run the worker\nif __name__ == \"__main__\":\n    worker = EmailService()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/aws-cloud/#sns-to-sqs-configuration","title":"SNS to SQS Configuration","text":"<p>Configure SNS topic to fan out messages to multiple SQS queues:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"*\"\n      },\n      \"Action\": \"sqs:SendMessage\",\n      \"Resource\": \"arn:aws:sqs:us-east-1:123456789012:emails\",\n      \"Condition\": {\n        \"ArnEquals\": {\n          \"aws:SourceArn\": \"arn:aws:sns:us-east-1:123456789012:order-events\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"examples/aws-cloud/#testing-the-system","title":"Testing the System","text":"<p>Create a test script to send orders:</p> <pre><code># test_orders.py\nimport asyncio\nimport json\nfrom pythia.brokers.cloud import SQSProducer\nfrom pythia.models import Message\n\nasync def send_test_order():\n    \"\"\"Send a test order to the SQS queue.\"\"\"\n    producer = SQSProducer(queue_url=\"${SQS_ORDERS_QUEUE_URL}\")\n\n    test_order = {\n        \"order_id\": \"ORD-12345\",\n        \"customer_id\": \"CUST-789\",\n        \"items\": [\n            {\"product_id\": \"PROD-1\", \"quantity\": 2, \"price\": 29.99},\n            {\"product_id\": \"PROD-2\", \"quantity\": 1, \"price\": 49.99}\n        ],\n        \"total_amount\": 109.97,\n        \"shipping_address\": {\n            \"street\": \"123 Main St\",\n            \"city\": \"Anytown\",\n            \"state\": \"CA\",\n            \"zip\": \"12345\"\n        }\n    }\n\n    await producer.send(Message(body=test_order))\n    print(f\"Test order {test_order['order_id']} sent!\")\n\nif __name__ == \"__main__\":\n    asyncio.run(send_test_order())\n</code></pre>"},{"location":"examples/aws-cloud/#running-the-example","title":"Running the Example","text":"<ol> <li>Start the workers:</li> </ol> <pre><code># Terminal 1: Order processor\npython order_processor.py\n\n# Terminal 2: Email service\npython email_service.py\n</code></pre> <ol> <li>Send test orders:</li> </ol> <pre><code># Terminal 3: Send test data\npython test_orders.py\n</code></pre>"},{"location":"examples/aws-cloud/#monitoring-and-troubleshooting","title":"Monitoring and Troubleshooting","text":""},{"location":"examples/aws-cloud/#cloudwatch-metrics","title":"CloudWatch Metrics","text":"<p>Monitor your workers with AWS CloudWatch:</p> <ul> <li>SQS Queue Metrics: ApproximateNumberOfMessages, ApproximateNumberOfMessagesVisible</li> <li>SNS Topic Metrics: NumberOfMessagesPublished, NumberOfNotificationsFailed</li> </ul>"},{"location":"examples/aws-cloud/#error-handling","title":"Error Handling","text":"<pre><code>from pythia.brokers.cloud import SQSConsumer\nfrom botocore.exceptions import BotoCoreError, ClientError\n\nclass RobustOrderProcessor(Worker):\n    source = SQSConsumer(\n        queue_url=\"${SQS_ORDERS_QUEUE_URL}\",\n        max_retries=3,  # Built-in retry handling\n        timeout=30\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        try:\n            # Your processing logic\n            return await self._process_order(message.body)\n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            if error_code in ['QueueDoesNotExist', 'InvalidQueueUrl']:\n                print(f\"AWS configuration error: {e}\")\n                # Handle configuration issues\n            raise\n        except BotoCoreError as e:\n            print(f\"AWS service error: {e}\")\n            raise\n</code></pre>"},{"location":"examples/aws-cloud/#best-practices","title":"Best Practices","text":"<ol> <li>Queue Configuration: Use dead letter queues for failed messages</li> <li>Batch Processing: Process multiple messages together when possible</li> <li>Monitoring: Set up CloudWatch alarms for queue depth and processing errors</li> <li>Security: Use IAM roles instead of access keys in production</li> <li>Cost Optimization: Use long polling to reduce SQS requests</li> </ol> <p>This example demonstrates a production-ready microservices architecture using AWS SQS/SNS with Pythia's cloud workers.</p>"},{"location":"examples/azure-cloud/","title":"Azure Service Bus &amp; Storage Queues Example","text":"<p>This example demonstrates building a comprehensive document processing pipeline using both Azure Service Bus for critical operations and Azure Storage Queues for background tasks.</p>"},{"location":"examples/azure-cloud/#architecture","title":"Architecture","text":"<pre><code>Document Upload \u2192 Service Bus Queue \u2192 Document Processor\n                                   \u2193\n                            Storage Queue \u2192 Thumbnail Generator\n                                   \u2193\n                            Storage Queue \u2192 OCR Processor\n                                   \u2193\n                            Service Bus \u2192 Notification Service\n</code></pre>"},{"location":"examples/azure-cloud/#prerequisites","title":"Prerequisites","text":"<p>Install Pythia with Azure support:</p> <pre><code>pip install pythia[azure]\n</code></pre> <p>Set up Azure resources and credentials:</p> <pre><code># Azure Service Bus (for critical operations)\nexport AZURE_SERVICE_BUS_CONNECTION_STRING=\"Endpoint=sb://your-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=your-key\"\nexport SERVICE_BUS_DOCUMENTS_QUEUE=\"document-processing\"\nexport SERVICE_BUS_NOTIFICATIONS_QUEUE=\"notifications\"\n\n# Azure Storage (for background tasks)\nexport AZURE_STORAGE_CONNECTION_STRING=\"DefaultEndpointsProtocol=https;AccountName=yourstorageaccount;AccountKey=your-key;EndpointSuffix=core.windows.net\"\nexport STORAGE_THUMBNAILS_QUEUE=\"thumbnails\"\nexport STORAGE_OCR_QUEUE=\"ocr-processing\"\n</code></pre>"},{"location":"examples/azure-cloud/#document-processing-worker-service-bus","title":"Document Processing Worker (Service Bus)","text":"<pre><code># document_processor.py\nimport asyncio\nimport json\nimport os\nfrom datetime import datetime\nfrom pythia import Worker\nfrom pythia.brokers.cloud import ServiceBusConsumer, StorageQueueProducer, ServiceBusProducer\nfrom pythia.models import Message\n\nclass DocumentProcessor(Worker):\n    \"\"\"Main document processor using Azure Service Bus for reliable processing.\"\"\"\n\n    source = ServiceBusConsumer(\n        queue_name=\"${SERVICE_BUS_DOCUMENTS_QUEUE}\",\n        max_messages=5,\n        max_wait_time=30  # 30 seconds max wait\n    )\n\n    def __init__(self):\n        super().__init__()\n        # Storage Queue producers for background tasks\n        self.thumbnail_producer = StorageQueueProducer(\n            queue_name=\"${STORAGE_THUMBNAILS_QUEUE}\",\n            auto_create_queue=True\n        )\n        self.ocr_producer = StorageQueueProducer(\n            queue_name=\"${STORAGE_OCR_QUEUE}\",\n            auto_create_queue=True\n        )\n        # Service Bus producer for notifications\n        self.notification_producer = ServiceBusProducer(\n            queue_name=\"${SERVICE_BUS_NOTIFICATIONS_QUEUE}\"\n        )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Process uploaded document and trigger background tasks.\"\"\"\n        doc_data = message.body\n        document_id = doc_data.get(\"document_id\")\n        file_path = doc_data.get(\"file_path\")\n        file_type = doc_data.get(\"file_type\", \"\").lower()\n        user_id = doc_data.get(\"user_id\")\n\n        print(f\"\ud83d\udd04 Processing document {document_id} ({file_type})\")\n\n        try:\n            # Validate document\n            if not await self._validate_document(file_path, file_type):\n                raise ValueError(f\"Invalid document: {document_id}\")\n\n            # Save document metadata to database\n            metadata = await self._save_document_metadata(doc_data)\n\n            # Trigger background tasks based on file type\n            background_tasks = []\n\n            # Always generate thumbnail for supported formats\n            if file_type in ['pdf', 'png', 'jpg', 'jpeg', 'gif', 'bmp']:\n                thumbnail_task = {\n                    \"task_type\": \"thumbnail\",\n                    \"document_id\": document_id,\n                    \"file_path\": file_path,\n                    \"file_type\": file_type,\n                    \"user_id\": user_id,\n                    \"priority\": \"normal\"\n                }\n                await self.thumbnail_producer.send(\n                    Message(body=thumbnail_task),\n                    message_ttl=3600  # 1 hour TTL\n                )\n                background_tasks.append(\"thumbnail\")\n\n            # OCR processing for documents and images\n            if file_type in ['pdf', 'png', 'jpg', 'jpeg', 'tiff']:\n                ocr_task = {\n                    \"task_type\": \"ocr\",\n                    \"document_id\": document_id,\n                    \"file_path\": file_path,\n                    \"file_type\": file_type,\n                    \"user_id\": user_id,\n                    \"language\": doc_data.get(\"language\", \"en\"),\n                    \"priority\": \"high\" if file_type == \"pdf\" else \"normal\"\n                }\n                await self.ocr_producer.send(\n                    Message(body=ocr_task),\n                    message_ttl=7200  # 2 hours TTL\n                )\n                background_tasks.append(\"ocr\")\n\n            # Send processing complete notification\n            await self.notification_producer.send(\n                message=Message(body={\n                    \"notification_type\": \"document_processed\",\n                    \"document_id\": document_id,\n                    \"user_id\": user_id,\n                    \"file_type\": file_type,\n                    \"background_tasks\": background_tasks,\n                    \"processed_at\": datetime.now().isoformat()\n                }),\n                # Service Bus message properties\n                correlation_id=f\"doc_{document_id}\",\n                subject=\"Document Processing Complete\"\n            )\n\n            print(f\"\u2705 Document {document_id} processed successfully\")\n\n            return {\n                \"status\": \"processed\",\n                \"document_id\": document_id,\n                \"metadata\": metadata,\n                \"background_tasks\": background_tasks\n            }\n\n        except Exception as e:\n            print(f\"\u274c Error processing document {document_id}: {e}\")\n\n            # Send error notification\n            await self.notification_producer.send(\n                message=Message(body={\n                    \"notification_type\": \"document_error\",\n                    \"document_id\": document_id,\n                    \"user_id\": user_id,\n                    \"error\": str(e),\n                    \"failed_at\": datetime.now().isoformat()\n                }),\n                correlation_id=f\"doc_{document_id}_error\"\n            )\n\n            # Re-raise to mark message as failed (will go to dead letter queue)\n            raise\n\n    async def _validate_document(self, file_path: str, file_type: str) -&gt; bool:\n        \"\"\"Validate document exists and is accessible.\"\"\"\n        if not file_path or not file_type:\n            return False\n\n        # Simulate file validation\n        await asyncio.sleep(0.1)\n\n        # Check file extension\n        allowed_types = ['pdf', 'doc', 'docx', 'txt', 'png', 'jpg', 'jpeg', 'gif', 'bmp', 'tiff']\n        return file_type in allowed_types\n\n    async def _save_document_metadata(self, doc_data: dict) -&gt; dict:\n        \"\"\"Save document metadata to database.\"\"\"\n        await asyncio.sleep(0.2)  # Simulate database operation\n\n        metadata = {\n            \"document_id\": doc_data[\"document_id\"],\n            \"original_filename\": doc_data.get(\"filename\"),\n            \"file_size\": doc_data.get(\"file_size\", 0),\n            \"mime_type\": doc_data.get(\"mime_type\"),\n            \"uploaded_by\": doc_data.get(\"user_id\"),\n            \"uploaded_at\": datetime.now().isoformat(),\n            \"status\": \"processing\"\n        }\n\n        print(f\"\ud83d\udcbe Saved metadata for document {doc_data['document_id']}\")\n        return metadata\n\n# Run the document processor\nif __name__ == \"__main__\":\n    worker = DocumentProcessor()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/azure-cloud/#thumbnail-generator-storage-queue","title":"Thumbnail Generator (Storage Queue)","text":"<pre><code># thumbnail_generator.py\nimport asyncio\nimport json\nfrom pythia import Worker\nfrom pythia.brokers.cloud import StorageQueueConsumer\nfrom pythia.models import Message\n\nclass ThumbnailGenerator(Worker):\n    \"\"\"Generates thumbnails for documents using Azure Storage Queues.\"\"\"\n\n    source = StorageQueueConsumer(\n        queue_name=\"${STORAGE_THUMBNAILS_QUEUE}\",\n        visibility_timeout=300,  # 5 minutes to process\n        max_messages=10\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Generate thumbnail for document.\"\"\"\n        task_data = message.body\n        document_id = task_data.get(\"document_id\")\n        file_path = task_data.get(\"file_path\")\n        file_type = task_data.get(\"file_type\")\n\n        print(f\"\ud83d\uddbc\ufe0f  Generating thumbnail for document {document_id}\")\n\n        try:\n            # Generate thumbnail based on file type\n            if file_type == \"pdf\":\n                thumbnail_path = await self._generate_pdf_thumbnail(file_path, document_id)\n            elif file_type in ['png', 'jpg', 'jpeg', 'gif', 'bmp']:\n                thumbnail_path = await self._generate_image_thumbnail(file_path, document_id)\n            else:\n                return {\"status\": \"skipped\", \"reason\": f\"Unsupported file type: {file_type}\"}\n\n            # Save thumbnail metadata\n            await self._save_thumbnail_metadata(document_id, thumbnail_path)\n\n            print(f\"\u2705 Thumbnail generated: {thumbnail_path}\")\n\n            return {\n                \"status\": \"completed\",\n                \"document_id\": document_id,\n                \"thumbnail_path\": thumbnail_path,\n                \"file_type\": file_type\n            }\n\n        except Exception as e:\n            print(f\"\u274c Failed to generate thumbnail for {document_id}: {e}\")\n\n            # For background tasks, we might want to retry a few times\n            retry_count = task_data.get(\"retry_count\", 0)\n            if retry_count &lt; 3:\n                print(f\"\ud83d\udd04 Will retry thumbnail generation (attempt {retry_count + 1})\")\n                # In a real implementation, you'd re-queue with retry_count + 1\n                raise  # This will make the message visible again after visibility_timeout\n            else:\n                print(f\"\ud83d\udc80 Max retries reached for document {document_id}\")\n                return {\"status\": \"failed\", \"max_retries_reached\": True}\n\n    async def _generate_pdf_thumbnail(self, file_path: str, document_id: str) -&gt; str:\n        \"\"\"Generate thumbnail from PDF first page.\"\"\"\n        await asyncio.sleep(2.0)  # Simulate PDF processing\n\n        thumbnail_path = f\"/thumbnails/{document_id}_thumb.jpg\"\n        print(f\"\ud83d\udcc4 Generated PDF thumbnail: {thumbnail_path}\")\n        return thumbnail_path\n\n    async def _generate_image_thumbnail(self, file_path: str, document_id: str) -&gt; str:\n        \"\"\"Generate thumbnail from image file.\"\"\"\n        await asyncio.sleep(0.5)  # Simulate image processing\n\n        thumbnail_path = f\"/thumbnails/{document_id}_thumb.jpg\"\n        print(f\"\ud83d\uddbc\ufe0f  Generated image thumbnail: {thumbnail_path}\")\n        return thumbnail_path\n\n    async def _save_thumbnail_metadata(self, document_id: str, thumbnail_path: str):\n        \"\"\"Save thumbnail metadata to database.\"\"\"\n        await asyncio.sleep(0.1)  # Simulate database update\n        print(f\"\ud83d\udcbe Updated document {document_id} with thumbnail path\")\n\n# Run the thumbnail generator\nif __name__ == \"__main__\":\n    worker = ThumbnailGenerator()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/azure-cloud/#ocr-processor-storage-queue","title":"OCR Processor (Storage Queue)","text":"<pre><code># ocr_processor.py\nimport asyncio\nimport random\nfrom pythia import Worker\nfrom pythia.brokers.cloud import StorageQueueConsumer\nfrom pythia.models import Message\n\nclass OCRProcessor(Worker):\n    \"\"\"Performs OCR on documents using Azure Storage Queues.\"\"\"\n\n    source = StorageQueueConsumer(\n        queue_name=\"${STORAGE_OCR_QUEUE}\",\n        visibility_timeout=600,  # 10 minutes for OCR processing\n        max_messages=3  # Limit concurrent OCR jobs\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Perform OCR on document.\"\"\"\n        task_data = message.body\n        document_id = task_data.get(\"document_id\")\n        file_path = task_data.get(\"file_path\")\n        file_type = task_data.get(\"file_type\")\n        language = task_data.get(\"language\", \"en\")\n\n        print(f\"\ud83d\udd0d Starting OCR for document {document_id} ({file_type}, {language})\")\n\n        try:\n            # Perform OCR based on file type\n            if file_type == \"pdf\":\n                text_content = await self._ocr_pdf(file_path, language)\n            elif file_type in ['png', 'jpg', 'jpeg', 'tiff']:\n                text_content = await self._ocr_image(file_path, language)\n            else:\n                return {\"status\": \"skipped\", \"reason\": f\"OCR not supported for {file_type}\"}\n\n            # Save extracted text\n            await self._save_extracted_text(document_id, text_content, language)\n\n            # Calculate confidence score and word count\n            word_count = len(text_content.split()) if text_content else 0\n            confidence = random.uniform(0.85, 0.98)  # Simulate OCR confidence\n\n            print(f\"\u2705 OCR completed: {word_count} words extracted with {confidence:.2%} confidence\")\n\n            return {\n                \"status\": \"completed\",\n                \"document_id\": document_id,\n                \"word_count\": word_count,\n                \"confidence\": confidence,\n                \"language\": language,\n                \"text_length\": len(text_content)\n            }\n\n        except Exception as e:\n            print(f\"\u274c OCR failed for document {document_id}: {e}\")\n\n            # OCR can be expensive, so we're more conservative with retries\n            retry_count = task_data.get(\"retry_count\", 0)\n            if retry_count &lt; 2:  # Only 2 retries for OCR\n                print(f\"\ud83d\udd04 Will retry OCR (attempt {retry_count + 1})\")\n                raise\n            else:\n                print(f\"\ud83d\udc80 Max OCR retries reached for document {document_id}\")\n                return {\"status\": \"failed\", \"max_retries_reached\": True}\n\n    async def _ocr_pdf(self, file_path: str, language: str) -&gt; str:\n        \"\"\"Perform OCR on PDF document.\"\"\"\n        # Simulate complex PDF OCR processing\n        await asyncio.sleep(5.0)\n\n        sample_text = f\"\"\"\n        Sample extracted text from PDF document.\n        Language: {language}\n\n        This is a multi-page document with various formatting.\n        The OCR process has successfully extracted text from\n        all pages including tables, headers, and footers.\n\n        Processing timestamp: {asyncio.get_event_loop().time()}\n        \"\"\"\n\n        return sample_text.strip()\n\n    async def _ocr_image(self, file_path: str, language: str) -&gt; str:\n        \"\"\"Perform OCR on image file.\"\"\"\n        # Simulate image OCR processing\n        await asyncio.sleep(2.0)\n\n        sample_text = f\"Text extracted from image file in {language} language. Quality: High\"\n        return sample_text\n\n    async def _save_extracted_text(self, document_id: str, text_content: str, language: str):\n        \"\"\"Save extracted text to database.\"\"\"\n        await asyncio.sleep(0.3)  # Simulate database operation\n        print(f\"\ud83d\udcbe Saved {len(text_content)} characters of extracted text for document {document_id}\")\n\n# Run the OCR processor\nif __name__ == \"__main__\":\n    worker = OCRProcessor()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/azure-cloud/#notification-service-service-bus","title":"Notification Service (Service Bus)","text":"<pre><code># notification_service.py\nimport asyncio\nfrom datetime import datetime\nfrom pythia import Worker\nfrom pythia.brokers.cloud import ServiceBusConsumer\nfrom pythia.models import Message\n\nclass NotificationService(Worker):\n    \"\"\"Handles notifications using Azure Service Bus for reliability.\"\"\"\n\n    source = ServiceBusConsumer(\n        queue_name=\"${SERVICE_BUS_NOTIFICATIONS_QUEUE}\",\n        max_messages=20,\n        max_wait_time=10\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Send notification to user.\"\"\"\n        notification_data = message.body\n        notification_type = notification_data.get(\"notification_type\")\n        user_id = notification_data.get(\"user_id\")\n\n        print(f\"\ud83d\udce2 Sending {notification_type} notification to user {user_id}\")\n\n        if notification_type == \"document_processed\":\n            return await self._send_processing_complete_notification(notification_data)\n        elif notification_type == \"document_error\":\n            return await self._send_error_notification(notification_data)\n        else:\n            return {\"status\": \"ignored\", \"reason\": f\"Unknown notification type: {notification_type}\"}\n\n    async def _send_processing_complete_notification(self, data: dict) -&gt; dict:\n        \"\"\"Send document processing complete notification.\"\"\"\n        document_id = data[\"document_id\"]\n        user_id = data[\"user_id\"]\n        background_tasks = data.get(\"background_tasks\", [])\n\n        # Simulate sending email/push notification\n        await asyncio.sleep(0.3)\n\n        notification_content = f\"\"\"\n        \u2705 Document Processing Complete\n\n        Document ID: {document_id}\n        Processed: {data.get('processed_at')}\n\n        Background tasks initiated:\n        {', '.join(background_tasks) if background_tasks else 'None'}\n\n        Your document is now available in your dashboard.\n        \"\"\"\n\n        print(f\"\ud83d\udce7 Sent completion notification to user {user_id}\")\n        print(notification_content)\n\n        return {\n            \"status\": \"sent\",\n            \"notification_type\": \"document_processed\",\n            \"user_id\": user_id,\n            \"document_id\": document_id\n        }\n\n    async def _send_error_notification(self, data: dict) -&gt; dict:\n        \"\"\"Send document processing error notification.\"\"\"\n        document_id = data[\"document_id\"]\n        user_id = data[\"user_id\"]\n        error = data.get(\"error\", \"Unknown error\")\n\n        # Simulate sending error notification\n        await asyncio.sleep(0.2)\n\n        notification_content = f\"\"\"\n        \u274c Document Processing Failed\n\n        Document ID: {document_id}\n        Failed: {data.get('failed_at')}\n        Error: {error}\n\n        Please try uploading your document again or contact support\n        if the problem persists.\n        \"\"\"\n\n        print(f\"\ud83d\udea8 Sent error notification to user {user_id}\")\n        print(notification_content)\n\n        return {\n            \"status\": \"sent\",\n            \"notification_type\": \"document_error\",\n            \"user_id\": user_id,\n            \"document_id\": document_id,\n            \"error\": error\n        }\n\n# Run the notification service\nif __name__ == \"__main__\":\n    worker = NotificationService()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/azure-cloud/#document-uploader-test-client","title":"Document Uploader (Test Client)","text":"<pre><code># document_uploader.py\nimport asyncio\nimport uuid\nfrom datetime import datetime\nfrom pythia.brokers.cloud import ServiceBusProducer\nfrom pythia.models import Message\n\nclass DocumentUploader:\n    \"\"\"Simulates document uploads by sending messages to Service Bus.\"\"\"\n\n    def __init__(self):\n        self.producer = ServiceBusProducer(\n            queue_name=\"${SERVICE_BUS_DOCUMENTS_QUEUE}\"\n        )\n\n    async def upload_document(self, filename: str, file_type: str, user_id: int = 1):\n        \"\"\"Simulate document upload.\"\"\"\n        document_id = str(uuid.uuid4())\n\n        document_data = {\n            \"document_id\": document_id,\n            \"filename\": filename,\n            \"file_path\": f\"/uploads/{document_id}.{file_type}\",\n            \"file_type\": file_type,\n            \"file_size\": 1024 * 1024 * 2,  # 2MB\n            \"mime_type\": self._get_mime_type(file_type),\n            \"user_id\": user_id,\n            \"uploaded_at\": datetime.now().isoformat(),\n            \"language\": \"en\"\n        }\n\n        await self.producer.send(\n            message=Message(body=document_data),\n            correlation_id=document_id,\n            subject=f\"Document Upload: {filename}\"\n        )\n\n        print(f\"\ud83d\udce4 Uploaded document: {filename} (ID: {document_id})\")\n        return document_id\n\n    def _get_mime_type(self, file_type: str) -&gt; str:\n        \"\"\"Get MIME type for file extension.\"\"\"\n        mime_types = {\n            'pdf': 'application/pdf',\n            'doc': 'application/msword',\n            'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',\n            'txt': 'text/plain',\n            'png': 'image/png',\n            'jpg': 'image/jpeg',\n            'jpeg': 'image/jpeg',\n            'gif': 'image/gif'\n        }\n        return mime_types.get(file_type, 'application/octet-stream')\n\nasync def upload_test_documents():\n    \"\"\"Upload various test documents.\"\"\"\n    uploader = DocumentUploader()\n\n    test_docs = [\n        (\"invoice_2024.pdf\", \"pdf\"),\n        (\"presentation.pdf\", \"pdf\"),\n        (\"scanned_receipt.jpg\", \"jpg\"),\n        (\"contract.docx\", \"docx\"),\n        (\"diagram.png\", \"png\"),\n        (\"notes.txt\", \"txt\")\n    ]\n\n    for filename, file_type in test_docs:\n        await uploader.upload_document(filename, file_type)\n        await asyncio.sleep(1)  # Space out uploads\n\nif __name__ == \"__main__\":\n    asyncio.run(upload_test_documents())\n</code></pre>"},{"location":"examples/azure-cloud/#azure-resource-setup","title":"Azure Resource Setup","text":"<p>Create the required Azure resources:</p> <pre><code># Create Resource Group\naz group create --name pythia-docs --location eastus\n\n# Create Service Bus Namespace\naz servicebus namespace create --resource-group pythia-docs --name pythia-sb --location eastus\n\n# Create Service Bus Queues\naz servicebus queue create --resource-group pythia-docs --namespace-name pythia-sb --name document-processing\naz servicebus queue create --resource-group pythia-docs --namespace-name pythia-sb --name notifications\n\n# Create Storage Account\naz storage account create --resource-group pythia-docs --name pythiastorage --location eastus --sku Standard_LRS\n\n# Get connection strings\naz servicebus namespace authorization-rule keys list --resource-group pythia-docs --namespace-name pythia-sb --name RootManageSharedAccessKey\naz storage account show-connection-string --resource-group pythia-docs --name pythiastorage\n</code></pre>"},{"location":"examples/azure-cloud/#running-the-complete-system","title":"Running the Complete System","text":"<ol> <li>Start all workers:</li> </ol> <pre><code># Terminal 1: Document processor (Service Bus)\npython document_processor.py\n\n# Terminal 2: Thumbnail generator (Storage Queue)\npython thumbnail_generator.py\n\n# Terminal 3: OCR processor (Storage Queue)\npython ocr_processor.py\n\n# Terminal 4: Notification service (Service Bus)\npython notification_service.py\n</code></pre> <ol> <li>Upload test documents:</li> </ol> <pre><code># Terminal 5: Upload documents\npython document_uploader.py\n</code></pre>"},{"location":"examples/azure-cloud/#error-handling-and-dead-letter-queues","title":"Error Handling and Dead Letter Queues","text":"<pre><code># Configure dead letter queues in Azure\nfrom azure.servicebus import ServiceBusClient\nfrom azure.servicebus.management import ServiceBusAdministrationClient\n\ndef setup_dead_letter_handling():\n    \"\"\"Set up dead letter queue handling.\"\"\"\n    mgmt_client = ServiceBusAdministrationClient.from_connection_string(\n        \"${AZURE_SERVICE_BUS_CONNECTION_STRING}\"\n    )\n\n    # Update queue properties for dead letter handling\n    queue_properties = mgmt_client.get_queue(\"document-processing\")\n    queue_properties.max_delivery_count = 3  # Move to DLQ after 3 attempts\n    queue_properties.dead_lettering_on_message_expiration = True\n\n    mgmt_client.update_queue(queue_properties)\n</code></pre>"},{"location":"examples/azure-cloud/#monitoring-with-azure","title":"Monitoring with Azure","text":""},{"location":"examples/azure-cloud/#application-insights-integration","title":"Application Insights Integration","text":"<pre><code># monitoring.py\nfrom azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\ndef setup_monitoring():\n    \"\"\"Set up Azure Application Insights monitoring.\"\"\"\n    tracer_provider = TracerProvider()\n    trace.set_tracer_provider(tracer_provider)\n\n    exporter = AzureMonitorTraceExporter(\n        connection_string=\"InstrumentationKey=your-key\"\n    )\n\n    span_processor = BatchSpanProcessor(exporter)\n    tracer_provider.add_span_processor(span_processor)\n</code></pre>"},{"location":"examples/azure-cloud/#best-practices","title":"Best Practices","text":"<ol> <li>Service Bus vs Storage Queues:</li> <li>Use Service Bus for critical operations requiring guaranteed delivery</li> <li> <p>Use Storage Queues for background tasks where occasional message loss is acceptable</p> </li> <li> <p>Message Handling:</p> </li> <li>Set appropriate visibility timeouts based on processing time</li> <li>Implement dead letter queue handling for failed messages</li> <li> <p>Use correlation IDs for message tracking</p> </li> <li> <p>Scaling:</p> </li> <li>Run multiple instances of workers for horizontal scaling</li> <li>Use auto-scaling based on queue depth metrics</li> <li> <p>Monitor processing times and adjust worker counts</p> </li> <li> <p>Cost Optimization:</p> </li> <li>Use Storage Queues for high-volume, low-priority tasks</li> <li>Set appropriate message TTL to avoid storage costs</li> <li>Monitor and optimize message sizes</li> </ol> <p>This example demonstrates a production-ready document processing pipeline using both Azure Service Bus and Storage Queues with Pythia's cloud workers.</p>"},{"location":"examples/background-jobs/","title":"Background Job Workers","text":"<p>Complete guide to implementing background job processing with Pythia's job system.</p>"},{"location":"examples/background-jobs/#overview","title":"Overview","text":"<p>Background job workers handle asynchronous task processing, scheduled jobs, and long-running operations outside of the main request/response cycle. Pythia's job system provides priority queues, retry logic, and comprehensive job management.</p>"},{"location":"examples/background-jobs/#basic-job-worker","title":"Basic Job Worker","text":""},{"location":"examples/background-jobs/#simple-email-job-processor","title":"Simple Email Job Processor","text":"<pre><code>import asyncio\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom typing import Dict, Any\nfrom pythia.jobs import BackgroundJobWorker, JobProcessor, Job, JobResult\nfrom pythia.jobs.queue import MemoryJobQueue\nfrom pythia.config import WorkerConfig\n\nclass EmailJobProcessor(JobProcessor):\n    \"\"\"Process email sending jobs\"\"\"\n\n    def __init__(self, smtp_config: Dict[str, str]):\n        self.smtp_config = smtp_config\n\n    async def process(self, job: Job) -&gt; JobResult:\n        \"\"\"Process email job\"\"\"\n        try:\n            # Extract email parameters from job\n            to_email = job.kwargs.get('to')\n            subject = job.kwargs.get('subject')\n            body = job.kwargs.get('body')\n\n            if not all([to_email, subject, body]):\n                return JobResult(\n                    success=False,\n                    error=\"Missing required email parameters\"\n                )\n\n            # Send email\n            await self._send_email(to_email, subject, body)\n\n            return JobResult(\n                success=True,\n                result={\n                    \"email_sent\": True,\n                    \"recipient\": to_email,\n                    \"subject\": subject\n                }\n            )\n\n        except Exception as e:\n            return JobResult(\n                success=False,\n                error=str(e),\n                error_type=type(e).__name__\n            )\n\n    async def _send_email(self, to_email: str, subject: str, body: str):\n        \"\"\"Send email via SMTP\"\"\"\n        # Create message\n        msg = MIMEText(body)\n        msg['Subject'] = subject\n        msg['From'] = self.smtp_config['from_email']\n        msg['To'] = to_email\n\n        # Send via SMTP (in production, use aiosmtplib for async)\n        # For demo purposes, we'll simulate the email send\n        await asyncio.sleep(0.5)  # Simulate network delay\n\n        print(f\"\ud83d\udce7 Email sent to {to_email}\")\n        print(f\"   Subject: {subject}\")\n        print(f\"   Body: {body[:100]}...\")\n\n# Usage example\nasync def run_email_jobs():\n    # Create SMTP configuration\n    smtp_config = {\n        'smtp_server': 'smtp.gmail.com',\n        'smtp_port': 587,\n        'from_email': 'noreply@example.com',\n        'username': 'your_email@gmail.com',\n        'password': 'your_password'\n    }\n\n    # Create job processor\n    processor = EmailJobProcessor(smtp_config)\n\n    # Create job worker\n    worker = BackgroundJobWorker(\n        queue=MemoryJobQueue(),\n        processor=processor,\n        max_concurrent_jobs=5,\n        polling_interval=1.0\n    )\n\n    # Submit email jobs\n    jobs = [\n        {\n            'name': 'welcome_email',\n            'func': 'send_email',\n            'kwargs': {\n                'to': 'new_user@example.com',\n                'subject': 'Welcome to Our Platform!',\n                'body': 'Thank you for joining us. Get started with your first project.'\n            }\n        },\n        {\n            'name': 'password_reset',\n            'func': 'send_email',\n            'kwargs': {\n                'to': 'user@example.com',\n                'subject': 'Password Reset Request',\n                'body': 'Click this link to reset your password: https://example.com/reset'\n            }\n        }\n    ]\n\n    # Submit jobs\n    for job_data in jobs:\n        await worker.submit_job(**job_data)\n\n    # Start processing jobs\n    print(\"\ud83d\ude80 Starting email job worker...\")\n\n    # Run for a limited time for demo\n    job_task = asyncio.create_task(worker.run())\n    await asyncio.sleep(10)  # Process for 10 seconds\n    await worker.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_email_jobs())\n</code></pre>"},{"location":"examples/background-jobs/#advanced-job-processing","title":"Advanced Job Processing","text":""},{"location":"examples/background-jobs/#multi-type-job-processor","title":"Multi-Type Job Processor","text":"<pre><code>import asyncio\nimport json\nimport httpx\nfrom typing import Dict, Any, Optional\nfrom pythia.jobs import BackgroundJobWorker, JobProcessor, Job, JobResult, JobPriority\nfrom pythia.jobs.queue import RedisJobQueue\nfrom datetime import datetime, timedelta\n\nclass MultiTypeJobProcessor(JobProcessor):\n    \"\"\"Handle multiple types of jobs\"\"\"\n\n    def __init__(self):\n        self.http_client = httpx.AsyncClient(timeout=30.0)\n\n    async def process(self, job: Job) -&gt; JobResult:\n        \"\"\"Route job to appropriate handler\"\"\"\n        job_type = job.name\n\n        try:\n            if job_type == 'send_email':\n                return await self._handle_email_job(job)\n            elif job_type == 'process_image':\n                return await self._handle_image_job(job)\n            elif job_type == 'sync_data':\n                return await self._handle_sync_job(job)\n            elif job_type == 'generate_report':\n                return await self._handle_report_job(job)\n            else:\n                return JobResult(\n                    success=False,\n                    error=f\"Unknown job type: {job_type}\"\n                )\n\n        except Exception as e:\n            return JobResult(\n                success=False,\n                error=str(e),\n                error_type=type(e).__name__\n            )\n\n    async def _handle_email_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Handle email sending job\"\"\"\n        # Email logic here\n        await asyncio.sleep(0.5)  # Simulate email send\n\n        return JobResult(\n            success=True,\n            result={\"email_sent\": True, \"recipient\": job.kwargs.get('to')}\n        )\n\n    async def _handle_image_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Handle image processing job\"\"\"\n        image_url = job.kwargs.get('image_url')\n        operations = job.kwargs.get('operations', ['resize'])\n\n        # Simulate image processing\n        await asyncio.sleep(2.0)  # Image processing takes longer\n\n        return JobResult(\n            success=True,\n            result={\n                \"image_processed\": True,\n                \"image_url\": image_url,\n                \"operations\": operations,\n                \"output_url\": f\"https://cdn.example.com/processed/{job.id}.jpg\"\n            }\n        )\n\n    async def _handle_sync_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Handle data synchronization job\"\"\"\n        source = job.kwargs.get('source')\n        destination = job.kwargs.get('destination')\n\n        # Simulate data sync via HTTP API\n        async with self.http_client.get(f\"https://api.{source}.com/data\") as response:\n            if response.status_code == 200:\n                data = response.json()\n\n                # Sync to destination\n                sync_response = await self.http_client.post(\n                    f\"https://api.{destination}.com/sync\",\n                    json=data\n                )\n\n                return JobResult(\n                    success=True,\n                    result={\n                        \"synced\": True,\n                        \"records_count\": len(data.get('records', [])),\n                        \"source\": source,\n                        \"destination\": destination\n                    }\n                )\n\n        return JobResult(\n            success=False,\n            error=\"Failed to sync data\"\n        )\n\n    async def _handle_report_job(self, job: Job) -&gt; JobResult:\n        \"\"\"Handle report generation job\"\"\"\n        report_type = job.kwargs.get('type')\n        date_range = job.kwargs.get('date_range')\n\n        # Simulate report generation\n        await asyncio.sleep(5.0)  # Reports take time to generate\n\n        return JobResult(\n            success=True,\n            result={\n                \"report_generated\": True,\n                \"report_type\": report_type,\n                \"date_range\": date_range,\n                \"download_url\": f\"https://reports.example.com/{job.id}.pdf\"\n            }\n        )\n\n# Advanced usage example\nasync def run_advanced_jobs():\n    # Use Redis for job persistence\n    redis_queue = RedisJobQueue(\n        host=\"localhost\",\n        port=6379,\n        db=0,\n        queue_name=\"pythia_jobs\"\n    )\n\n    processor = MultiTypeJobProcessor()\n\n    worker = BackgroundJobWorker(\n        queue=redis_queue,\n        processor=processor,\n        max_concurrent_jobs=10,\n        polling_interval=0.5\n    )\n\n    # Submit various job types with different priorities\n    jobs = [\n        # High priority email\n        {\n            'name': 'send_email',\n            'func': 'send_email',\n            'priority': JobPriority.HIGH,\n            'kwargs': {\n                'to': 'urgent@example.com',\n                'subject': 'Urgent: System Alert',\n                'body': 'Critical system alert requiring immediate attention.'\n            }\n        },\n\n        # Normal priority image processing\n        {\n            'name': 'process_image',\n            'func': 'process_image',\n            'priority': JobPriority.NORMAL,\n            'kwargs': {\n                'image_url': 'https://example.com/upload/image1.jpg',\n                'operations': ['resize', 'compress', 'watermark']\n            }\n        },\n\n        # Low priority data sync\n        {\n            'name': 'sync_data',\n            'func': 'sync_data',\n            'priority': JobPriority.LOW,\n            'kwargs': {\n                'source': 'shopify',\n                'destination': 'warehouse'\n            }\n        },\n\n        # Scheduled report generation\n        {\n            'name': 'generate_report',\n            'func': 'generate_report',\n            'scheduled_at': datetime.now() + timedelta(minutes=5),  # Run in 5 minutes\n            'kwargs': {\n                'type': 'monthly_sales',\n                'date_range': '2024-01-01 to 2024-01-31'\n            }\n        }\n    ]\n\n    # Submit jobs\n    submitted_jobs = []\n    for job_data in jobs:\n        job = await worker.submit_job(**job_data)\n        submitted_jobs.append(job)\n        print(f\"\ud83d\udcdd Submitted job: {job.name} (ID: {job.id})\")\n\n    # Start worker\n    print(\"\ud83d\ude80 Starting multi-type job worker...\")\n\n    # Monitor jobs\n    async def monitor_jobs():\n        while True:\n            stats = await worker.get_queue_stats()\n            print(f\"\ud83d\udcca Queue: {stats['queue_size']} | Active: {stats['active_jobs']} | Processed: {stats['worker_stats']['jobs_processed']}\")\n\n            # Check individual job status\n            for job in submitted_jobs:\n                status = await worker.get_job_status(job.id)\n                if status:\n                    print(f\"   Job {job.name}: {status.value}\")\n\n            await asyncio.sleep(5)\n\n    # Run worker and monitor concurrently\n    monitor_task = asyncio.create_task(monitor_jobs())\n    worker_task = asyncio.create_task(worker.run())\n\n    # Run for demo period\n    await asyncio.sleep(30)\n\n    # Cleanup\n    monitor_task.cancel()\n    await worker.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_jobs())\n</code></pre>"},{"location":"examples/background-jobs/#scheduled-jobs","title":"Scheduled Jobs","text":""},{"location":"examples/background-jobs/#cron-like-job-scheduling","title":"Cron-Like Job Scheduling","text":"<pre><code>import asyncio\nfrom datetime import datetime, timedelta\nfrom pythia.jobs import BackgroundJobWorker, JobProcessor, Job, JobResult, JobPriority\nfrom pythia.jobs.scheduler import JobScheduler\n\nclass ScheduledJobProcessor(JobProcessor):\n    \"\"\"Handle scheduled maintenance jobs\"\"\"\n\n    async def process(self, job: Job) -&gt; JobResult:\n        \"\"\"Process scheduled jobs\"\"\"\n        job_type = job.name\n\n        if job_type == 'cleanup_logs':\n            return await self._cleanup_old_logs()\n        elif job_type == 'backup_database':\n            return await self._backup_database()\n        elif job_type == 'send_daily_report':\n            return await self._send_daily_report()\n        elif job_type == 'health_check':\n            return await self._perform_health_check()\n\n        return JobResult(success=False, error=f\"Unknown scheduled job: {job_type}\")\n\n    async def _cleanup_old_logs(self) -&gt; JobResult:\n        \"\"\"Clean up log files older than 30 days\"\"\"\n        # Simulate log cleanup\n        await asyncio.sleep(2.0)\n\n        deleted_files = 15\n        freed_space = \"2.5GB\"\n\n        return JobResult(\n            success=True,\n            result={\n                \"task\": \"log_cleanup\",\n                \"deleted_files\": deleted_files,\n                \"freed_space\": freed_space\n            }\n        )\n\n    async def _backup_database(self) -&gt; JobResult:\n        \"\"\"Perform database backup\"\"\"\n        # Simulate database backup\n        await asyncio.sleep(10.0)\n\n        backup_file = f\"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql\"\n\n        return JobResult(\n            success=True,\n            result={\n                \"task\": \"database_backup\",\n                \"backup_file\": backup_file,\n                \"size\": \"1.2GB\"\n            }\n        )\n\n    async def _send_daily_report(self) -&gt; JobResult:\n        \"\"\"Send daily report to stakeholders\"\"\"\n        # Simulate report generation and sending\n        await asyncio.sleep(3.0)\n\n        return JobResult(\n            success=True,\n            result={\n                \"task\": \"daily_report\",\n                \"recipients\": [\"manager@example.com\", \"team@example.com\"],\n                \"metrics_included\": [\"users\", \"revenue\", \"errors\"]\n            }\n        )\n\n    async def _perform_health_check(self) -&gt; JobResult:\n        \"\"\"Perform system health check\"\"\"\n        # Simulate health check\n        await asyncio.sleep(1.0)\n\n        return JobResult(\n            success=True,\n            result={\n                \"task\": \"health_check\",\n                \"status\": \"healthy\",\n                \"services_checked\": [\"database\", \"cache\", \"api\", \"queue\"]\n            }\n        )\n\n# Scheduled jobs example\nasync def run_scheduled_jobs():\n    processor = ScheduledJobProcessor()\n\n    worker = BackgroundJobWorker(\n        processor=processor,\n        max_concurrent_jobs=3,\n        polling_interval=1.0\n    )\n\n    # Schedule recurring jobs\n    now = datetime.now()\n\n    scheduled_jobs = [\n        # Health check every 5 minutes\n        {\n            'name': 'health_check',\n            'func': 'health_check',\n            'scheduled_at': now + timedelta(minutes=1),\n            'recurring': True,\n            'interval': timedelta(minutes=5)\n        },\n\n        # Log cleanup daily at 2 AM\n        {\n            'name': 'cleanup_logs',\n            'func': 'cleanup_logs',\n            'scheduled_at': now.replace(hour=2, minute=0, second=0) + timedelta(days=1),\n            'recurring': True,\n            'interval': timedelta(days=1)\n        },\n\n        # Database backup daily at 3 AM\n        {\n            'name': 'backup_database',\n            'func': 'backup_database',\n            'priority': JobPriority.HIGH,\n            'scheduled_at': now.replace(hour=3, minute=0, second=0) + timedelta(days=1),\n            'recurring': True,\n            'interval': timedelta(days=1)\n        },\n\n        # Daily report at 9 AM\n        {\n            'name': 'send_daily_report',\n            'func': 'send_daily_report',\n            'scheduled_at': now.replace(hour=9, minute=0, second=0) + timedelta(days=1),\n            'recurring': True,\n            'interval': timedelta(days=1)\n        }\n    ]\n\n    # Submit scheduled jobs\n    for job_data in scheduled_jobs:\n        job = await worker.submit_job(**job_data)\n        print(f\"\ud83d\udcc5 Scheduled job: {job.name} at {job.scheduled_at}\")\n\n    print(\"\u23f0 Starting scheduled job worker...\")\n    await worker.run()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_scheduled_jobs())\n</code></pre>"},{"location":"examples/background-jobs/#job-monitoring-management","title":"Job Monitoring &amp; Management","text":""},{"location":"examples/background-jobs/#job-status-dashboard","title":"Job Status Dashboard","text":"<pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom pythia.jobs import BackgroundJobWorker, JobStatus\n\nclass JobManager:\n    \"\"\"Manage and monitor background jobs\"\"\"\n\n    def __init__(self, worker: BackgroundJobWorker):\n        self.worker = worker\n\n    async def get_job_summary(self) -&gt; dict:\n        \"\"\"Get summary of all jobs\"\"\"\n        stats = await self.worker.get_queue_stats()\n\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"queue_stats\": stats,\n            \"worker_health\": await self.worker.health_check()\n        }\n\n    async def list_jobs_by_status(self, status: JobStatus = None) -&gt; list:\n        \"\"\"List jobs filtered by status\"\"\"\n        # This would require extending the job queue interface\n        # For demo purposes, we'll simulate\n        jobs = [\n            {\n                \"id\": \"job_001\",\n                \"name\": \"send_email\",\n                \"status\": JobStatus.COMPLETED.value,\n                \"created_at\": \"2024-01-15T10:30:00Z\",\n                \"completed_at\": \"2024-01-15T10:30:02Z\"\n            },\n            {\n                \"id\": \"job_002\",\n                \"name\": \"process_image\",\n                \"status\": JobStatus.RUNNING.value,\n                \"created_at\": \"2024-01-15T10:31:00Z\",\n                \"started_at\": \"2024-01-15T10:31:01Z\"\n            },\n            {\n                \"id\": \"job_003\",\n                \"name\": \"generate_report\",\n                \"status\": JobStatus.PENDING.value,\n                \"created_at\": \"2024-01-15T10:32:00Z\",\n                \"scheduled_at\": \"2024-01-15T11:00:00Z\"\n            }\n        ]\n\n        if status:\n            jobs = [job for job in jobs if job[\"status\"] == status.value]\n\n        return jobs\n\n    async def retry_failed_jobs(self) -&gt; int:\n        \"\"\"Retry all failed jobs that can be retried\"\"\"\n        failed_jobs = await self.list_jobs_by_status(JobStatus.FAILED)\n        retried_count = 0\n\n        for job_data in failed_jobs:\n            success = await self.worker.retry_job(job_data[\"id\"])\n            if success:\n                retried_count += 1\n\n        return retried_count\n\n    async def cancel_pending_jobs(self, job_names: list = None) -&gt; int:\n        \"\"\"Cancel pending jobs, optionally filtered by name\"\"\"\n        pending_jobs = await self.list_jobs_by_status(JobStatus.PENDING)\n        cancelled_count = 0\n\n        for job_data in pending_jobs:\n            if job_names is None or job_data[\"name\"] in job_names:\n                success = await self.worker.cancel_job(job_data[\"id\"])\n                if success:\n                    cancelled_count += 1\n\n        return cancelled_count\n\n    async def monitor_continuously(self, interval: int = 10):\n        \"\"\"Continuously monitor job worker\"\"\"\n        print(\"\ud83d\udcca Starting continuous job monitoring...\")\n\n        while True:\n            try:\n                summary = await self.get_job_summary()\n\n                print(f\"\\n--- Job Summary at {summary['timestamp']} ---\")\n                print(f\"Queue Size: {summary['queue_stats']['queue_size']}\")\n                print(f\"Active Jobs: {summary['queue_stats']['active_jobs']}\")\n                print(f\"Available Slots: {summary['queue_stats']['available_slots']}\")\n                print(f\"Jobs Processed: {summary['queue_stats']['worker_stats']['jobs_processed']}\")\n                print(f\"Jobs Failed: {summary['queue_stats']['worker_stats']['jobs_failed']}\")\n                print(f\"Worker Health: {'\u2705 Healthy' if summary['worker_health'] else '\u274c Unhealthy'}\")\n\n                # Show jobs by status\n                for status in [JobStatus.RUNNING, JobStatus.PENDING, JobStatus.FAILED]:\n                    jobs = await self.list_jobs_by_status(status)\n                    if jobs:\n                        print(f\"{status.value.upper()}: {len(jobs)} jobs\")\n                        for job in jobs[:3]:  # Show first 3\n                            print(f\"  - {job['name']} (ID: {job['id']})\")\n\n                await asyncio.sleep(interval)\n\n            except KeyboardInterrupt:\n                break\n            except Exception as e:\n                print(f\"\u274c Monitor error: {e}\")\n                await asyncio.sleep(interval)\n\n# Usage example\nasync def run_job_monitoring():\n    # Create worker\n    processor = MultiTypeJobProcessor()\n    worker = BackgroundJobWorker(processor=processor)\n\n    # Create job manager\n    manager = JobManager(worker)\n\n    # Submit some test jobs\n    test_jobs = [\n        {'name': 'send_email', 'func': 'send_email', 'kwargs': {'to': 'test@example.com'}},\n        {'name': 'process_image', 'func': 'process_image', 'kwargs': {'image_url': 'test.jpg'}},\n        {'name': 'generate_report', 'func': 'generate_report', 'kwargs': {'type': 'daily'}},\n    ]\n\n    for job_data in test_jobs:\n        await worker.submit_job(**job_data)\n\n    # Start worker and monitoring\n    worker_task = asyncio.create_task(worker.run())\n    monitor_task = asyncio.create_task(manager.monitor_continuously())\n\n    # Run for demo\n    await asyncio.sleep(30)\n\n    # Cleanup\n    monitor_task.cancel()\n    await worker.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_job_monitoring())\n</code></pre>"},{"location":"examples/background-jobs/#best-practices","title":"Best Practices","text":""},{"location":"examples/background-jobs/#1-job-design","title":"1. Job Design","text":"<pre><code># Good: Specific, focused jobs\nclass EmailJob(JobProcessor):\n    async def process(self, job: Job) -&gt; JobResult:\n        # Single responsibility: send email\n        pass\n\n# Good: Idempotent jobs\nclass DataSyncJob(JobProcessor):\n    async def process(self, job: Job) -&gt; JobResult:\n        # Check if sync already completed\n        if await self._already_synced(job.kwargs['sync_id']):\n            return JobResult(success=True, result=\"Already synced\")\n        # Proceed with sync...\n</code></pre>"},{"location":"examples/background-jobs/#2-error-handling","title":"2. Error Handling","text":"<pre><code>class RobustJobProcessor(JobProcessor):\n    async def process(self, job: Job) -&gt; JobResult:\n        try:\n            # Job logic here\n            result = await self._do_work(job)\n            return JobResult(success=True, result=result)\n\n        except TemporaryError as e:\n            # Retryable error\n            return JobResult(\n                success=False,\n                error=str(e),\n                retryable=True\n            )\n\n        except PermanentError as e:\n            # Non-retryable error\n            return JobResult(\n                success=False,\n                error=str(e),\n                retryable=False\n            )\n</code></pre>"},{"location":"examples/background-jobs/#3-resource-management","title":"3. Resource Management","text":"<pre><code>class ResourceManagedProcessor(JobProcessor):\n    def __init__(self):\n        self.db_pool = None\n        self.http_client = None\n\n    async def startup(self):\n        \"\"\"Initialize resources\"\"\"\n        self.db_pool = await create_db_pool()\n        self.http_client = httpx.AsyncClient()\n\n    async def shutdown(self):\n        \"\"\"Cleanup resources\"\"\"\n        if self.db_pool:\n            await self.db_pool.close()\n        if self.http_client:\n            await self.http_client.aclose()\n\n    async def process(self, job: Job) -&gt; JobResult:\n        # Use managed resources\n        async with self.db_pool.acquire() as conn:\n            # Database operations\n            pass\n</code></pre>"},{"location":"examples/background-jobs/#testing-background-jobs","title":"Testing Background Jobs","text":"<pre><code>import pytest\nfrom pythia.jobs import Job, JobResult, JobPriority\nfrom pythia.jobs.queue import MemoryJobQueue\n\n@pytest.mark.asyncio\nclass TestEmailJobProcessor:\n    async def test_successful_email_job(self):\n        \"\"\"Test successful email processing\"\"\"\n        processor = EmailJobProcessor({})\n\n        job = Job(\n            name=\"test_email\",\n            func=\"send_email\",\n            kwargs={\n                \"to\": \"test@example.com\",\n                \"subject\": \"Test\",\n                \"body\": \"Test body\"\n            }\n        )\n\n        result = await processor.process(job)\n\n        assert result.success is True\n        assert result.result[\"email_sent\"] is True\n        assert result.result[\"recipient\"] == \"test@example.com\"\n\n    async def test_missing_email_parameters(self):\n        \"\"\"Test job with missing parameters\"\"\"\n        processor = EmailJobProcessor({})\n\n        job = Job(\n            name=\"test_email\",\n            func=\"send_email\",\n            kwargs={\"to\": \"test@example.com\"}  # Missing subject and body\n        )\n\n        result = await processor.process(job)\n\n        assert result.success is False\n        assert \"Missing required email parameters\" in result.error\n\n@pytest.mark.asyncio\nasync def test_job_queue_operations():\n    \"\"\"Test job queue operations\"\"\"\n    queue = MemoryJobQueue()\n\n    # Create test job\n    job = Job(\n        name=\"test_job\",\n        func=\"test_func\",\n        priority=JobPriority.HIGH\n    )\n\n    # Test put/get\n    await queue.put(job)\n    assert await queue.size() == 1\n\n    retrieved_job = await queue.get()\n    assert retrieved_job.id == job.id\n    assert retrieved_job.priority == JobPriority.HIGH\n</code></pre>"},{"location":"examples/background-jobs/#next-steps","title":"Next Steps","text":"<ul> <li>HTTP Workers - HTTP polling and webhook workers</li> <li>Message Workers - Message-based worker patterns</li> <li>Worker Lifecycle - Understanding worker management</li> </ul>"},{"location":"examples/basic-worker/","title":"Basic Worker Examples","text":"<p>Collection of basic worker examples demonstrating different patterns and use cases.</p>"},{"location":"examples/basic-worker/#message-processing-worker","title":"Message Processing Worker","text":"<p>Simple message processing worker for handling events:</p> <pre><code>import asyncio\nimport json\nfrom typing import Any, Dict\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\nfrom pythia.config.redis import RedisConfig\n\nclass EventProcessor(Worker):\n    \"\"\"Basic event processing worker\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process a single event\"\"\"\n        try:\n            # Parse event data\n            event_data = json.loads(message.body)\n\n            self.logger.info(f\"Processing event: {event_data.get('type')}\")\n\n            # Process based on event type\n            if event_data['type'] == 'user_registered':\n                return await self._handle_user_registration(event_data)\n            elif event_data['type'] == 'order_created':\n                return await self._handle_order_creation(event_data)\n            else:\n                return await self._handle_generic_event(event_data)\n\n        except Exception as e:\n            self.logger.error(f\"Failed to process event: {e}\")\n            raise\n\n    async def _handle_user_registration(self, event: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle user registration event\"\"\"\n        user_id = event['data']['user_id']\n        email = event['data']['email']\n\n        # Send welcome email (simulate)\n        await asyncio.sleep(0.1)\n\n        return {\n            \"status\": \"processed\",\n            \"action\": \"welcome_email_sent\",\n            \"user_id\": user_id,\n            \"email\": email\n        }\n\n    async def _handle_order_creation(self, event: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle order creation event\"\"\"\n        order_id = event['data']['order_id']\n\n        # Process order (simulate)\n        await asyncio.sleep(0.2)\n\n        return {\n            \"status\": \"processed\",\n            \"action\": \"order_confirmed\",\n            \"order_id\": order_id\n        }\n\n    async def _handle_generic_event(self, event: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle generic events\"\"\"\n        return {\n            \"status\": \"processed\",\n            \"action\": \"logged\",\n            \"event_type\": event.get('type', 'unknown')\n        }\n\n# Configuration and startup\nasync def main():\n    # Redis configuration\n    redis_config = RedisConfig(\n        host=\"localhost\",\n        port=6379,\n        queue=\"events-queue\"\n    )\n\n    # Worker configuration\n    config = WorkerConfig(\n        worker_name=\"event-processor\",\n        broker_type=\"redis\",\n        max_concurrent=5,\n        log_level=\"INFO\"\n    )\n\n    # Create and start worker\n    worker = EventProcessor(config=config)\n    await worker.start()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-worker/#background-job-worker","title":"Background Job Worker","text":"<p>Background job processing with retry logic:</p> <pre><code>import asyncio\nimport smtplib\nfrom email.mime.text import MIMEText\nfrom typing import Dict, Any\nfrom pythia.jobs import BackgroundJobWorker, JobProcessor, Job, JobResult\nfrom pythia.jobs.queue import MemoryJobQueue\nfrom pythia.config import WorkerConfig\n\nclass EmailJobProcessor(JobProcessor):\n    \"\"\"Process email sending jobs\"\"\"\n\n    def __init__(self):\n        self.smtp_server = \"smtp.example.com\"\n        self.smtp_port = 587\n\n    async def process(self, job: Job) -&gt; JobResult:\n        \"\"\"Process email job\"\"\"\n        try:\n            # Extract job data\n            email_data = job.kwargs\n\n            # Send email\n            await self._send_email(\n                to_email=email_data['to'],\n                subject=email_data['subject'],\n                body=email_data['body']\n            )\n\n            return JobResult(\n                success=True,\n                result={\"email_sent\": True, \"recipient\": email_data['to']}\n            )\n\n        except Exception as e:\n            return JobResult(\n                success=False,\n                error=str(e),\n                error_type=type(e).__name__\n            )\n\n    async def _send_email(self, to_email: str, subject: str, body: str):\n        \"\"\"Send email via SMTP (simulated)\"\"\"\n        # Simulate email sending\n        await asyncio.sleep(0.5)\n\n        print(f\"\ud83d\udce7 Sending email to {to_email}\")\n        print(f\"   Subject: {subject}\")\n        print(f\"   Body: {body[:50]}...\")\n\n# Example usage\nasync def run_email_worker():\n    \"\"\"Run email background job worker\"\"\"\n\n    # Create job queue\n    queue = MemoryJobQueue()\n\n    # Create email processor\n    processor = EmailJobProcessor()\n\n    # Create worker\n    worker = BackgroundJobWorker(\n        queue=queue,\n        processor=processor,\n        max_concurrent_jobs=10,\n        polling_interval=1.0\n    )\n\n    # Submit some test jobs\n    await worker.submit_job(\n        name=\"welcome_email\",\n        func=\"send_email\",\n        kwargs={\n            \"to\": \"user@example.com\",\n            \"subject\": \"Welcome!\",\n            \"body\": \"Thanks for joining our service!\"\n        }\n    )\n\n    await worker.submit_job(\n        name=\"password_reset\",\n        func=\"send_email\",\n        kwargs={\n            \"to\": \"user2@example.com\",\n            \"subject\": \"Password Reset\",\n            \"body\": \"Click here to reset your password\"\n        }\n    )\n\n    # Start processing\n    print(\"\ud83d\ude80 Starting email worker...\")\n    await worker.run()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_email_worker())\n</code></pre>"},{"location":"examples/basic-worker/#http-polling-worker","title":"HTTP Polling Worker","text":"<p>Polling external APIs for data:</p> <pre><code>import asyncio\nimport json\nfrom typing import Dict, Any, List\nfrom pythia.core import Worker, Message\nfrom pythia.http.poller import HTTPPoller\nfrom pythia.config import WorkerConfig\n\nclass APIPollingWorker(Worker):\n    \"\"\"Worker that polls external APIs\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup HTTP poller\n        self.poller = HTTPPoller(\n            url=\"https://api.github.com/repos/python/cpython/events\",\n            interval=300,  # Poll every 5 minutes\n            method=\"GET\",\n            headers={\"Accept\": \"application/vnd.github.v3+json\"}\n        )\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process polled API data\"\"\"\n        try:\n            # Parse GitHub events\n            events = json.loads(message.body)\n\n            processed_events = []\n            for event in events[:5]:  # Process first 5 events\n                processed_event = await self._process_github_event(event)\n                processed_events.append(processed_event)\n\n            return {\n                \"status\": \"processed\",\n                \"events_processed\": len(processed_events),\n                \"events\": processed_events\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Failed to process API data: {e}\")\n            raise\n\n    async def _process_github_event(self, event: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process a single GitHub event\"\"\"\n        return {\n            \"id\": event.get(\"id\"),\n            \"type\": event.get(\"type\"),\n            \"actor\": event.get(\"actor\", {}).get(\"login\"),\n            \"repo\": event.get(\"repo\", {}).get(\"name\"),\n            \"created_at\": event.get(\"created_at\")\n        }\n\n    async def start_polling(self):\n        \"\"\"Start the HTTP poller\"\"\"\n        async for message in self.poller.consume():\n            await self.process_message(message)\n\n# Run the polling worker\nasync def main():\n    config = WorkerConfig(\n        worker_name=\"github-poller\",\n        log_level=\"INFO\"\n    )\n\n    worker = APIPollingWorker(config)\n    await worker.start_polling()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-worker/#webhook-processing-worker","title":"Webhook Processing Worker","text":"<p>Processing incoming webhooks:</p> <pre><code>import asyncio\nimport json\nimport hmac\nimport hashlib\nfrom typing import Dict, Any\nfrom pythia.core import Worker, Message\nfrom pythia.http.webhook import WebhookClient\nfrom pythia.config import WorkerConfig\n\nclass WebhookProcessor(Worker):\n    \"\"\"Process incoming webhook events\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup webhook client for responses\n        self.webhook_client = WebhookClient(\n            base_url=\"https://api.partner.com\"\n        )\n\n        self.webhook_secret = \"your-webhook-secret\"\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process webhook message\"\"\"\n        try:\n            # Extract webhook data\n            webhook_data = json.loads(message.body)\n\n            # Verify webhook signature (if provided)\n            if not await self._verify_webhook_signature(message):\n                raise ValueError(\"Invalid webhook signature\")\n\n            # Process based on webhook type\n            event_type = webhook_data.get(\"type\")\n\n            if event_type == \"payment.completed\":\n                return await self._handle_payment_completed(webhook_data)\n            elif event_type == \"user.updated\":\n                return await self._handle_user_updated(webhook_data)\n            else:\n                return await self._handle_unknown_webhook(webhook_data)\n\n        except Exception as e:\n            self.logger.error(f\"Webhook processing failed: {e}\")\n            raise\n\n    async def _verify_webhook_signature(self, message: Message) -&gt; bool:\n        \"\"\"Verify webhook signature\"\"\"\n        signature = message.headers.get(\"X-Webhook-Signature\")\n        if not signature:\n            return True  # Skip verification if no signature\n\n        # Calculate expected signature\n        expected = hmac.new(\n            self.webhook_secret.encode(),\n            message.body.encode(),\n            hashlib.sha256\n        ).hexdigest()\n\n        return hmac.compare_digest(signature, expected)\n\n    async def _handle_payment_completed(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle payment completion webhook\"\"\"\n        payment_id = data[\"data\"][\"payment_id\"]\n        amount = data[\"data\"][\"amount\"]\n\n        self.logger.info(f\"Payment completed: {payment_id} for ${amount}\")\n\n        # Send confirmation webhook\n        confirmation_sent = await self.webhook_client.send(\n            \"/payment-confirmation\",\n            {\n                \"payment_id\": payment_id,\n                \"status\": \"processed\",\n                \"processed_at\": data.get(\"created_at\")\n            }\n        )\n\n        return {\n            \"status\": \"processed\",\n            \"event_type\": \"payment_completed\",\n            \"payment_id\": payment_id,\n            \"confirmation_sent\": confirmation_sent\n        }\n\n    async def _handle_user_updated(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle user update webhook\"\"\"\n        user_id = data[\"data\"][\"user_id\"]\n\n        # Process user update\n        await self._sync_user_data(user_id, data[\"data\"])\n\n        return {\n            \"status\": \"processed\",\n            \"event_type\": \"user_updated\",\n            \"user_id\": user_id\n        }\n\n    async def _handle_unknown_webhook(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle unknown webhook types\"\"\"\n        self.logger.warning(f\"Unknown webhook type: {data.get('type')}\")\n\n        return {\n            \"status\": \"logged\",\n            \"event_type\": \"unknown\",\n            \"webhook_type\": data.get(\"type\")\n        }\n\n    async def _sync_user_data(self, user_id: str, user_data: Dict[str, Any]):\n        \"\"\"Sync user data with local database\"\"\"\n        # Simulate database update\n        await asyncio.sleep(0.1)\n        self.logger.info(f\"Synced user data for {user_id}\")\n\n# Usage example\nasync def main():\n    config = WorkerConfig(\n        worker_name=\"webhook-processor\",\n        broker_type=\"kafka\",  # Webhooks received via Kafka\n        log_level=\"INFO\"\n    )\n\n    worker = WebhookProcessor(config)\n    await worker.start()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-worker/#batch-processing-worker","title":"Batch Processing Worker","text":"<p>Processing messages in batches for efficiency:</p> <pre><code>import asyncio\nimport json\nfrom typing import List, Dict, Any\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\n\nclass BatchProcessor(Worker):\n    \"\"\"Process messages in batches for better performance\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.batch_size = 50\n        self.batch_timeout = 30  # seconds\n        self.message_buffer = []\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Add message to batch buffer\"\"\"\n        self.message_buffer.append(message)\n\n        # Process batch when full or timeout reached\n        if len(self.message_buffer) &gt;= self.batch_size:\n            return await self._process_batch()\n\n        return {\"status\": \"buffered\", \"buffer_size\": len(self.message_buffer)}\n\n    async def _process_batch(self) -&gt; Dict[str, Any]:\n        \"\"\"Process accumulated batch of messages\"\"\"\n        if not self.message_buffer:\n            return {\"status\": \"no_messages\", \"processed\": 0}\n\n        batch = self.message_buffer.copy()\n        self.message_buffer.clear()\n\n        self.logger.info(f\"Processing batch of {len(batch)} messages\")\n\n        try:\n            # Extract all data\n            batch_data = []\n            for message in batch:\n                data = json.loads(message.body)\n                batch_data.append(data)\n\n            # Process batch efficiently (e.g., bulk database operation)\n            results = await self._bulk_process(batch_data)\n\n            return {\n                \"status\": \"processed\",\n                \"batch_size\": len(batch),\n                \"processed\": len(results),\n                \"results\": results\n            }\n\n        except Exception as e:\n            self.logger.error(f\"Batch processing failed: {e}\")\n            # Put messages back in buffer for retry\n            self.message_buffer.extend(batch)\n            raise\n\n    async def _bulk_process(self, data: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Bulk process the batch data\"\"\"\n        # Simulate bulk database operation\n        await asyncio.sleep(0.5)  # Simulate processing time\n\n        results = []\n        for item in data:\n            results.append({\n                \"id\": item.get(\"id\"),\n                \"status\": \"processed\",\n                \"processed_at\": asyncio.get_event_loop().time()\n            })\n\n        return results\n\n    async def on_shutdown(self):\n        \"\"\"Process remaining messages on shutdown\"\"\"\n        if self.message_buffer:\n            self.logger.info(\"Processing remaining messages on shutdown\")\n            await self._process_batch()\n\n        await super().on_shutdown()\n\n# Usage\nasync def main():\n    config = WorkerConfig(\n        worker_name=\"batch-processor\",\n        broker_type=\"redis\",\n        max_concurrent=1,  # Single-threaded for batch processing\n        log_level=\"INFO\"\n    )\n\n    worker = BatchProcessor(config)\n    await worker.start()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/basic-worker/#testing-workers","title":"Testing Workers","text":"<p>Basic testing patterns for workers:</p> <pre><code>import pytest\nimport json\nfrom pythia.utils.testing import WorkerTestCase\nfrom pythia.core import Message\n\nclass TestEventProcessor(WorkerTestCase):\n    worker_class = EventProcessor\n\n    async def test_user_registration_event(self):\n        \"\"\"Test user registration event processing\"\"\"\n        event_data = {\n            \"type\": \"user_registered\",\n            \"data\": {\n                \"user_id\": \"123\",\n                \"email\": \"test@example.com\"\n            }\n        }\n\n        message = self.create_test_message(json.dumps(event_data))\n        result = await self.worker.process_message(message)\n\n        assert result[\"status\"] == \"processed\"\n        assert result[\"action\"] == \"welcome_email_sent\"\n        assert result[\"user_id\"] == \"123\"\n\n    async def test_unknown_event_type(self):\n        \"\"\"Test handling of unknown event types\"\"\"\n        event_data = {\n            \"type\": \"unknown_event\",\n            \"data\": {}\n        }\n\n        message = self.create_test_message(json.dumps(event_data))\n        result = await self.worker.process_message(message)\n\n        assert result[\"status\"] == \"processed\"\n        assert result[\"action\"] == \"logged\"\n        assert result[\"event_type\"] == \"unknown_event\"\n\n# Run tests\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n</code></pre>"},{"location":"examples/basic-worker/#next-steps","title":"Next Steps","text":"<ul> <li>Redis Worker - Redis-specific worker examples</li> <li>Kafka Worker - Kafka-specific worker examples</li> <li>RabbitMQ Worker - RabbitMQ-specific worker examples</li> <li>User Guide - Understanding worker lifecycle</li> </ul>"},{"location":"examples/database-cdc/","title":"Database CDC Worker Example","text":"<p>This example demonstrates how to create a Change Data Capture (CDC) worker that monitors PostgreSQL database changes in real-time.</p>"},{"location":"examples/database-cdc/#scenario","title":"Scenario","text":"<p>We'll build an e-commerce system that: - Monitors order changes in PostgreSQL - Sends email confirmations for new orders - Updates inventory when orders are placed - Tracks order status changes</p>"},{"location":"examples/database-cdc/#code","title":"Code","text":"<pre><code>import asyncio\nfrom typing import Any\nfrom pythia.brokers.database import PostgreSQLCDCWorker, DatabaseChange, ChangeType\n\nclass ECommerceCDCWorker(PostgreSQLCDCWorker):\n    \"\"\"\n    E-commerce CDC worker that processes order changes\n    \"\"\"\n\n    def __init__(self):\n        super().__init__(\n            connection_string=\"postgresql://user:password@localhost:5432/ecommerce\",\n            tables=[\"orders\", \"order_items\", \"inventory\"],\n            slot_name=\"ecommerce_cdc_slot\",\n            publication_name=\"ecommerce_publication\"\n        )\n\n    async def process_change(self, change: DatabaseChange) -&gt; Any:\n        \"\"\"Process database changes based on table and operation\"\"\"\n\n        if change.table == \"orders\":\n            return await self._process_order_change(change)\n        elif change.table == \"order_items\":\n            return await self._process_order_item_change(change)\n        elif change.table == \"inventory\":\n            return await self._process_inventory_change(change)\n\n        return {\"processed\": False, \"reason\": \"Unknown table\"}\n\n    async def _process_order_change(self, change: DatabaseChange) -&gt; dict:\n        \"\"\"Process order table changes\"\"\"\n\n        if change.change_type == ChangeType.INSERT:\n            # New order created\n            order_data = change.new_data\n            self.logger.info(f\"New order created: {order_data.get('id')}\")\n\n            # Send order confirmation email\n            await self._send_order_confirmation(order_data)\n\n            # Update inventory for ordered items\n            await self._reserve_inventory(order_data['id'])\n\n            return {\n                \"action\": \"new_order_processed\",\n                \"order_id\": order_data.get('id'),\n                \"customer_email\": order_data.get('customer_email'),\n                \"total_amount\": order_data.get('total_amount')\n            }\n\n        elif change.change_type == ChangeType.UPDATE:\n            # Order status changed\n            old_status = change.old_data.get('status')\n            new_status = change.new_data.get('status')\n\n            if old_status != new_status:\n                self.logger.info(f\"Order {change.new_data.get('id')} status: {old_status} \u2192 {new_status}\")\n                await self._handle_status_change(change.new_data, old_status, new_status)\n\n            return {\n                \"action\": \"status_change\",\n                \"order_id\": change.new_data.get('id'),\n                \"old_status\": old_status,\n                \"new_status\": new_status\n            }\n\n        elif change.change_type == ChangeType.DELETE:\n            # Order cancelled/deleted\n            order_data = change.old_data\n            self.logger.info(f\"Order deleted: {order_data.get('id')}\")\n\n            # Release reserved inventory\n            await self._release_inventory(order_data['id'])\n\n            return {\n                \"action\": \"order_deleted\",\n                \"order_id\": order_data.get('id')\n            }\n\n    async def _process_order_item_change(self, change: DatabaseChange) -&gt; dict:\n        \"\"\"Process order items changes\"\"\"\n\n        if change.change_type == ChangeType.INSERT:\n            # New item added to order\n            item_data = change.new_data\n\n            # Update product analytics\n            await self._update_product_analytics(\n                product_id=item_data.get('product_id'),\n                quantity=item_data.get('quantity'),\n                action='ordered'\n            )\n\n            return {\n                \"action\": \"item_added\",\n                \"order_id\": item_data.get('order_id'),\n                \"product_id\": item_data.get('product_id'),\n                \"quantity\": item_data.get('quantity')\n            }\n\n    async def _process_inventory_change(self, change: DatabaseChange) -&gt; dict:\n        \"\"\"Process inventory changes\"\"\"\n\n        if change.change_type == ChangeType.UPDATE:\n            old_stock = change.old_data.get('stock_quantity', 0)\n            new_stock = change.new_data.get('stock_quantity', 0)\n\n            # Check for low stock alerts\n            if new_stock &lt;= change.new_data.get('low_stock_threshold', 5):\n                await self._send_low_stock_alert(change.new_data)\n\n            return {\n                \"action\": \"inventory_updated\",\n                \"product_id\": change.new_data.get('product_id'),\n                \"old_stock\": old_stock,\n                \"new_stock\": new_stock\n            }\n\n    # Business logic methods\n    async def _send_order_confirmation(self, order_data: dict):\n        \"\"\"Send order confirmation email\"\"\"\n        self.logger.info(f\"Sending confirmation email to {order_data.get('customer_email')}\")\n        # Integration with email service (SendGrid, SES, etc.)\n        # await email_service.send_confirmation(order_data)\n\n    async def _reserve_inventory(self, order_id: str):\n        \"\"\"Reserve inventory for order items\"\"\"\n        self.logger.info(f\"Reserving inventory for order {order_id}\")\n        # Update inventory reservations\n        # await inventory_service.reserve_items(order_id)\n\n    async def _release_inventory(self, order_id: str):\n        \"\"\"Release reserved inventory\"\"\"\n        self.logger.info(f\"Releasing inventory for order {order_id}\")\n        # Release inventory reservations\n        # await inventory_service.release_items(order_id)\n\n    async def _handle_status_change(self, order_data: dict, old_status: str, new_status: str):\n        \"\"\"Handle order status changes\"\"\"\n        status_handlers = {\n            'paid': self._handle_payment_received,\n            'shipped': self._handle_order_shipped,\n            'delivered': self._handle_order_delivered,\n            'cancelled': self._handle_order_cancelled\n        }\n\n        handler = status_handlers.get(new_status)\n        if handler:\n            await handler(order_data)\n\n    async def _handle_payment_received(self, order_data: dict):\n        \"\"\"Handle payment confirmation\"\"\"\n        self.logger.info(f\"Payment received for order {order_data.get('id')}\")\n        # Trigger fulfillment process\n        # await fulfillment_service.process_order(order_data['id'])\n\n    async def _handle_order_shipped(self, order_data: dict):\n        \"\"\"Handle order shipment\"\"\"\n        self.logger.info(f\"Order {order_data.get('id')} shipped\")\n        # Send shipping notification\n        # await notification_service.send_shipping_update(order_data)\n\n    async def _handle_order_delivered(self, order_data: dict):\n        \"\"\"Handle order delivery\"\"\"\n        self.logger.info(f\"Order {order_data.get('id')} delivered\")\n        # Send delivery confirmation and request review\n        # await review_service.request_review(order_data)\n\n    async def _handle_order_cancelled(self, order_data: dict):\n        \"\"\"Handle order cancellation\"\"\"\n        self.logger.info(f\"Order {order_data.get('id')} cancelled\")\n        # Process refund and release inventory\n        # await payment_service.process_refund(order_data['id'])\n\n    async def _update_product_analytics(self, product_id: str, quantity: int, action: str):\n        \"\"\"Update product analytics\"\"\"\n        # Update analytics database or send to analytics service\n        self.logger.debug(f\"Analytics: Product {product_id}, {action}, qty: {quantity}\")\n\n    async def _send_low_stock_alert(self, inventory_data: dict):\n        \"\"\"Send low stock alert\"\"\"\n        product_id = inventory_data.get('product_id')\n        current_stock = inventory_data.get('stock_quantity')\n\n        self.logger.warning(f\"Low stock alert: Product {product_id}, Stock: {current_stock}\")\n        # Send alert to inventory management team\n        # await alert_service.send_low_stock_alert(inventory_data)\n\n\nasync def main():\n    \"\"\"Run the CDC worker\"\"\"\n    worker = ECommerceCDCWorker()\n\n    try:\n        # Start the CDC worker\n        async with worker:\n            await worker.start_cdc()\n\n            print(\"\ud83d\ude80 E-commerce CDC Worker started\")\n            print(\"\ud83d\udcca Monitoring: orders, order_items, inventory\")\n            print(\"\ud83d\udce7 Features: Email confirmation, inventory tracking, analytics\")\n            print(\"Press Ctrl+C to stop...\")\n\n            # Process changes indefinitely\n            async for change in worker.consume_changes():\n                try:\n                    result = await worker.process_change(change)\n                    print(f\"\u2705 Processed {change.change_type.value} on {change.table}: {result}\")\n\n                except Exception as e:\n                    print(f\"\u274c Error processing change: {e}\")\n                    worker.logger.error(f\"Processing error: {e}\")\n\n    except KeyboardInterrupt:\n        print(\"\\n\ud83d\uded1 Worker stopped by user\")\n    except Exception as e:\n        print(f\"\ud83d\udca5 Worker error: {e}\")\n    finally:\n        await worker.stop_cdc()\n        print(\"\ud83d\udc4b CDC Worker stopped\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/database-cdc/#database-setup","title":"Database Setup","text":""},{"location":"examples/database-cdc/#postgresql-configuration","title":"PostgreSQL Configuration","text":"<ol> <li> <p>Enable logical replication in <code>postgresql.conf</code>: <pre><code>wal_level = logical\nmax_replication_slots = 10\nmax_wal_senders = 10\n</code></pre></p> </li> <li> <p>Create database and tables: <pre><code>-- Create database\nCREATE DATABASE ecommerce;\n\n-- Create tables\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_email VARCHAR(255) NOT NULL,\n    status VARCHAR(50) DEFAULT 'pending',\n    total_amount DECIMAL(10,2),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE order_items (\n    id SERIAL PRIMARY KEY,\n    order_id INTEGER REFERENCES orders(id),\n    product_id INTEGER NOT NULL,\n    quantity INTEGER NOT NULL,\n    price DECIMAL(10,2)\n);\n\nCREATE TABLE inventory (\n    product_id INTEGER PRIMARY KEY,\n    stock_quantity INTEGER NOT NULL DEFAULT 0,\n    low_stock_threshold INTEGER DEFAULT 5,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Grant permissions\nALTER USER your_user REPLICATION;\n</code></pre></p> </li> <li> <p>Test data: <pre><code>-- Insert test data\nINSERT INTO inventory (product_id, stock_quantity) VALUES\n(1, 100), (2, 50), (3, 25);\n\nINSERT INTO orders (customer_email, total_amount) VALUES\n('customer@example.com', 99.99);\n\nINSERT INTO order_items (order_id, product_id, quantity, price) VALUES\n(1, 1, 2, 49.99);\n</code></pre></p> </li> </ol>"},{"location":"examples/database-cdc/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Install dependencies: <pre><code>pip install asyncpg\n</code></pre></p> </li> <li> <p>Configure connection string: Update the connection string in the worker to match your PostgreSQL setup.</p> </li> <li> <p>Run the worker: <pre><code>python cdc_worker.py\n</code></pre></p> </li> <li> <p>Test with database changes: <pre><code>-- In another PostgreSQL session, make changes:\nUPDATE orders SET status = 'paid' WHERE id = 1;\nUPDATE orders SET status = 'shipped' WHERE id = 1;\nUPDATE inventory SET stock_quantity = 3 WHERE product_id = 1;\n</code></pre></p> </li> </ol>"},{"location":"examples/database-cdc/#expected-output","title":"Expected Output","text":"<pre><code>\ud83d\ude80 E-commerce CDC Worker started\n\ud83d\udcca Monitoring: orders, order_items, inventory\n\ud83d\udce7 Features: Email confirmation, inventory tracking, analytics\nPress Ctrl+C to stop...\n\n\u2705 Processed UPDATE on orders: {'action': 'status_change', 'order_id': 1, 'old_status': 'pending', 'new_status': 'paid'}\n\u2705 Processed UPDATE on orders: {'action': 'status_change', 'order_id': 1, 'old_status': 'paid', 'new_status': 'shipped'}\n\u2705 Processed UPDATE on inventory: {'action': 'inventory_updated', 'product_id': 1, 'old_stock': 100, 'new_stock': 3}\n</code></pre>"},{"location":"examples/database-cdc/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ol> <li>Real-time Processing: Changes are processed as they happen</li> <li>Table-specific Logic: Different processing for different tables</li> <li>Operation-specific Handling: INSERT, UPDATE, DELETE handled differently</li> <li>Business Logic Integration: Email, inventory, analytics integration points</li> <li>Error Handling: Robust error handling with logging</li> <li>Monitoring: Built-in logging and metrics</li> </ol>"},{"location":"examples/database-cdc/#extensions","title":"Extensions","text":"<p>This example can be extended with: - Integration with email services (SendGrid, AWS SES) - Real-time analytics (ClickHouse, BigQuery) - Message queues for async processing - Webhook notifications - Audit logging - Data validation and transformation</p>"},{"location":"examples/database-sync/","title":"Database Sync Worker Example","text":"<p>This example demonstrates how to create a database synchronization worker that replicates data between PostgreSQL and MySQL databases.</p>"},{"location":"examples/database-sync/#scenario","title":"Scenario","text":"<p>We'll build a data replication system that: - Syncs customer data from PostgreSQL (production) to MySQL (analytics) - Performs incremental syncs based on timestamps - Validates data integrity after sync - Handles cross-database type conversions</p>"},{"location":"examples/database-sync/#code","title":"Code","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Any\nfrom datetime import datetime, timedelta\nfrom pythia.brokers.database import DatabaseSyncWorker\n\nclass DataReplicationWorker(DatabaseSyncWorker):\n    \"\"\"\n    Production to Analytics database sync worker\n    \"\"\"\n\n    def __init__(self):\n        sync_config = {\n            'batch_size': 1000,\n            'mode': 'incremental',\n            'conflict_resolution': 'source_wins',\n            'timestamp_column': 'updated_at',\n            'truncate_target': False\n        }\n\n        super().__init__(\n            source_connection=\"postgresql://user:password@prod-db:5432/ecommerce\",\n            target_connection=\"mysql://user:password@analytics-db:3306/analytics\",\n            sync_config=sync_config\n        )\n\n        # Define tables to sync with their configurations\n        self.sync_tables = {\n            'customers': {\n                'primary_key': 'id',\n                'timestamp_column': 'updated_at',\n                'batch_size': 1000,\n                'validations': ['email_format', 'required_fields']\n            },\n            'orders': {\n                'primary_key': 'id',\n                'timestamp_column': 'updated_at',\n                'batch_size': 500,\n                'validations': ['amount_positive', 'customer_exists']\n            },\n            'products': {\n                'primary_key': 'id',\n                'timestamp_column': 'updated_at',\n                'batch_size': 2000,\n                'validations': ['price_positive']\n            },\n            'order_items': {\n                'primary_key': 'id',\n                'timestamp_column': 'created_at',  # Different timestamp column\n                'batch_size': 2000,\n                'validations': ['quantity_positive']\n            }\n        }\n\n    async def sync_all_configured_tables(self) -&gt; Dict[str, Any]:\n        \"\"\"Sync all configured tables with their specific settings\"\"\"\n\n        results = []\n        start_time = datetime.now()\n\n        for table_name, config in self.sync_tables.items():\n            try:\n                self.logger.info(f\"Starting sync for {table_name}\")\n\n                # Override timestamp column for this table\n                original_timestamp_column = self.timestamp_column\n                self.timestamp_column = config['timestamp_column']\n\n                # Override batch size for this table\n                original_batch_size = self.batch_size\n                self.batch_size = config['batch_size']\n\n                # Perform sync\n                sync_result = await self.sync_table(table_name)\n\n                # Validate sync\n                validation_result = await self.validate_sync(table_name)\n\n                # Run custom validations\n                custom_validations = await self._run_custom_validations(\n                    table_name, config['validations']\n                )\n\n                # Restore original settings\n                self.timestamp_column = original_timestamp_column\n                self.batch_size = original_batch_size\n\n                result = {\n                    **sync_result,\n                    'validation': validation_result,\n                    'custom_validations': custom_validations,\n                    'success': True\n                }\n\n                results.append(result)\n                self.logger.info(f\"\u2705 {table_name} sync completed: {sync_result['rows_synced']} rows\")\n\n            except Exception as e:\n                self.logger.error(f\"\u274c Error syncing {table_name}: {e}\")\n                results.append({\n                    'table': table_name,\n                    'error': str(e),\n                    'success': False\n                })\n\n        end_time = datetime.now()\n        total_duration = (end_time - start_time).total_seconds()\n        total_rows_synced = sum(r.get('rows_synced', 0) for r in results if 'rows_synced' in r)\n\n        summary = {\n            'total_tables': len(self.sync_tables),\n            'successful_syncs': len([r for r in results if r.get('success', False)]),\n            'failed_syncs': len([r for r in results if not r.get('success', False)]),\n            'total_rows_synced': total_rows_synced,\n            'total_duration_seconds': total_duration,\n            'start_time': start_time.isoformat(),\n            'end_time': end_time.isoformat(),\n            'results': results\n        }\n\n        return summary\n\n    async def _run_custom_validations(self, table_name: str, validations: List[str]) -&gt; Dict[str, Any]:\n        \"\"\"Run custom data validations\"\"\"\n\n        validation_results = {}\n\n        for validation in validations:\n            try:\n                if validation == 'email_format':\n                    result = await self._validate_email_format(table_name)\n                elif validation == 'required_fields':\n                    result = await self._validate_required_fields(table_name)\n                elif validation == 'amount_positive':\n                    result = await self._validate_positive_amounts(table_name)\n                elif validation == 'customer_exists':\n                    result = await self._validate_customer_references(table_name)\n                elif validation == 'price_positive':\n                    result = await self._validate_positive_prices(table_name)\n                elif validation == 'quantity_positive':\n                    result = await self._validate_positive_quantities(table_name)\n                else:\n                    result = {'status': 'skipped', 'reason': 'Unknown validation'}\n\n                validation_results[validation] = result\n\n            except Exception as e:\n                validation_results[validation] = {\n                    'status': 'error',\n                    'error': str(e)\n                }\n\n        return validation_results\n\n    async def _validate_email_format(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate email format in target database\"\"\"\n\n        if self.target_type == 'mysql':\n            async with self.target_conn.cursor() as cursor:\n                query = f\"\"\"\n                    SELECT COUNT(*) as total,\n                           SUM(CASE WHEN email REGEXP '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{{2,}}$' THEN 1 ELSE 0 END) as valid\n                    FROM {table_name}\n                \"\"\"\n                await cursor.execute(query)\n                result = await cursor.fetchone()\n\n                return {\n                    'status': 'completed',\n                    'total_rows': result[0],\n                    'valid_emails': result[1],\n                    'invalid_emails': result[0] - result[1],\n                    'valid_percentage': (result[1] / result[0] * 100) if result[0] &gt; 0 else 0\n                }\n\n        return {'status': 'skipped', 'reason': 'Only supported for MySQL target'}\n\n    async def _validate_required_fields(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate required fields are not null\"\"\"\n\n        required_fields = {\n            'customers': ['email', 'first_name', 'last_name'],\n            'orders': ['customer_id', 'total_amount', 'status'],\n            'products': ['name', 'price'],\n            'order_items': ['order_id', 'product_id', 'quantity']\n        }\n\n        fields = required_fields.get(table_name, [])\n        if not fields:\n            return {'status': 'skipped', 'reason': 'No required fields defined'}\n\n        validation_results = {}\n\n        for field in fields:\n            if self.target_type == 'mysql':\n                async with self.target_conn.cursor() as cursor:\n                    query = f\"SELECT COUNT(*) FROM {table_name} WHERE {field} IS NULL\"\n                    await cursor.execute(query)\n                    null_count = (await cursor.fetchone())[0]\n\n                    validation_results[field] = {\n                        'null_count': null_count,\n                        'is_valid': null_count == 0\n                    }\n\n        return {\n            'status': 'completed',\n            'field_validations': validation_results,\n            'all_valid': all(v['is_valid'] for v in validation_results.values())\n        }\n\n    async def _validate_positive_amounts(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate amounts are positive\"\"\"\n\n        if self.target_type == 'mysql':\n            async with self.target_conn.cursor() as cursor:\n                query = f\"\"\"\n                    SELECT COUNT(*) as total,\n                           SUM(CASE WHEN total_amount &gt; 0 THEN 1 ELSE 0 END) as positive\n                    FROM {table_name}\n                \"\"\"\n                await cursor.execute(query)\n                result = await cursor.fetchone()\n\n                return {\n                    'status': 'completed',\n                    'total_orders': result[0],\n                    'positive_amounts': result[1],\n                    'negative_or_zero': result[0] - result[1],\n                    'valid_percentage': (result[1] / result[0] * 100) if result[0] &gt; 0 else 0\n                }\n\n        return {'status': 'skipped', 'reason': 'Only supported for MySQL target'}\n\n    async def _validate_customer_references(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate customer references exist\"\"\"\n\n        if table_name == 'orders' and self.target_type == 'mysql':\n            async with self.target_conn.cursor() as cursor:\n                query = f\"\"\"\n                    SELECT COUNT(*) as total_orders,\n                           COUNT(c.id) as valid_customers\n                    FROM {table_name} o\n                    LEFT JOIN customers c ON o.customer_id = c.id\n                \"\"\"\n                await cursor.execute(query)\n                result = await cursor.fetchone()\n\n                return {\n                    'status': 'completed',\n                    'total_orders': result[0],\n                    'valid_references': result[1],\n                    'orphaned_orders': result[0] - result[1],\n                    'valid_percentage': (result[1] / result[0] * 100) if result[0] &gt; 0 else 0\n                }\n\n        return {'status': 'skipped', 'reason': 'Not applicable for this table'}\n\n    async def _validate_positive_prices(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate product prices are positive\"\"\"\n\n        if self.target_type == 'mysql':\n            async with self.target_conn.cursor() as cursor:\n                query = f\"\"\"\n                    SELECT COUNT(*) as total,\n                           SUM(CASE WHEN price &gt; 0 THEN 1 ELSE 0 END) as positive\n                    FROM {table_name}\n                \"\"\"\n                await cursor.execute(query)\n                result = await cursor.fetchone()\n\n                return {\n                    'status': 'completed',\n                    'total_products': result[0],\n                    'positive_prices': result[1],\n                    'invalid_prices': result[0] - result[1],\n                    'valid_percentage': (result[1] / result[0] * 100) if result[0] &gt; 0 else 0\n                }\n\n        return {'status': 'skipped'}\n\n    async def _validate_positive_quantities(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Validate quantities are positive\"\"\"\n\n        if self.target_type == 'mysql':\n            async with self.target_conn.cursor() as cursor:\n                query = f\"\"\"\n                    SELECT COUNT(*) as total,\n                           SUM(CASE WHEN quantity &gt; 0 THEN 1 ELSE 0 END) as positive\n                    FROM {table_name}\n                \"\"\"\n                await cursor.execute(query)\n                result = await cursor.fetchone()\n\n                return {\n                    'status': 'completed',\n                    'total_items': result[0],\n                    'positive_quantities': result[1],\n                    'invalid_quantities': result[0] - result[1],\n                    'valid_percentage': (result[1] / result[0] * 100) if result[0] &gt; 0 else 0\n                }\n\n        return {'status': 'skipped'}\n\n    async def generate_sync_report(self, sync_result: Dict[str, Any]) -&gt; str:\n        \"\"\"Generate a detailed sync report\"\"\"\n\n        report_lines = [\n            \"=\" * 60,\n            \"DATABASE SYNCHRONIZATION REPORT\",\n            \"=\" * 60,\n            f\"Sync completed at: {sync_result['end_time']}\",\n            f\"Total duration: {sync_result['total_duration_seconds']:.2f} seconds\",\n            f\"Total tables processed: {sync_result['total_tables']}\",\n            f\"Successful syncs: {sync_result['successful_syncs']}\",\n            f\"Failed syncs: {sync_result['failed_syncs']}\",\n            f\"Total rows synced: {sync_result['total_rows_synced']:,}\",\n            \"\",\n            \"TABLE DETAILS:\",\n            \"-\" * 40\n        ]\n\n        for result in sync_result['results']:\n            if result.get('success'):\n                table = result['table']\n                rows_synced = result.get('rows_synced', 0)\n                duration = result.get('duration_seconds', 0)\n                validation = result.get('validation', {})\n\n                report_lines.extend([\n                    f\"\u2705 {table}:\",\n                    f\"   Rows synced: {rows_synced:,}\",\n                    f\"   Duration: {duration:.2f}s\",\n                    f\"   Rate: {rows_synced/duration:.0f} rows/sec\" if duration &gt; 0 else \"   Rate: N/A\",\n                    f\"   Validation: {'\u2705 PASSED' if validation.get('is_valid', False) else '\u274c FAILED'}\",\n                    \"\"\n                ])\n            else:\n                table = result['table']\n                error = result.get('error', 'Unknown error')\n                report_lines.extend([\n                    f\"\u274c {table}:\",\n                    f\"   Error: {error}\",\n                    \"\"\n                ])\n\n        return \"\\n\".join(report_lines)\n\n\nasync def scheduled_sync():\n    \"\"\"Scheduled sync job (run every hour)\"\"\"\n\n    worker = DataReplicationWorker()\n\n    try:\n        async with worker:\n            print(\"\ud83d\ude80 Starting scheduled database sync...\")\n\n            # Run the sync\n            result = await worker.sync_all_configured_tables()\n\n            # Generate and print report\n            report = await worker.generate_sync_report(result)\n            print(report)\n\n            # Check if any syncs failed\n            if result['failed_syncs'] &gt; 0:\n                print(f\"\u26a0\ufe0f  {result['failed_syncs']} table sync(s) failed!\")\n                # In production, send alert email/notification\n\n            return result\n\n    except Exception as e:\n        print(f\"\ud83d\udca5 Sync job failed: {e}\")\n        raise\n\n\nasync def full_resync():\n    \"\"\"Full database resync (run weekly or on-demand)\"\"\"\n\n    # Create worker with full sync mode\n    worker = DataReplicationWorker()\n    worker.sync_mode = 'full'\n    worker.sync_config['truncate_target'] = True  # Clear target tables\n\n    try:\n        async with worker:\n            print(\"\ud83d\udd04 Starting FULL database resync...\")\n            print(\"\u26a0\ufe0f  Target tables will be truncated!\")\n\n            # Confirm in production\n            confirmation = input(\"Continue? (y/N): \")\n            if confirmation.lower() != 'y':\n                print(\"Sync cancelled.\")\n                return\n\n            result = await worker.sync_all_configured_tables()\n\n            # Generate report\n            report = await worker.generate_sync_report(result)\n            print(report)\n\n            return result\n\n    except Exception as e:\n        print(f\"\ud83d\udca5 Full resync failed: {e}\")\n        raise\n\n\nasync def validate_existing_data():\n    \"\"\"Validate data integrity without syncing\"\"\"\n\n    worker = DataReplicationWorker()\n\n    try:\n        async with worker:\n            print(\"\ud83d\udd0d Validating existing data integrity...\")\n\n            for table_name in worker.sync_tables.keys():\n                print(f\"\\nValidating {table_name}...\")\n\n                # Basic validation\n                validation = await worker.validate_sync(table_name)\n                print(f\"  Row count validation: {'\u2705 PASS' if validation['is_valid'] else '\u274c FAIL'}\")\n                print(f\"  Source: {validation['source_count']:,} rows\")\n                print(f\"  Target: {validation['target_count']:,} rows\")\n\n                # Custom validations\n                validations = worker.sync_tables[table_name]['validations']\n                custom_results = await worker._run_custom_validations(table_name, validations)\n\n                for validation_name, result in custom_results.items():\n                    if result['status'] == 'completed':\n                        print(f\"  {validation_name}: \u2705 COMPLETED\")\n                    else:\n                        print(f\"  {validation_name}: \u26a0\ufe0f {result.get('reason', 'SKIPPED')}\")\n\n    except Exception as e:\n        print(f\"\ud83d\udca5 Validation failed: {e}\")\n        raise\n\n\nasync def main():\n    \"\"\"Main CLI interface\"\"\"\n    import sys\n\n    if len(sys.argv) &lt; 2:\n        print(\"Usage:\")\n        print(\"  python sync_worker.py sync      # Incremental sync\")\n        print(\"  python sync_worker.py full      # Full resync\")\n        print(\"  python sync_worker.py validate  # Validate data only\")\n        return\n\n    command = sys.argv[1]\n\n    if command == \"sync\":\n        await scheduled_sync()\n    elif command == \"full\":\n        await full_resync()\n    elif command == \"validate\":\n        await validate_existing_data()\n    else:\n        print(f\"Unknown command: {command}\")\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/database-sync/#database-setup","title":"Database Setup","text":""},{"location":"examples/database-sync/#source-database-postgresql","title":"Source Database (PostgreSQL)","text":"<pre><code>-- Create production database\nCREATE DATABASE ecommerce;\n\n-- Create tables\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(100) NOT NULL,\n    last_name VARCHAR(100) NOT NULL,\n    phone VARCHAR(20),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    price DECIMAL(10,2) NOT NULL,\n    category VARCHAR(100),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    status VARCHAR(50) DEFAULT 'pending',\n    total_amount DECIMAL(10,2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\nCREATE TABLE order_items (\n    id SERIAL PRIMARY KEY,\n    order_id INTEGER REFERENCES orders(id),\n    product_id INTEGER REFERENCES products(id),\n    quantity INTEGER NOT NULL,\n    price DECIMAL(10,2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Create indexes on timestamp columns\nCREATE INDEX idx_customers_updated_at ON customers(updated_at);\nCREATE INDEX idx_products_updated_at ON products(updated_at);\nCREATE INDEX idx_orders_updated_at ON orders(updated_at);\nCREATE INDEX idx_order_items_created_at ON order_items(created_at);\n</code></pre>"},{"location":"examples/database-sync/#target-database-mysql","title":"Target Database (MySQL)","text":"<pre><code>-- Create analytics database\nCREATE DATABASE analytics;\n\n-- Create corresponding tables\nCREATE TABLE customers (\n    id INT PRIMARY KEY,\n    email VARCHAR(255) NOT NULL,\n    first_name VARCHAR(100) NOT NULL,\n    last_name VARCHAR(100) NOT NULL,\n    phone VARCHAR(20),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n\nCREATE TABLE products (\n    id INT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    price DECIMAL(10,2) NOT NULL,\n    category VARCHAR(100),\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n\nCREATE TABLE orders (\n    id INT PRIMARY KEY,\n    customer_id INT,\n    status VARCHAR(50) DEFAULT 'pending',\n    total_amount DECIMAL(10,2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    INDEX idx_customer_id (customer_id)\n);\n\nCREATE TABLE order_items (\n    id INT PRIMARY KEY,\n    order_id INT,\n    product_id INT,\n    quantity INT NOT NULL,\n    price DECIMAL(10,2) NOT NULL,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    INDEX idx_order_id (order_id),\n    INDEX idx_product_id (product_id)\n);\n</code></pre>"},{"location":"examples/database-sync/#running-the-example","title":"Running the Example","text":"<ol> <li> <p>Install dependencies: <pre><code>pip install asyncpg aiomysql\n</code></pre></p> </li> <li> <p>Configure connections: Update the connection strings in the worker code.</p> </li> <li> <p>Run sync operations: <pre><code># Incremental sync\npython sync_worker.py sync\n\n# Full resync\npython sync_worker.py full\n\n# Validate data only\npython sync_worker.py validate\n</code></pre></p> </li> </ol>"},{"location":"examples/database-sync/#expected-output","title":"Expected Output","text":""},{"location":"examples/database-sync/#incremental-sync","title":"Incremental Sync","text":"<pre><code>\ud83d\ude80 Starting scheduled database sync...\n============================================================\nDATABASE SYNCHRONIZATION REPORT\n============================================================\nSync completed at: 2024-01-15T10:30:00\nTotal duration: 12.45 seconds\nTotal tables processed: 4\nSuccessful syncs: 4\nFailed syncs: 0\nTotal rows synced: 15,234\n\nTABLE DETAILS:\n----------------------------------------\n\u2705 customers:\n   Rows synced: 1,234\n   Duration: 2.1s\n   Rate: 587 rows/sec\n   Validation: \u2705 PASSED\n\n\u2705 orders:\n   Rows synced: 5,678\n   Duration: 4.2s\n   Rate: 1,351 rows/sec\n   Validation: \u2705 PASSED\n</code></pre>"},{"location":"examples/database-sync/#validation-only","title":"Validation Only","text":"<pre><code>\ud83d\udd0d Validating existing data integrity...\n\nValidating customers...\n  Row count validation: \u2705 PASS\n  Source: 10,234 rows\n  Target: 10,234 rows\n  email_format: \u2705 COMPLETED\n  required_fields: \u2705 COMPLETED\n\nValidating orders...\n  Row count validation: \u2705 PASS\n  Source: 45,678 rows\n  Target: 45,678 rows\n  amount_positive: \u2705 COMPLETED\n  customer_exists: \u2705 COMPLETED\n</code></pre>"},{"location":"examples/database-sync/#key-features-demonstrated","title":"Key Features Demonstrated","text":"<ol> <li>Cross-Database Sync: PostgreSQL to MySQL replication</li> <li>Incremental Sync: Only sync changed data using timestamps</li> <li>Custom Validations: Business logic validation after sync</li> <li>Batch Processing: Configurable batch sizes per table</li> <li>Error Handling: Robust error handling with detailed reporting</li> <li>Data Integrity: Validation of synced data</li> <li>Performance Monitoring: Sync rates and duration tracking</li> <li>Flexible Configuration: Per-table configuration options</li> </ol>"},{"location":"examples/database-sync/#production-considerations","title":"Production Considerations","text":"<ol> <li>Scheduling: Use cron or task scheduler for regular syncs</li> <li>Monitoring: Set up alerting for failed syncs</li> <li>Performance: Tune batch sizes based on data volume</li> <li>Security: Use read-only source connections</li> <li>Backup: Always backup target before full resync</li> <li>Network: Consider connection timeouts and retries</li> </ol>"},{"location":"examples/gcp-cloud/","title":"GCP Pub/Sub Example","text":"<p>This example shows how to build a real-time analytics pipeline using Google Cloud Pub/Sub with Pythia for processing user events and generating insights.</p>"},{"location":"examples/gcp-cloud/#architecture","title":"Architecture","text":"<pre><code>Web App \u2192 Pub/Sub Topic (user-events) \u2192 Analytics Processor\n                                     \u2193\n                               Pub/Sub Topic (insights) \u2192 Dashboard Updater\n</code></pre>"},{"location":"examples/gcp-cloud/#prerequisites","title":"Prerequisites","text":"<p>Install Pythia with GCP support:</p> <pre><code>pip install pythia[gcp]\n</code></pre> <p>Set up GCP credentials and resources:</p> <pre><code># Service account authentication\nexport GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account.json\"\nexport GCP_PROJECT_ID=\"your-project-id\"\n\n# Pub/Sub resources\nexport PUBSUB_USER_EVENTS_TOPIC=\"user-events\"\nexport PUBSUB_USER_EVENTS_SUBSCRIPTION=\"user-events-analytics\"\nexport PUBSUB_INSIGHTS_TOPIC=\"insights\"\nexport PUBSUB_INSIGHTS_SUBSCRIPTION=\"insights-dashboard\"\n</code></pre>"},{"location":"examples/gcp-cloud/#user-events-analytics-worker","title":"User Events Analytics Worker","text":"<pre><code># analytics_processor.py\nimport asyncio\nimport json\nfrom datetime import datetime, timedelta\nfrom collections import defaultdict\nfrom pythia import Worker\nfrom pythia.brokers.cloud import PubSubSubscriber, PubSubPublisher\nfrom pythia.models import Message\n\nclass UserEventsAnalytics(Worker):\n    \"\"\"Processes user events and generates real-time analytics.\"\"\"\n\n    source = PubSubSubscriber(\n        subscription_name=\"${PUBSUB_USER_EVENTS_SUBSCRIPTION}\",\n        max_messages=50,\n        ack_deadline=60  # 60 seconds to process\n    )\n\n    def __init__(self):\n        super().__init__()\n        # Publisher for insights\n        self.insights_publisher = PubSubPublisher(\n            topic_name=\"${PUBSUB_INSIGHTS_TOPIC}\"\n        )\n\n        # In-memory analytics state (use Redis in production)\n        self.user_sessions = defaultdict(dict)\n        self.page_views = defaultdict(int)\n        self.conversion_funnel = defaultdict(int)\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Process user event and generate analytics.\"\"\"\n        event_data = message.body\n        event_type = event_data.get(\"event_type\")\n        user_id = event_data.get(\"user_id\")\n        timestamp = datetime.fromisoformat(event_data.get(\"timestamp\"))\n\n        print(f\"Processing {event_type} for user {user_id}\")\n\n        # Update analytics based on event type\n        if event_type == \"page_view\":\n            return await self._process_page_view(event_data, timestamp)\n        elif event_type == \"button_click\":\n            return await self._process_button_click(event_data, timestamp)\n        elif event_type == \"purchase\":\n            return await self._process_purchase(event_data, timestamp)\n        elif event_type == \"user_signup\":\n            return await self._process_signup(event_data, timestamp)\n        else:\n            print(f\"Unknown event type: {event_type}\")\n            return {\"status\": \"ignored\"}\n\n    async def _process_page_view(self, event_data: dict, timestamp: datetime) -&gt; dict:\n        \"\"\"Process page view event.\"\"\"\n        user_id = event_data[\"user_id\"]\n        page = event_data.get(\"page\", \"unknown\")\n\n        # Update page view counters\n        self.page_views[page] += 1\n\n        # Update user session\n        session = self.user_sessions[user_id]\n        session[\"last_activity\"] = timestamp\n        session.setdefault(\"pages_visited\", []).append({\n            \"page\": page,\n            \"timestamp\": timestamp.isoformat()\n        })\n\n        # Publish real-time page view insight\n        await self.insights_publisher.send(\n            message=Message(body={\n                \"insight_type\": \"page_popularity\",\n                \"page\": page,\n                \"view_count\": self.page_views[page],\n                \"timestamp\": timestamp.isoformat()\n            }),\n            ordering_key=f\"page:{page}\",\n            attributes={\n                \"insight_type\": \"page_popularity\",\n                \"page\": page\n            }\n        )\n\n        return {\"status\": \"processed\", \"page\": page, \"user_id\": user_id}\n\n    async def _process_button_click(self, event_data: dict, timestamp: datetime) -&gt; dict:\n        \"\"\"Process button click event.\"\"\"\n        user_id = event_data[\"user_id\"]\n        button_id = event_data.get(\"button_id\")\n        page = event_data.get(\"page\")\n\n        # Track conversion funnel\n        funnel_step = self._get_funnel_step(button_id, page)\n        if funnel_step:\n            self.conversion_funnel[funnel_step] += 1\n\n            # Publish funnel insight\n            await self.insights_publisher.send(\n                message=Message(body={\n                    \"insight_type\": \"conversion_funnel\",\n                    \"step\": funnel_step,\n                    \"count\": self.conversion_funnel[funnel_step],\n                    \"timestamp\": timestamp.isoformat()\n                }),\n                ordering_key=\"funnel:conversion\",\n                attributes={\n                    \"insight_type\": \"conversion_funnel\",\n                    \"funnel_step\": funnel_step\n                }\n            )\n\n        return {\"status\": \"processed\", \"button_id\": button_id, \"funnel_step\": funnel_step}\n\n    async def _process_purchase(self, event_data: dict, timestamp: datetime) -&gt; dict:\n        \"\"\"Process purchase event.\"\"\"\n        user_id = event_data[\"user_id\"]\n        amount = event_data.get(\"amount\", 0)\n        items = event_data.get(\"items\", [])\n\n        # Update conversion funnel\n        self.conversion_funnel[\"purchase\"] += 1\n\n        # Calculate session value\n        session = self.user_sessions[user_id]\n        session[\"total_spent\"] = session.get(\"total_spent\", 0) + amount\n        session[\"purchases\"] = session.get(\"purchases\", 0) + 1\n\n        # Publish purchase insights\n        await self.insights_publisher.send(\n            message=Message(body={\n                \"insight_type\": \"purchase_analytics\",\n                \"user_id\": user_id,\n                \"amount\": amount,\n                \"items_count\": len(items),\n                \"session_value\": session[\"total_spent\"],\n                \"timestamp\": timestamp.isoformat()\n            }),\n            ordering_key=f\"user:{user_id}\",\n            attributes={\n                \"insight_type\": \"purchase_analytics\",\n                \"user_id\": str(user_id),\n                \"amount\": str(amount)\n            }\n        )\n\n        return {\n            \"status\": \"processed\",\n            \"amount\": amount,\n            \"user_id\": user_id,\n            \"session_value\": session[\"total_spent\"]\n        }\n\n    async def _process_signup(self, event_data: dict, timestamp: datetime) -&gt; dict:\n        \"\"\"Process user signup event.\"\"\"\n        user_id = event_data[\"user_id\"]\n        source = event_data.get(\"source\", \"direct\")\n\n        # Track acquisition\n        self.conversion_funnel[\"signup\"] += 1\n\n        # Initialize user session\n        self.user_sessions[user_id] = {\n            \"signup_time\": timestamp,\n            \"signup_source\": source,\n            \"total_spent\": 0,\n            \"purchases\": 0,\n            \"pages_visited\": []\n        }\n\n        # Publish acquisition insight\n        await self.insights_publisher.send(\n            message=Message(body={\n                \"insight_type\": \"user_acquisition\",\n                \"user_id\": user_id,\n                \"source\": source,\n                \"total_signups\": self.conversion_funnel[\"signup\"],\n                \"timestamp\": timestamp.isoformat()\n            }),\n            ordering_key=f\"acquisition:{source}\",\n            attributes={\n                \"insight_type\": \"user_acquisition\",\n                \"source\": source,\n                \"user_id\": str(user_id)\n            }\n        )\n\n        return {\"status\": \"processed\", \"user_id\": user_id, \"source\": source}\n\n    def _get_funnel_step(self, button_id: str, page: str) -&gt; str:\n        \"\"\"Map button clicks to funnel steps.\"\"\"\n        funnel_mapping = {\n            (\"home\", \"cta_button\"): \"landing_cta\",\n            (\"pricing\", \"select_plan\"): \"plan_selection\",\n            (\"signup\", \"create_account\"): \"signup_attempt\",\n            (\"checkout\", \"purchase_button\"): \"purchase_attempt\"\n        }\n        return funnel_mapping.get((page, button_id))\n\n# Run the analytics worker\nif __name__ == \"__main__\":\n    worker = UserEventsAnalytics()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/gcp-cloud/#dashboard-updater-worker","title":"Dashboard Updater Worker","text":"<pre><code># dashboard_updater.py\nimport asyncio\nimport json\nfrom pythia import Worker\nfrom pythia.brokers.cloud import PubSubSubscriber\nfrom pythia.models import Message\n\nclass DashboardUpdater(Worker):\n    \"\"\"Updates real-time dashboard with analytics insights.\"\"\"\n\n    source = PubSubSubscriber(\n        subscription_name=\"${PUBSUB_INSIGHTS_SUBSCRIPTION}\",\n        max_messages=20,\n        ack_deadline=30\n    )\n\n    def __init__(self):\n        super().__init__()\n        # In production, this would connect to your dashboard backend\n        self.dashboard_state = {\n            \"page_views\": {},\n            \"conversion_funnel\": {},\n            \"revenue_metrics\": {},\n            \"user_acquisition\": {}\n        }\n\n    async def process(self, message: Message) -&gt; dict:\n        \"\"\"Update dashboard with new insights.\"\"\"\n        insight_data = message.body\n        insight_type = insight_data.get(\"insight_type\")\n\n        print(f\"Updating dashboard with {insight_type} insight\")\n\n        if insight_type == \"page_popularity\":\n            return await self._update_page_views(insight_data)\n        elif insight_type == \"conversion_funnel\":\n            return await self._update_funnel_metrics(insight_data)\n        elif insight_type == \"purchase_analytics\":\n            return await self._update_revenue_metrics(insight_data)\n        elif insight_type == \"user_acquisition\":\n            return await self._update_acquisition_metrics(insight_data)\n        else:\n            return {\"status\": \"ignored\", \"reason\": f\"Unknown insight type: {insight_type}\"}\n\n    async def _update_page_views(self, insight_data: dict) -&gt; dict:\n        \"\"\"Update page view dashboard.\"\"\"\n        page = insight_data[\"page\"]\n        view_count = insight_data[\"view_count\"]\n\n        self.dashboard_state[\"page_views\"][page] = view_count\n\n        # Simulate dashboard API call\n        await asyncio.sleep(0.1)\n        print(f\"\ud83d\udcca Dashboard updated: {page} has {view_count} views\")\n\n        return {\"status\": \"updated\", \"component\": \"page_views\", \"page\": page}\n\n    async def _update_funnel_metrics(self, insight_data: dict) -&gt; dict:\n        \"\"\"Update conversion funnel dashboard.\"\"\"\n        step = insight_data[\"step\"]\n        count = insight_data[\"count\"]\n\n        self.dashboard_state[\"conversion_funnel\"][step] = count\n\n        # Calculate conversion rates\n        funnel_order = [\"landing_cta\", \"plan_selection\", \"signup_attempt\", \"purchase_attempt\", \"purchase\"]\n        conversion_rates = {}\n\n        for i, current_step in enumerate(funnel_order[1:], 1):\n            previous_step = funnel_order[i-1]\n            current_count = self.dashboard_state[\"conversion_funnel\"].get(current_step, 0)\n            previous_count = self.dashboard_state[\"conversion_funnel\"].get(previous_step, 1)\n            conversion_rates[f\"{previous_step}_to_{current_step}\"] = (current_count / previous_count) * 100 if previous_count &gt; 0 else 0\n\n        await asyncio.sleep(0.1)\n        print(f\"\ud83c\udfaf Funnel updated: {step} = {count} ({conversion_rates})\")\n\n        return {\"status\": \"updated\", \"component\": \"conversion_funnel\", \"step\": step, \"rates\": conversion_rates}\n\n    async def _update_revenue_metrics(self, insight_data: dict) -&gt; dict:\n        \"\"\"Update revenue dashboard.\"\"\"\n        amount = insight_data[\"amount\"]\n        session_value = insight_data[\"session_value\"]\n\n        current_revenue = self.dashboard_state[\"revenue_metrics\"].get(\"total_revenue\", 0)\n        self.dashboard_state[\"revenue_metrics\"][\"total_revenue\"] = current_revenue + amount\n        self.dashboard_state[\"revenue_metrics\"][\"avg_session_value\"] = session_value\n\n        await asyncio.sleep(0.1)\n        print(f\"\ud83d\udcb0 Revenue updated: +${amount} (Total: ${current_revenue + amount})\")\n\n        return {\"status\": \"updated\", \"component\": \"revenue\", \"new_revenue\": amount}\n\n    async def _update_acquisition_metrics(self, insight_data: dict) -&gt; dict:\n        \"\"\"Update user acquisition dashboard.\"\"\"\n        source = insight_data[\"source\"]\n        total_signups = insight_data[\"total_signups\"]\n\n        if \"sources\" not in self.dashboard_state[\"user_acquisition\"]:\n            self.dashboard_state[\"user_acquisition\"][\"sources\"] = {}\n\n        self.dashboard_state[\"user_acquisition\"][\"sources\"][source] = self.dashboard_state[\"user_acquisition\"][\"sources\"].get(source, 0) + 1\n        self.dashboard_state[\"user_acquisition\"][\"total_signups\"] = total_signups\n\n        await asyncio.sleep(0.1)\n        print(f\"\ud83d\udc65 Acquisition updated: {source} (+1), Total signups: {total_signups}\")\n\n        return {\"status\": \"updated\", \"component\": \"acquisition\", \"source\": source}\n\n# Run the dashboard updater\nif __name__ == \"__main__\":\n    worker = DashboardUpdater()\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"examples/gcp-cloud/#event-generator-for-testing","title":"Event Generator (for Testing)","text":"<pre><code># event_generator.py\nimport asyncio\nimport json\nimport random\nfrom datetime import datetime\nfrom pythia.brokers.cloud import PubSubPublisher\nfrom pythia.models import Message\n\nclass EventGenerator:\n    \"\"\"Generates realistic user events for testing.\"\"\"\n\n    def __init__(self):\n        self.publisher = PubSubPublisher(\n            topic_name=\"${PUBSUB_USER_EVENTS_TOPIC}\"\n        )\n\n        self.pages = [\"home\", \"pricing\", \"signup\", \"checkout\", \"dashboard\"]\n        self.buttons = {\n            \"home\": [\"cta_button\", \"learn_more\", \"pricing_link\"],\n            \"pricing\": [\"select_plan\", \"contact_sales\"],\n            \"signup\": [\"create_account\", \"google_signup\"],\n            \"checkout\": [\"purchase_button\", \"back_button\"]\n        }\n        self.sources = [\"google\", \"facebook\", \"direct\", \"referral\", \"email\"]\n\n    async def generate_user_session(self, user_id: int):\n        \"\"\"Generate a realistic user session with multiple events.\"\"\"\n        source = random.choice(self.sources)\n\n        # Start with signup\n        await self._send_event(\"user_signup\", {\n            \"user_id\": user_id,\n            \"source\": source,\n            \"timestamp\": datetime.now().isoformat()\n        })\n\n        await asyncio.sleep(0.5)\n\n        # Browse some pages\n        current_page = \"home\"\n        for _ in range(random.randint(2, 6)):\n            # Page view\n            await self._send_event(\"page_view\", {\n                \"user_id\": user_id,\n                \"page\": current_page,\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n            await asyncio.sleep(random.uniform(0.1, 0.5))\n\n            # Maybe click a button\n            if current_page in self.buttons and random.random() &lt; 0.7:\n                button = random.choice(self.buttons[current_page])\n                await self._send_event(\"button_click\", {\n                    \"user_id\": user_id,\n                    \"page\": current_page,\n                    \"button_id\": button,\n                    \"timestamp\": datetime.now().isoformat()\n                })\n\n                await asyncio.sleep(random.uniform(0.1, 0.3))\n\n            # Navigate to next page\n            current_page = random.choice(self.pages)\n\n        # Maybe make a purchase (30% chance)\n        if random.random() &lt; 0.3:\n            await self._send_event(\"purchase\", {\n                \"user_id\": user_id,\n                \"amount\": round(random.uniform(29.99, 199.99), 2),\n                \"items\": [f\"item_{i}\" for i in range(random.randint(1, 3))],\n                \"timestamp\": datetime.now().isoformat()\n            })\n\n    async def _send_event(self, event_type: str, event_data: dict):\n        \"\"\"Send an event to Pub/Sub.\"\"\"\n        event_data[\"event_type\"] = event_type\n\n        await self.publisher.send(\n            message=Message(body=event_data),\n            ordering_key=f\"user:{event_data['user_id']}\",\n            attributes={\n                \"event_type\": event_type,\n                \"user_id\": str(event_data[\"user_id\"])\n            }\n        )\n\n        print(f\"\ud83d\udce4 Sent {event_type} for user {event_data['user_id']}\")\n\nasync def run_simulation(num_users: int = 10):\n    \"\"\"Run a simulation with multiple users.\"\"\"\n    generator = EventGenerator()\n\n    # Generate sessions for multiple users concurrently\n    tasks = [\n        generator.generate_user_session(user_id)\n        for user_id in range(1, num_users + 1)\n    ]\n\n    await asyncio.gather(*tasks)\n    print(f\"\u2705 Generated events for {num_users} users\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_simulation(num_users=20))\n</code></pre>"},{"location":"examples/gcp-cloud/#gcp-pubsub-setup","title":"GCP Pub/Sub Setup","text":"<p>Create the required topics and subscriptions:</p> <pre><code># Create topics\ngcloud pubsub topics create user-events\ngcloud pubsub topics create insights\n\n# Create subscriptions\ngcloud pubsub subscriptions create user-events-analytics --topic=user-events\ngcloud pubsub subscriptions create insights-dashboard --topic=insights\n\n# Set up IAM permissions (replace with your service account email)\ngcloud pubsub topics add-iam-policy-binding user-events \\\n    --member=\"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\" \\\n    --role=\"roles/pubsub.publisher\"\n\ngcloud pubsub subscriptions add-iam-policy-binding user-events-analytics \\\n    --member=\"serviceAccount:your-service-account@your-project.iam.gserviceaccount.com\" \\\n    --role=\"roles/pubsub.subscriber\"\n</code></pre>"},{"location":"examples/gcp-cloud/#running-the-example","title":"Running the Example","text":"<ol> <li>Start the workers:</li> </ol> <pre><code># Terminal 1: Analytics processor\npython analytics_processor.py\n\n# Terminal 2: Dashboard updater\npython dashboard_updater.py\n</code></pre> <ol> <li>Generate test events:</li> </ol> <pre><code># Terminal 3: Event generator\npython event_generator.py\n</code></pre>"},{"location":"examples/gcp-cloud/#monitoring-with-google-cloud","title":"Monitoring with Google Cloud","text":""},{"location":"examples/gcp-cloud/#cloud-monitoring-metrics","title":"Cloud Monitoring Metrics","text":"<p>Set up monitoring for your Pub/Sub resources:</p> <pre><code># monitoring_setup.py\nfrom google.cloud import monitoring_v3\n\ndef create_pubsub_alerts(project_id: str):\n    \"\"\"Create monitoring alerts for Pub/Sub metrics.\"\"\"\n    client = monitoring_v3.AlertPolicyServiceClient()\n    project_name = f\"projects/{project_id}\"\n\n    # Alert for high message backlog\n    policy = monitoring_v3.AlertPolicy(\n        display_name=\"High Pub/Sub Message Backlog\",\n        conditions=[\n            monitoring_v3.AlertPolicy.Condition(\n                display_name=\"Message backlog &gt; 1000\",\n                condition_threshold=monitoring_v3.AlertPolicy.Condition.MetricThreshold(\n                    filter='resource.type=\"pubsub_subscription\"',\n                    comparison=monitoring_v3.ComparisonType.COMPARISON_GREATER_THAN,\n                    threshold_value=1000,\n                )\n            )\n        ]\n    )\n\n    client.create_alert_policy(name=project_name, alert_policy=policy)\n</code></pre>"},{"location":"examples/gcp-cloud/#error-handling-and-dead-letter-topics","title":"Error Handling and Dead Letter Topics","text":"<pre><code>from pythia.brokers.cloud import PubSubSubscriber\nfrom google.api_core.exceptions import GoogleAPIError\n\nclass RobustAnalyticsProcessor(Worker):\n    source = PubSubSubscriber(\n        subscription_name=\"${PUBSUB_USER_EVENTS_SUBSCRIPTION}\",\n        max_messages=10,\n        ack_deadline=60\n    )\n\n    async def process(self, message: Message) -&gt; dict:\n        try:\n            return await self._process_event(message.body)\n        except GoogleAPIError as e:\n            print(f\"Google API error: {e}\")\n            # Handle API issues (retry, dead letter, etc.)\n            raise\n        except json.JSONDecodeError as e:\n            print(f\"Invalid JSON in message: {e}\")\n            # Don't retry for malformed messages\n            return {\"status\": \"skipped\", \"reason\": \"invalid_json\"}\n</code></pre>"},{"location":"examples/gcp-cloud/#best-practices","title":"Best Practices","text":"<ol> <li>Message Ordering: Use ordering keys for events that need to be processed in sequence</li> <li>Batch Processing: Process multiple messages together to improve throughput</li> <li>Error Handling: Implement dead letter topics for failed messages</li> <li>Monitoring: Set up Cloud Monitoring alerts for message backlogs and processing errors</li> <li>Cost Optimization: Use appropriate acknowledgment deadlines and message retention policies</li> <li>Scaling: Use multiple worker instances with the same subscription for horizontal scaling</li> </ol> <p>This example demonstrates how to build a real-time analytics pipeline using Google Cloud Pub/Sub with Pythia's cloud workers.</p>"},{"location":"examples/http-workers/","title":"HTTP Workers","text":"<p>Complete guide to implementing HTTP-based workers for API polling, webhook processing, and web scraping.</p>"},{"location":"examples/http-workers/#overview","title":"Overview","text":"<p>HTTP workers handle external HTTP integrations including: - API Pollers - Periodically fetch data from REST APIs - Webhook Processors - Handle incoming HTTP callbacks - Web Scrapers - Extract data from web pages - Health Checkers - Monitor external service availability</p>"},{"location":"examples/http-workers/#http-polling-workers","title":"HTTP Polling Workers","text":""},{"location":"examples/http-workers/#basic-api-poller","title":"Basic API Poller","text":"<pre><code>import asyncio\nimport json\nfrom typing import Dict, Any, List, Optional\nfrom pythia.core import Worker, Message\nfrom pythia.http.poller import HTTPPoller\nfrom pythia.config import WorkerConfig\n\nclass APIPollingWorker(Worker):\n    \"\"\"Worker that polls external APIs for data\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup HTTP poller for GitHub API\n        self.github_poller = HTTPPoller(\n            url=\"https://api.github.com/repos/python/cpython/events\",\n            interval=300,  # Poll every 5 minutes\n            method=\"GET\",\n            headers={\n                \"Accept\": \"application/vnd.github.v3+json\",\n                \"User-Agent\": \"Pythia-Worker/1.0\"\n            }\n        )\n\n    async def start_polling(self):\n        \"\"\"Start polling for GitHub events\"\"\"\n        self.logger.info(\"Starting GitHub events polling...\")\n\n        async for message in self.github_poller.consume():\n            try:\n                await self.process_message(message)\n            except Exception as e:\n                self.logger.error(f\"Failed to process polled data: {e}\")\n                # Continue polling despite errors\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process GitHub events from API\"\"\"\n        try:\n            # Parse GitHub events\n            events = json.loads(message.body)\n\n            if not isinstance(events, list):\n                self.logger.warning(\"Unexpected API response format\")\n                return {\"status\": \"error\", \"message\": \"Invalid response format\"}\n\n            # Process each event\n            processed_events = []\n            for event in events[:10]:  # Process first 10 events\n                processed_event = await self._process_github_event(event)\n                if processed_event:\n                    processed_events.append(processed_event)\n\n            self.logger.info(f\"Processed {len(processed_events)} GitHub events\")\n\n            return {\n                \"status\": \"processed\",\n                \"events_processed\": len(processed_events),\n                \"events\": processed_events,\n                \"poll_timestamp\": message.timestamp.isoformat()\n            }\n\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Failed to parse API response: {e}\")\n            raise\n        except Exception as e:\n            self.logger.error(f\"Error processing GitHub events: {e}\")\n            raise\n\n    async def _process_github_event(self, event: Dict[str, Any]) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Process a single GitHub event\"\"\"\n        try:\n            event_type = event.get(\"type\")\n\n            # Filter for interesting events\n            interesting_events = [\"PushEvent\", \"PullRequestEvent\", \"IssuesEvent\", \"ReleaseEvent\"]\n\n            if event_type not in interesting_events:\n                return None\n\n            processed = {\n                \"id\": event.get(\"id\"),\n                \"type\": event_type,\n                \"actor\": event.get(\"actor\", {}).get(\"login\"),\n                \"repo\": event.get(\"repo\", {}).get(\"name\"),\n                \"created_at\": event.get(\"created_at\"),\n                \"public\": event.get(\"public\", False)\n            }\n\n            # Add event-specific data\n            if event_type == \"PushEvent\":\n                payload = event.get(\"payload\", {})\n                processed[\"commits\"] = len(payload.get(\"commits\", []))\n                processed[\"branch\"] = payload.get(\"ref\", \"\").replace(\"refs/heads/\", \"\")\n\n            elif event_type == \"PullRequestEvent\":\n                pr = event.get(\"payload\", {}).get(\"pull_request\", {})\n                processed[\"pr_action\"] = event.get(\"payload\", {}).get(\"action\")\n                processed[\"pr_title\"] = pr.get(\"title\")\n                processed[\"pr_state\"] = pr.get(\"state\")\n\n            elif event_type == \"ReleaseEvent\":\n                release = event.get(\"payload\", {}).get(\"release\", {})\n                processed[\"release_tag\"] = release.get(\"tag_name\")\n                processed[\"release_name\"] = release.get(\"name\")\n                processed[\"prerelease\"] = release.get(\"prerelease\", False)\n\n            return processed\n\n        except Exception as e:\n            self.logger.error(f\"Error processing event {event.get('id')}: {e}\")\n            return None\n\n# Usage example\nasync def run_github_poller():\n    config = WorkerConfig(\n        worker_name=\"github-events-poller\",\n        log_level=\"INFO\"\n    )\n\n    worker = APIPollingWorker(config)\n\n    # Start polling (runs indefinitely)\n    await worker.start_polling()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_github_poller())\n</code></pre>"},{"location":"examples/http-workers/#advanced-multi-endpoint-poller","title":"Advanced Multi-Endpoint Poller","text":"<pre><code>import asyncio\nimport json\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\nfrom pythia.core import Worker, Message\nfrom pythia.http.poller import HTTPPoller\nfrom pythia.config import WorkerConfig\n\n@dataclass\nclass EndpointConfig:\n    \"\"\"Configuration for API endpoint\"\"\"\n    name: str\n    url: str\n    interval: int\n    headers: Dict[str, str] = None\n    params: Dict[str, str] = None\n    method: str = \"GET\"\n\nclass MultiEndpointPoller(Worker):\n    \"\"\"Poll multiple API endpoints with different intervals\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Define multiple endpoints to poll\n        self.endpoints = [\n            EndpointConfig(\n                name=\"github_events\",\n                url=\"https://api.github.com/repos/python/cpython/events\",\n                interval=300,  # 5 minutes\n                headers={\"Accept\": \"application/vnd.github.v3+json\"}\n            ),\n            EndpointConfig(\n                name=\"hackernews_top\",\n                url=\"https://hacker-news.firebaseio.com/v0/topstories.json\",\n                interval=600,  # 10 minutes\n            ),\n            EndpointConfig(\n                name=\"jsonplaceholder_posts\",\n                url=\"https://jsonplaceholder.typicode.com/posts\",\n                interval=1800,  # 30 minutes\n                params={\"_limit\": \"10\"}\n            )\n        ]\n\n        self.pollers = {}\n\n    async def on_startup(self):\n        \"\"\"Setup all pollers on startup\"\"\"\n        await super().on_startup()\n\n        for endpoint in self.endpoints:\n            poller = HTTPPoller(\n                url=endpoint.url,\n                interval=endpoint.interval,\n                method=endpoint.method,\n                headers=endpoint.headers or {},\n                params=endpoint.params or {}\n            )\n            self.pollers[endpoint.name] = poller\n\n        self.logger.info(f\"Initialized {len(self.pollers)} API pollers\")\n\n    async def start_polling(self):\n        \"\"\"Start polling all endpoints concurrently\"\"\"\n        polling_tasks = []\n\n        for endpoint_name, poller in self.pollers.items():\n            task = asyncio.create_task(\n                self._poll_endpoint(endpoint_name, poller)\n            )\n            polling_tasks.append(task)\n\n        self.logger.info(\"Starting concurrent polling of all endpoints...\")\n\n        # Wait for all polling tasks\n        try:\n            await asyncio.gather(*polling_tasks)\n        except Exception as e:\n            self.logger.error(f\"Polling error: {e}\")\n        finally:\n            # Cancel remaining tasks\n            for task in polling_tasks:\n                if not task.done():\n                    task.cancel()\n\n    async def _poll_endpoint(self, endpoint_name: str, poller: HTTPPoller):\n        \"\"\"Poll a specific endpoint\"\"\"\n        self.logger.info(f\"Starting poller for {endpoint_name}\")\n\n        try:\n            async for message in poller.consume():\n                # Add endpoint context to message\n                message.headers[\"endpoint_name\"] = endpoint_name\n\n                # Process the message\n                result = await self.process_message(message)\n\n                self.logger.info(\n                    f\"Processed data from {endpoint_name}\",\n                    extra={\"endpoint\": endpoint_name, \"result\": result}\n                )\n\n        except Exception as e:\n            self.logger.error(f\"Error polling {endpoint_name}: {e}\")\n            # Implement exponential backoff or circuit breaker here\n            await asyncio.sleep(60)  # Wait before retrying\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process message based on endpoint\"\"\"\n        endpoint_name = message.headers.get(\"endpoint_name\")\n\n        if endpoint_name == \"github_events\":\n            return await self._process_github_events(message)\n        elif endpoint_name == \"hackernews_top\":\n            return await self._process_hackernews(message)\n        elif endpoint_name == \"jsonplaceholder_posts\":\n            return await self._process_jsonplaceholder(message)\n        else:\n            return {\"status\": \"unknown_endpoint\", \"endpoint\": endpoint_name}\n\n    async def _process_github_events(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process GitHub events\"\"\"\n        events = json.loads(message.body)\n\n        # Extract relevant information\n        event_types = {}\n        for event in events[:20]:  # First 20 events\n            event_type = event.get(\"type\", \"unknown\")\n            event_types[event_type] = event_types.get(event_type, 0) + 1\n\n        return {\n            \"endpoint\": \"github_events\",\n            \"total_events\": len(events),\n            \"event_types\": event_types,\n            \"most_common\": max(event_types.items(), key=lambda x: x[1]) if event_types else None\n        }\n\n    async def _process_hackernews(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process Hacker News top stories\"\"\"\n        story_ids = json.loads(message.body)\n\n        return {\n            \"endpoint\": \"hackernews_top\",\n            \"total_stories\": len(story_ids),\n            \"top_story_id\": story_ids[0] if story_ids else None,\n            \"story_id_range\": f\"{min(story_ids)}-{max(story_ids)}\" if story_ids else None\n        }\n\n    async def _process_jsonplaceholder(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process JSONPlaceholder posts\"\"\"\n        posts = json.loads(message.body)\n\n        user_posts = {}\n        for post in posts:\n            user_id = post.get(\"userId\")\n            user_posts[user_id] = user_posts.get(user_id, 0) + 1\n\n        return {\n            \"endpoint\": \"jsonplaceholder_posts\",\n            \"total_posts\": len(posts),\n            \"unique_users\": len(user_posts),\n            \"posts_per_user\": user_posts\n        }\n\n# Usage\nasync def run_multi_endpoint_poller():\n    config = WorkerConfig(\n        worker_name=\"multi-endpoint-poller\",\n        log_level=\"INFO\"\n    )\n\n    worker = MultiEndpointPoller(config)\n    await worker.on_startup()\n    await worker.start_polling()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_multi_endpoint_poller())\n</code></pre>"},{"location":"examples/http-workers/#webhook-processing-workers","title":"Webhook Processing Workers","text":""},{"location":"examples/http-workers/#basic-webhook-processor","title":"Basic Webhook Processor","text":"<pre><code>import asyncio\nimport json\nimport hmac\nimport hashlib\nfrom typing import Dict, Any, Optional\nfrom pythia.core import Worker, Message\nfrom pythia.http.webhook import WebhookClient\nfrom pythia.config import WorkerConfig\n\nclass WebhookProcessor(Worker):\n    \"\"\"Process incoming webhook events\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Setup webhook client for sending responses\n        self.webhook_client = WebhookClient(\n            base_url=\"https://api.partner.com\"\n        )\n\n        # Webhook verification settings\n        self.webhook_secrets = {\n            \"stripe\": \"whsec_stripe_secret_key\",\n            \"github\": \"github_webhook_secret\",\n            \"shopify\": \"shopify_webhook_secret\"\n        }\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process incoming webhook\"\"\"\n        try:\n            # Determine webhook source\n            webhook_source = self._identify_webhook_source(message)\n\n            # Verify webhook signature\n            if not await self._verify_webhook_signature(message, webhook_source):\n                raise ValueError(f\"Invalid webhook signature from {webhook_source}\")\n\n            # Parse webhook data\n            webhook_data = json.loads(message.body)\n\n            self.logger.info(\n                f\"Processing {webhook_source} webhook\",\n                extra={\n                    \"source\": webhook_source,\n                    \"event_type\": webhook_data.get(\"type\"),\n                    \"webhook_id\": webhook_data.get(\"id\")\n                }\n            )\n\n            # Route to appropriate handler\n            if webhook_source == \"stripe\":\n                return await self._handle_stripe_webhook(webhook_data)\n            elif webhook_source == \"github\":\n                return await self._handle_github_webhook(webhook_data)\n            elif webhook_source == \"shopify\":\n                return await self._handle_shopify_webhook(webhook_data)\n            else:\n                return await self._handle_unknown_webhook(webhook_data, webhook_source)\n\n        except Exception as e:\n            self.logger.error(f\"Webhook processing failed: {e}\")\n            raise\n\n    def _identify_webhook_source(self, message: Message) -&gt; str:\n        \"\"\"Identify the source of the webhook\"\"\"\n        headers = message.headers\n\n        if \"stripe-signature\" in headers:\n            return \"stripe\"\n        elif \"x-github-event\" in headers:\n            return \"github\"\n        elif \"x-shopify-topic\" in headers:\n            return \"shopify\"\n        else:\n            # Try to identify from User-Agent or other headers\n            user_agent = headers.get(\"user-agent\", \"\").lower()\n            if \"stripe\" in user_agent:\n                return \"stripe\"\n            elif \"github\" in user_agent:\n                return \"github\"\n            elif \"shopify\" in user_agent:\n                return \"shopify\"\n\n        return \"unknown\"\n\n    async def _verify_webhook_signature(self, message: Message, source: str) -&gt; bool:\n        \"\"\"Verify webhook signature\"\"\"\n        if source not in self.webhook_secrets:\n            self.logger.warning(f\"No secret configured for webhook source: {source}\")\n            return True  # Skip verification\n\n        secret = self.webhook_secrets[source]\n        headers = message.headers\n\n        if source == \"stripe\":\n            return self._verify_stripe_signature(message.body, headers.get(\"stripe-signature\"), secret)\n        elif source == \"github\":\n            return self._verify_github_signature(message.body, headers.get(\"x-hub-signature-256\"), secret)\n        elif source == \"shopify\":\n            return self._verify_shopify_signature(message.body, headers.get(\"x-shopify-hmac-sha256\"), secret)\n\n        return True\n\n    def _verify_stripe_signature(self, body: str, signature: str, secret: str) -&gt; bool:\n        \"\"\"Verify Stripe webhook signature\"\"\"\n        if not signature:\n            return False\n\n        # Stripe signature format: t=timestamp,v1=signature\n        elements = signature.split(\",\")\n        signature_dict = {}\n\n        for element in elements:\n            key, value = element.split(\"=\", 1)\n            signature_dict[key] = value\n\n        timestamp = signature_dict.get(\"t\")\n        v1_signature = signature_dict.get(\"v1\")\n\n        if not timestamp or not v1_signature:\n            return False\n\n        # Create expected signature\n        payload = f\"{timestamp}.{body}\"\n        expected_signature = hmac.new(\n            secret.encode(),\n            payload.encode(),\n            hashlib.sha256\n        ).hexdigest()\n\n        return hmac.compare_digest(v1_signature, expected_signature)\n\n    def _verify_github_signature(self, body: str, signature: str, secret: str) -&gt; bool:\n        \"\"\"Verify GitHub webhook signature\"\"\"\n        if not signature or not signature.startswith(\"sha256=\"):\n            return False\n\n        expected_signature = \"sha256=\" + hmac.new(\n            secret.encode(),\n            body.encode(),\n            hashlib.sha256\n        ).hexdigest()\n\n        return hmac.compare_digest(signature, expected_signature)\n\n    def _verify_shopify_signature(self, body: str, signature: str, secret: str) -&gt; bool:\n        \"\"\"Verify Shopify webhook signature\"\"\"\n        if not signature:\n            return False\n\n        expected_signature = hmac.new(\n            secret.encode(),\n            body.encode(),\n            hashlib.sha256\n        ).hexdigest()\n\n        return hmac.compare_digest(signature, expected_signature)\n\n    async def _handle_stripe_webhook(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle Stripe webhook events\"\"\"\n        event_type = data.get(\"type\")\n        event_data = data.get(\"data\", {})\n\n        self.logger.info(f\"Processing Stripe event: {event_type}\")\n\n        if event_type == \"payment_intent.succeeded\":\n            return await self._handle_stripe_payment_succeeded(event_data)\n        elif event_type == \"invoice.payment_failed\":\n            return await self._handle_stripe_payment_failed(event_data)\n        elif event_type == \"customer.subscription.updated\":\n            return await self._handle_stripe_subscription_updated(event_data)\n        else:\n            return {\"status\": \"ignored\", \"event_type\": event_type, \"source\": \"stripe\"}\n\n    async def _handle_github_webhook(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle GitHub webhook events\"\"\"\n        action = data.get(\"action\")\n\n        if \"pull_request\" in data:\n            return await self._handle_github_pr_event(data, action)\n        elif \"issue\" in data:\n            return await self._handle_github_issue_event(data, action)\n        elif \"release\" in data:\n            return await self._handle_github_release_event(data, action)\n        else:\n            return {\"status\": \"ignored\", \"action\": action, \"source\": \"github\"}\n\n    async def _handle_shopify_webhook(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle Shopify webhook events\"\"\"\n        # Shopify webhook topic is in headers, but we can infer from data structure\n        if \"line_items\" in data:\n            return await self._handle_shopify_order_event(data)\n        elif \"email\" in data and \"first_name\" in data:\n            return await self._handle_shopify_customer_event(data)\n        else:\n            return {\"status\": \"processed\", \"data_keys\": list(data.keys()), \"source\": \"shopify\"}\n\n    async def _handle_stripe_payment_succeeded(self, event_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle successful Stripe payment\"\"\"\n        payment_intent = event_data.get(\"object\", {})\n        amount = payment_intent.get(\"amount\", 0) / 100  # Convert from cents\n        customer_id = payment_intent.get(\"customer\")\n\n        # Process payment success (update database, send confirmation, etc.)\n        await self._update_payment_status(payment_intent.get(\"id\"), \"succeeded\")\n        await self._send_payment_confirmation(customer_id, amount)\n\n        return {\n            \"status\": \"processed\",\n            \"event_type\": \"payment_succeeded\",\n            \"amount\": amount,\n            \"customer_id\": customer_id,\n            \"source\": \"stripe\"\n        }\n\n    async def _handle_github_pr_event(self, data: Dict[str, Any], action: str) -&gt; Dict[str, Any]:\n        \"\"\"Handle GitHub PR events\"\"\"\n        pr = data.get(\"pull_request\", {})\n        pr_number = pr.get(\"number\")\n        pr_title = pr.get(\"title\")\n\n        if action == \"opened\":\n            # Run CI checks, notify team, etc.\n            await self._trigger_pr_checks(pr_number)\n            await self._notify_team_pr_opened(pr_number, pr_title)\n        elif action == \"closed\" and pr.get(\"merged\"):\n            # Handle merged PR\n            await self._handle_pr_merged(pr_number)\n\n        return {\n            \"status\": \"processed\",\n            \"event_type\": f\"pr_{action}\",\n            \"pr_number\": pr_number,\n            \"merged\": pr.get(\"merged\", False),\n            \"source\": \"github\"\n        }\n\n    async def _handle_shopify_order_event(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle Shopify order events\"\"\"\n        order_id = data.get(\"id\")\n        customer_email = data.get(\"email\")\n        total_price = data.get(\"total_price\")\n\n        # Process order (update inventory, send confirmation, etc.)\n        await self._update_inventory(data.get(\"line_items\", []))\n        await self._send_order_confirmation(customer_email, order_id)\n\n        return {\n            \"status\": \"processed\",\n            \"event_type\": \"order_created\",\n            \"order_id\": order_id,\n            \"total_price\": total_price,\n            \"source\": \"shopify\"\n        }\n\n    async def _handle_unknown_webhook(self, data: Dict[str, Any], source: str) -&gt; Dict[str, Any]:\n        \"\"\"Handle unknown webhook types\"\"\"\n        self.logger.warning(f\"Unknown webhook from {source}: {data.keys()}\")\n\n        # Store for later analysis or forward to dead letter queue\n        await self._store_unknown_webhook(data, source)\n\n        return {\n            \"status\": \"logged\",\n            \"source\": source,\n            \"data_keys\": list(data.keys())\n        }\n\n    # Helper methods (implement based on your business logic)\n    async def _update_payment_status(self, payment_id: str, status: str):\n        \"\"\"Update payment status in database\"\"\"\n        self.logger.info(f\"Updating payment {payment_id} to {status}\")\n        # Database update logic here\n\n    async def _send_payment_confirmation(self, customer_id: str, amount: float):\n        \"\"\"Send payment confirmation to customer\"\"\"\n        self.logger.info(f\"Sending payment confirmation to customer {customer_id} for ${amount}\")\n        # Email/notification logic here\n\n    async def _trigger_pr_checks(self, pr_number: int):\n        \"\"\"Trigger CI checks for PR\"\"\"\n        self.logger.info(f\"Triggering CI checks for PR #{pr_number}\")\n        # CI integration logic here\n\n    async def _notify_team_pr_opened(self, pr_number: int, title: str):\n        \"\"\"Notify team of new PR\"\"\"\n        self.logger.info(f\"Notifying team of PR #{pr_number}: {title}\")\n        # Slack/Teams notification logic here\n\n    async def _handle_pr_merged(self, pr_number: int):\n        \"\"\"Handle merged PR\"\"\"\n        self.logger.info(f\"PR #{pr_number} was merged\")\n        # Deployment trigger, cleanup, etc.\n\n    async def _update_inventory(self, line_items: List[Dict]):\n        \"\"\"Update inventory based on order items\"\"\"\n        for item in line_items:\n            product_id = item.get(\"product_id\")\n            quantity = item.get(\"quantity\", 0)\n            self.logger.info(f\"Updating inventory for product {product_id}: -{quantity}\")\n        # Inventory update logic here\n\n    async def _send_order_confirmation(self, email: str, order_id: str):\n        \"\"\"Send order confirmation email\"\"\"\n        self.logger.info(f\"Sending order confirmation for {order_id} to {email}\")\n        # Email sending logic here\n\n    async def _store_unknown_webhook(self, data: Dict[str, Any], source: str):\n        \"\"\"Store unknown webhook for analysis\"\"\"\n        self.logger.info(f\"Storing unknown webhook from {source}\")\n        # Storage logic here\n\n# Usage example with message broker integration\nasync def run_webhook_processor():\n    # Configure for Kafka to receive webhooks\n    config = WorkerConfig(\n        worker_name=\"webhook-processor\",\n        broker_type=\"kafka\",  # Webhooks received via Kafka topic\n        log_level=\"INFO\"\n    )\n\n    worker = WebhookProcessor(config)\n    await worker.start()\n\nif __name__ == \"__main__\":\n    asyncio.run(run_webhook_processor())\n</code></pre>"},{"location":"examples/http-workers/#health-check-workers","title":"Health Check Workers","text":""},{"location":"examples/http-workers/#service-health-monitor","title":"Service Health Monitor","text":"<pre><code>import asyncio\nimport aiohttp\nfrom typing import Dict, Any, List\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pythia.core import Worker\nfrom pythia.config import WorkerConfig\n\n@dataclass\nclass HealthCheckConfig:\n    name: str\n    url: str\n    method: str = \"GET\"\n    timeout: int = 10\n    expected_status: int = 200\n    interval: int = 60\n    headers: Dict[str, str] = None\n\nclass HealthCheckWorker(Worker):\n    \"\"\"Monitor health of external services\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Define services to monitor\n        self.services = [\n            HealthCheckConfig(\n                name=\"main_api\",\n                url=\"https://api.example.com/health\",\n                timeout=5,\n                interval=30\n            ),\n            HealthCheckConfig(\n                name=\"database\",\n                url=\"https://db.example.com/ping\",\n                timeout=10,\n                interval=60\n            ),\n            HealthCheckConfig(\n                name=\"redis_cache\",\n                url=\"https://cache.example.com/ping\",\n                timeout=3,\n                interval=30\n            ),\n            HealthCheckConfig(\n                name=\"external_service\",\n                url=\"https://partner.example.com/status\",\n                headers={\"Authorization\": \"Bearer token\"},\n                interval=120\n            )\n        ]\n\n        self.health_status = {}\n        self.last_check = {}\n\n    async def on_startup(self):\n        \"\"\"Initialize health monitoring\"\"\"\n        await super().on_startup()\n\n        # Initialize status tracking\n        for service in self.services:\n            self.health_status[service.name] = None\n            self.last_check[service.name] = None\n\n        # Start health checking\n        asyncio.create_task(self._start_health_monitoring())\n\n    async def _start_health_monitoring(self):\n        \"\"\"Start monitoring all services\"\"\"\n        self.logger.info(f\"Starting health monitoring for {len(self.services)} services\")\n\n        # Create tasks for each service\n        tasks = []\n        for service in self.services:\n            task = asyncio.create_task(self._monitor_service(service))\n            tasks.append(task)\n\n        # Wait for all monitoring tasks\n        await asyncio.gather(*tasks, return_exceptions=True)\n\n    async def _monitor_service(self, service: HealthCheckConfig):\n        \"\"\"Monitor a single service continuously\"\"\"\n        while True:\n            try:\n                await self._check_service_health(service)\n                await asyncio.sleep(service.interval)\n            except Exception as e:\n                self.logger.error(f\"Error monitoring {service.name}: {e}\")\n                await asyncio.sleep(service.interval)\n\n    async def _check_service_health(self, service: HealthCheckConfig):\n        \"\"\"Check health of a single service\"\"\"\n        check_start = datetime.now()\n\n        try:\n            timeout = aiohttp.ClientTimeout(total=service.timeout)\n            async with aiohttp.ClientSession(timeout=timeout) as session:\n\n                async with session.request(\n                    method=service.method,\n                    url=service.url,\n                    headers=service.headers or {}\n                ) as response:\n\n                    response_time = (datetime.now() - check_start).total_seconds()\n\n                    # Check if response is healthy\n                    is_healthy = response.status == service.expected_status\n\n                    # Update status\n                    previous_status = self.health_status.get(service.name)\n                    self.health_status[service.name] = {\n                        \"healthy\": is_healthy,\n                        \"status_code\": response.status,\n                        \"response_time\": response_time,\n                        \"checked_at\": check_start.isoformat(),\n                        \"error\": None\n                    }\n                    self.last_check[service.name] = check_start\n\n                    # Log status changes\n                    if previous_status is None:\n                        self.logger.info(f\"Initial health check for {service.name}: {'\u2705' if is_healthy else '\u274c'}\")\n                    elif previous_status[\"healthy\"] != is_healthy:\n                        status_change = \"recovered\" if is_healthy else \"failed\"\n                        self.logger.warning(f\"Service {service.name} {status_change}\")\n\n                        # Send alert for status changes\n                        await self._send_health_alert(service, is_healthy, response_time)\n\n                    # Log periodic status for healthy services\n                    if is_healthy:\n                        self.logger.debug(\n                            f\"Health check passed for {service.name}\",\n                            extra={\n                                \"service\": service.name,\n                                \"response_time\": response_time,\n                                \"status_code\": response.status\n                            }\n                        )\n\n        except asyncio.TimeoutError:\n            await self._handle_health_check_error(service, \"Timeout\", check_start)\n        except aiohttp.ClientError as e:\n            await self._handle_health_check_error(service, f\"Client error: {e}\", check_start)\n        except Exception as e:\n            await self._handle_health_check_error(service, f\"Unexpected error: {e}\", check_start)\n\n    async def _handle_health_check_error(self, service: HealthCheckConfig, error: str, check_start: datetime):\n        \"\"\"Handle health check errors\"\"\"\n        response_time = (datetime.now() - check_start).total_seconds()\n\n        previous_status = self.health_status.get(service.name)\n        self.health_status[service.name] = {\n            \"healthy\": False,\n            \"status_code\": None,\n            \"response_time\": response_time,\n            \"checked_at\": check_start.isoformat(),\n            \"error\": error\n        }\n        self.last_check[service.name] = check_start\n\n        # Log and alert on status change\n        if previous_status is None or previous_status.get(\"healthy\", False):\n            self.logger.error(f\"Health check failed for {service.name}: {error}\")\n            await self._send_health_alert(service, False, response_time, error)\n\n    async def _send_health_alert(self, service: HealthCheckConfig, is_healthy: bool, response_time: float, error: str = None):\n        \"\"\"Send health status alert\"\"\"\n        status = \"RECOVERED\" if is_healthy else \"FAILED\"\n\n        alert_data = {\n            \"service\": service.name,\n            \"status\": status,\n            \"url\": service.url,\n            \"response_time\": response_time,\n            \"timestamp\": datetime.now().isoformat()\n        }\n\n        if error:\n            alert_data[\"error\"] = error\n\n        # Send to monitoring system (implement based on your needs)\n        self.logger.warning(f\"ALERT: Service {service.name} {status}\", extra=alert_data)\n\n        # Here you would integrate with:\n        # - Slack/Teams notifications\n        # - PagerDuty/OpsGenie\n        # - Email alerts\n        # - Webhook notifications\n\n    async def get_health_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get overall health summary\"\"\"\n        healthy_count = sum(1 for status in self.health_status.values()\n                          if status and status.get(\"healthy\", False))\n        total_count = len(self.services)\n\n        # Find services with issues\n        unhealthy_services = []\n        for service_name, status in self.health_status.items():\n            if not status or not status.get(\"healthy\", False):\n                unhealthy_services.append({\n                    \"name\": service_name,\n                    \"error\": status.get(\"error\") if status else \"Not checked yet\",\n                    \"last_check\": self.last_check.get(service_name)\n                })\n\n        return {\n            \"overall_health\": \"healthy\" if healthy_count == total_count else \"degraded\",\n            \"healthy_services\": healthy_count,\n            \"total_services\": total_count,\n            \"health_percentage\": (healthy_count / total_count) * 100 if total_count &gt; 0 else 0,\n            \"unhealthy_services\": unhealthy_services,\n            \"last_update\": datetime.now().isoformat()\n        }\n\n    async def process_message(self, message):\n        \"\"\"Health check worker doesn't process traditional messages\"\"\"\n        # Could be used to process health check requests or configuration updates\n        pass\n\n# Usage example\nasync def run_health_monitor():\n    config = WorkerConfig(\n        worker_name=\"health-monitor\",\n        log_level=\"INFO\"\n    )\n\n    worker = HealthCheckWorker(config)\n    await worker.on_startup()\n\n    # Monitor health status\n    async def status_reporter():\n        while True:\n            await asyncio.sleep(300)  # Report every 5 minutes\n            summary = await worker.get_health_summary()\n            print(f\"\\n\ud83c\udfe5 Health Summary:\")\n            print(f\"   Overall: {summary['overall_health'].upper()}\")\n            print(f\"   Healthy: {summary['healthy_services']}/{summary['total_services']} ({summary['health_percentage']:.1f}%)\")\n\n            if summary['unhealthy_services']:\n                print(\"   \u274c Issues:\")\n                for service in summary['unhealthy_services']:\n                    print(f\"      - {service['name']}: {service['error']}\")\n\n    # Start monitoring and reporting\n    await asyncio.gather(\n        status_reporter(),\n        return_exceptions=True\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(run_health_monitor())\n</code></pre>"},{"location":"examples/http-workers/#best-practices","title":"Best Practices","text":""},{"location":"examples/http-workers/#1-rate-limiting-and-respect-api-limits","title":"1. Rate Limiting and Respect API Limits","text":"<pre><code>class RateLimitedPoller(HTTPPoller):\n    def __init__(self, *args, rate_limit: int = 60, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rate_limit = rate_limit  # requests per minute\n        self.request_times = []\n\n    async def _check_rate_limit(self):\n        \"\"\"Check and enforce rate limiting\"\"\"\n        now = datetime.now()\n\n        # Remove old requests (older than 1 minute)\n        self.request_times = [\n            req_time for req_time in self.request_times\n            if now - req_time &lt; timedelta(minutes=1)\n        ]\n\n        # Check if we're at the limit\n        if len(self.request_times) &gt;= self.rate_limit:\n            wait_time = 60 - (now - self.request_times[0]).seconds\n            await asyncio.sleep(wait_time)\n\n        self.request_times.append(now)\n</code></pre>"},{"location":"examples/http-workers/#2-circuit-breaker-pattern","title":"2. Circuit Breaker Pattern","text":"<pre><code>from pythia.http.circuit_breaker import CircuitBreaker\n\nclass ResilientHTTPWorker(Worker):\n    def __init__(self, config):\n        super().__init__(config)\n        self.circuit_breaker = CircuitBreaker(\n            failure_threshold=5,\n            recovery_timeout=60\n        )\n\n    async def make_http_request(self, url: str):\n        \"\"\"Make HTTP request with circuit breaker protection\"\"\"\n        try:\n            return await self.circuit_breaker.call(\n                self._http_request, url\n            )\n        except CircuitBreakerError:\n            # Circuit is open, use fallback\n            return await self._fallback_response()\n</code></pre>"},{"location":"examples/http-workers/#3-error-handling-and-retries","title":"3. Error Handling and Retries","text":"<pre><code>class RetryableHTTPWorker(Worker):\n    async def process_with_retry(self, request_func, max_retries: int = 3):\n        \"\"\"Process HTTP request with retry logic\"\"\"\n        for attempt in range(max_retries + 1):\n            try:\n                return await request_func()\n            except aiohttp.ClientTimeout:\n                if attempt &lt; max_retries:\n                    wait_time = 2 ** attempt  # Exponential backoff\n                    await asyncio.sleep(wait_time)\n                else:\n                    raise\n            except aiohttp.ClientError as e:\n                if attempt &lt; max_retries and self._is_retryable_error(e):\n                    await asyncio.sleep(2 ** attempt)\n                else:\n                    raise\n\n    def _is_retryable_error(self, error) -&gt; bool:\n        \"\"\"Determine if error is retryable\"\"\"\n        # Implement based on your needs\n        return isinstance(error, (aiohttp.ClientConnectorError, aiohttp.ServerTimeoutError))\n</code></pre>"},{"location":"examples/http-workers/#testing-http-workers","title":"Testing HTTP Workers","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\nimport aiohttp\n\nclass TestAPIPollingWorker:\n    @pytest.fixture\n    def worker(self):\n        config = WorkerConfig(worker_name=\"test-poller\")\n        return APIPollingWorker(config)\n\n    @pytest.mark.asyncio\n    async def test_github_event_processing(self, worker):\n        \"\"\"Test GitHub event processing\"\"\"\n        mock_events = [\n            {\n                \"id\": \"123\",\n                \"type\": \"PushEvent\",\n                \"actor\": {\"login\": \"testuser\"},\n                \"repo\": {\"name\": \"test/repo\"},\n                \"created_at\": \"2024-01-01T00:00:00Z\"\n            }\n        ]\n\n        message = Message(body=json.dumps(mock_events))\n        result = await worker.process_message(message)\n\n        assert result[\"status\"] == \"processed\"\n        assert result[\"events_processed\"] == 1\n\n    @pytest.mark.asyncio\n    async def test_webhook_signature_verification(self):\n        \"\"\"Test webhook signature verification\"\"\"\n        processor = WebhookProcessor(WorkerConfig())\n\n        # Mock Stripe webhook\n        body = '{\"type\":\"payment_intent.succeeded\"}'\n        signature = \"t=1234567890,v1=test_signature\"\n\n        with patch.object(processor, '_verify_stripe_signature') as mock_verify:\n            mock_verify.return_value = True\n\n            message = Message(\n                body=body,\n                headers={\"stripe-signature\": signature}\n            )\n\n            result = await processor._verify_webhook_signature(message, \"stripe\")\n            assert result is True\n</code></pre>"},{"location":"examples/http-workers/#next-steps","title":"Next Steps","text":"<ul> <li>Background Jobs - Job queue worker patterns</li> <li>Message Workers - Message-based worker examples</li> <li>Error Handling - Advanced error handling patterns</li> </ul>"},{"location":"getting-started/first-worker/","title":"Your First Worker","text":"<p>Step-by-step tutorial to build a complete worker with error handling, logging, and monitoring.</p>"},{"location":"getting-started/first-worker/#project-setup","title":"Project Setup","text":"<p>Create a new directory for your worker:</p> <pre><code>mkdir my-pythia-worker\ncd my-pythia-worker\n</code></pre> <p>Install Pythia:</p> <pre><code>pip install pythia-framework[redis]  # or [kafka] or [rabbitmq]\n</code></pre>"},{"location":"getting-started/first-worker/#step-1-basic-worker-structure","title":"Step 1: Basic Worker Structure","text":"<p>Create <code>worker.py</code>:</p> <pre><code>import asyncio\nimport json\nfrom datetime import datetime\nfrom typing import Any, Dict\n\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\nfrom pythia.logging import get_pythia_logger\n\n\nclass EmailProcessorWorker(Worker):\n    \"\"\"\n    Email processing worker that validates and processes email messages\n    \"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.processed_count = 0\n\n    async def process_message(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Process an email message\"\"\"\n        self.logger.info(f\"Processing message {message.message_id}\")\n\n        try:\n            # Parse the message body\n            email_data = json.loads(message.body)\n\n            # Validate required fields\n            if not self._validate_email_data(email_data):\n                raise ValueError(\"Invalid email data structure\")\n\n            # Process the email\n            result = await self._process_email(email_data)\n\n            self.processed_count += 1\n            self.logger.info(f\"Successfully processed email to {email_data['to']}\")\n\n            return result\n\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Failed to parse message body: {e}\")\n            raise\n\n        except Exception as e:\n            self.logger.error(f\"Error processing message: {e}\")\n            raise\n\n    def _validate_email_data(self, data: Dict[str, Any]) -&gt; bool:\n        \"\"\"Validate email data structure\"\"\"\n        required_fields = ['to', 'subject', 'body']\n        return all(field in data for field in required_fields)\n\n    async def _process_email(self, email_data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process the email (simulate sending)\"\"\"\n        # Simulate email processing delay\n        await asyncio.sleep(0.1)\n\n        # Return processing result\n        return {\n            \"status\": \"sent\",\n            \"to\": email_data[\"to\"],\n            \"subject\": email_data[\"subject\"],\n            \"processed_at\": datetime.now().isoformat(),\n            \"worker_id\": self.config.worker_id,\n            \"message_count\": self.processed_count\n        }\n\n    async def on_startup(self):\n        \"\"\"Called when worker starts\"\"\"\n        self.logger.info(\"Email processor worker starting up\")\n        await super().on_startup()\n\n    async def on_shutdown(self):\n        \"\"\"Called when worker shuts down\"\"\"\n        self.logger.info(f\"Email processor shutting down. Processed {self.processed_count} messages\")\n        await super().on_shutdown()\n</code></pre>"},{"location":"getting-started/first-worker/#step-2-configuration","title":"Step 2: Configuration","text":"<p>Create <code>config.py</code>:</p> <pre><code>from pythia.config import WorkerConfig\nfrom pythia.config.redis import RedisConfig\n\ndef get_worker_config() -&gt; WorkerConfig:\n    \"\"\"Get worker configuration\"\"\"\n    return WorkerConfig(\n        worker_name=\"email-processor\",\n        broker_type=\"redis\",\n        max_concurrent=5,\n        max_retries=3,\n        retry_delay=1.0,\n        log_level=\"INFO\",\n        health_check_interval=30\n    )\n\ndef get_redis_config() -&gt; RedisConfig:\n    \"\"\"Get Redis broker configuration\"\"\"\n    return RedisConfig(\n        host=\"localhost\",\n        port=6379,\n        db=0,\n        queue_name=\"email-queue\",\n        connection_pool_size=10\n    )\n</code></pre>"},{"location":"getting-started/first-worker/#step-3-main-application","title":"Step 3: Main Application","text":"<p>Create <code>main.py</code>:</p> <pre><code>import asyncio\nimport signal\nimport sys\nfrom worker import EmailProcessorWorker\nfrom config import get_worker_config\n\nasync def main():\n    \"\"\"Main application entry point\"\"\"\n    # Create worker configuration\n    config = get_worker_config()\n\n    # Create and start worker\n    worker = EmailProcessorWorker(config)\n\n    # Setup graceful shutdown\n    def signal_handler():\n        print(\"Received shutdown signal...\")\n        asyncio.create_task(worker.stop())\n\n    # Register signal handlers\n    for sig in (signal.SIGTERM, signal.SIGINT):\n        signal.signal(sig, lambda s, f: signal_handler())\n\n    try:\n        # Start the worker\n        await worker.start()\n    except KeyboardInterrupt:\n        print(\"Received keyboard interrupt\")\n    except Exception as e:\n        print(f\"Worker failed with error: {e}\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"getting-started/first-worker/#step-4-testing","title":"Step 4: Testing","text":"<p>Create <code>test_worker.py</code>:</p> <pre><code>import pytest\nimport json\nfrom pythia.utils.testing import WorkerTestCase\nfrom worker import EmailProcessorWorker\nfrom config import get_worker_config\n\nclass TestEmailProcessorWorker(WorkerTestCase):\n    worker_class = EmailProcessorWorker\n    config = get_worker_config()\n\n    async def test_valid_email_processing(self):\n        \"\"\"Test processing valid email message\"\"\"\n        email_data = {\n            \"to\": \"user@example.com\",\n            \"subject\": \"Test Email\",\n            \"body\": \"This is a test email\"\n        }\n\n        message = self.create_test_message(json.dumps(email_data))\n        result = await self.worker.process_message(message)\n\n        assert result[\"status\"] == \"sent\"\n        assert result[\"to\"] == \"user@example.com\"\n        assert result[\"subject\"] == \"Test Email\"\n        assert \"processed_at\" in result\n\n    async def test_invalid_email_data(self):\n        \"\"\"Test handling invalid email data\"\"\"\n        invalid_data = {\"to\": \"user@example.com\"}  # Missing required fields\n\n        message = self.create_test_message(json.dumps(invalid_data))\n\n        with pytest.raises(ValueError):\n            await self.worker.process_message(message)\n\n    async def test_malformed_json(self):\n        \"\"\"Test handling malformed JSON\"\"\"\n        message = self.create_test_message(\"invalid json\")\n\n        with pytest.raises(json.JSONDecodeError):\n            await self.worker.process_message(message)\n\n# Run tests\nif __name__ == \"__main__\":\n    pytest.main([__file__])\n</code></pre>"},{"location":"getting-started/first-worker/#step-5-running-your-worker","title":"Step 5: Running Your Worker","text":"<ol> <li> <p>Start Redis (if using Redis):    <pre><code>redis-server\n</code></pre></p> </li> <li> <p>Run the worker:    <pre><code>python main.py\n</code></pre></p> </li> <li> <p>Send test messages (in another terminal):    <pre><code>redis-cli LPUSH email-queue '{\"to\": \"test@example.com\", \"subject\": \"Hello\", \"body\": \"Test message\"}'\n</code></pre></p> </li> <li> <p>Run tests:    <pre><code>python test_worker.py\n</code></pre></p> </li> </ol>"},{"location":"getting-started/first-worker/#step-6-production-deployment","title":"Step 6: Production Deployment","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\nservices:\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n\n  email-worker:\n    build: .\n    depends_on:\n      - redis\n    environment:\n      - PYTHIA_BROKER_TYPE=redis\n      - PYTHIA_LOG_LEVEL=INFO\n      - REDIS_HOST=redis\n    restart: unless-stopped\n    deploy:\n      replicas: 3\n</code></pre> <p>Create <code>Dockerfile</code>:</p> <pre><code>FROM python:3.11-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"main.py\"]\n</code></pre>"},{"location":"getting-started/first-worker/#what-youve-built","title":"What You've Built","text":"<p>\u2705 Complete worker with error handling and logging \u2705 Configuration management with environment support \u2705 Testing suite with comprehensive test cases \u2705 Production deployment with Docker and scaling \u2705 Graceful shutdown handling \u2705 Performance monitoring built-in</p>"},{"location":"getting-started/first-worker/#next-steps","title":"Next Steps","text":"<ul> <li>Worker Lifecycle - Deep dive into worker internals</li> <li>Configuration Guide - Advanced configuration options</li> <li>Error Handling - Robust error handling patterns</li> <li>Monitoring - Production monitoring setup</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get Pythia up and running in minutes.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>Operating System: Linux, macOS, or Windows</li> <li>Message Broker: Redis, Kafka, or RabbitMQ (optional for development)</li> </ul>"},{"location":"getting-started/installation/#install-from-pypi","title":"Install from PyPI","text":"<pre><code>pip install pythia-framework\n</code></pre>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>For the latest development version:</p> <pre><code>git clone https://github.com/ralonso20/pythia.git\ncd pythia\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#broker-specific-dependencies","title":"Broker-Specific Dependencies","text":"<p>Choose your message broker and install the corresponding dependencies:</p> RedisKafkaRabbitMQAll Brokers <pre><code>pip install pythia-framework[redis]\n</code></pre> <pre><code>pip install pythia-framework[kafka]\n</code></pre> <pre><code>pip install pythia-framework[rabbitmq]\n</code></pre> <pre><code>pip install pythia-framework[all]\n</code></pre>"},{"location":"getting-started/installation/#quick-setup-test","title":"Quick Setup Test","text":"<p>Verify your installation:</p> <pre><code>import pythia\nprint(f\"Pythia version: {pythia.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Create your first worker in 5 minutes</li> <li>Your First Worker - Step-by-step tutorial</li> <li>Configuration - Advanced setup options</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get your first Pythia worker running in 5 minutes.</p>"},{"location":"getting-started/quick-start/#create-your-first-worker","title":"Create Your First Worker","text":"<p>Pythia workers are Python classes that inherit from the base <code>Worker</code> class:</p> <pre><code>import asyncio\nfrom pythia.core import Worker, Message\nfrom pythia.config import WorkerConfig\n\nclass MyFirstWorker(Worker):\n    \"\"\"A simple message processing worker\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process a single message\"\"\"\n        print(f\"Processing message: {message.body}\")\n\n        # Your business logic here\n        result = message.body.upper()\n\n        return {\"processed\": result, \"worker\": \"my-first-worker\"}\n\n# Configuration\nconfig = WorkerConfig(\n    worker_name=\"my-first-worker\",\n    broker_type=\"redis\",  # or \"kafka\", \"rabbitmq\"\n    max_concurrent=5\n)\n\n# Run the worker\nif __name__ == \"__main__\":\n    worker = MyFirstWorker(config=config)\n    asyncio.run(worker.start())\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-with-auto-detection","title":"Configuration with Auto-Detection","text":"<p>Pythia can auto-detect your broker configuration:</p> <pre><code>from pythia.config import auto_detect_config\n\n# Auto-detect from environment or running services\nconfig = auto_detect_config()\nworker = MyFirstWorker(config=config)\n</code></pre>"},{"location":"getting-started/quick-start/#environment-configuration","title":"Environment Configuration","text":"<p>Set configuration via environment variables:</p> <pre><code>export PYTHIA_WORKER_NAME=\"my-worker\"\nexport PYTHIA_BROKER_TYPE=\"kafka\"\nexport PYTHIA_MAX_CONCURRENT=10\nexport PYTHIA_LOG_LEVEL=\"DEBUG\"\n</code></pre> <pre><code>from pythia.config import WorkerConfig\n\n# Automatically loads from environment\nconfig = WorkerConfig()\nworker = MyFirstWorker(config=config)\n</code></pre>"},{"location":"getting-started/quick-start/#broker-specific-quick-start","title":"Broker-Specific Quick Start","text":"RedisKafkaRabbitMQ <pre><code>from pythia.config.redis import RedisConfig\n\nredis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    queue_name=\"my-queue\"\n)\n\nconfig = WorkerConfig(broker_type=\"redis\")\nworker = MyFirstWorker(config=config)\n</code></pre> <pre><code>from pythia.config.kafka import KafkaConfig\n\nkafka_config = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    topic=\"my-topic\",\n    group_id=\"my-group\"\n)\n\nconfig = WorkerConfig(broker_type=\"kafka\")\nworker = MyFirstWorker(config=config)\n</code></pre> <pre><code>from pythia.config.rabbitmq import RabbitMQConfig\n\nrabbitmq_config = RabbitMQConfig(\n    host=\"localhost\",\n    queue_name=\"my-queue\"\n)\n\nconfig = WorkerConfig(broker_type=\"rabbitmq\")\nworker = MyFirstWorker(config=config)\n</code></pre>"},{"location":"getting-started/quick-start/#testing-your-worker","title":"Testing Your Worker","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\n\nclass TestMyFirstWorker(WorkerTestCase):\n    worker_class = MyFirstWorker\n\n    async def test_message_processing(self):\n        message = self.create_test_message({\"data\": \"hello\"})\n        result = await self.worker.process_message(message)\n\n        assert result[\"processed\"] == \"HELLO\"\n        assert result[\"worker\"] == \"my-first-worker\"\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Your First Worker - Detailed tutorial with real examples</li> <li>Worker Lifecycle - Understanding worker startup and shutdown</li> <li>Configuration Guide - Complete configuration options</li> </ul>"},{"location":"performance/benchmarks/","title":"Performance Benchmarks","text":"<p>Comprehensive performance analysis of Pythia across different message brokers.</p>"},{"location":"performance/benchmarks/#benchmark-overview","title":"Benchmark Overview","text":"<p>Our benchmarking infrastructure validates performance across Redis, Kafka, and RabbitMQ using standardized workloads and monitoring.</p>"},{"location":"performance/benchmarks/#test-environment","title":"Test Environment","text":"<ul> <li>Machine: macOS (Darwin 24.5.0)</li> <li>Python: 3.11</li> <li>Test Duration: 60 seconds per test</li> <li>Message Size: 1KB</li> <li>Monitoring: Prometheus + Grafana + InfluxDB</li> </ul>"},{"location":"performance/benchmarks/#performance-results","title":"Performance Results","text":""},{"location":"performance/benchmarks/#overall-performance-summary","title":"\ud83c\udfc6 Overall Performance Summary","text":"Broker Throughput P95 Latency P99 Latency CPU Usage Memory Usage Redis 3,304 msg/s 0.6ms 2.2ms 4.2% 7,877 MB Kafka 1,872 msg/s 2.0ms 5.0ms 9.0% 7,728 MB RabbitMQ 1,292 msg/s 0.0ms 0.0ms 6.8% 7,893 MB"},{"location":"performance/benchmarks/#detailed-results-by-configuration","title":"\ud83d\udcca Detailed Results by Configuration","text":""},{"location":"performance/benchmarks/#redis-performance","title":"Redis Performance","text":"<pre><code>Configuration: 3000 msg/s target, 4 workers\nActual Throughput: 3,304 msg/s (110% of target)\nLatency P95: 0.618ms\nLatency P99: 2.24ms\nCPU Usage: 4.23%\nError Rate: 0%\n</code></pre>"},{"location":"performance/benchmarks/#kafka-performance","title":"Kafka Performance","text":"<pre><code>Configuration: 3000 msg/s target, 4 workers\nActual Throughput: 1,872 msg/s (62% of target)\nLatency P95: 2.0ms\nLatency P99: 5.0ms\nCPU Usage: 9.05%\nError Rate: 0%\n</code></pre>"},{"location":"performance/benchmarks/#rabbitmq-performance","title":"RabbitMQ Performance","text":"<pre><code>Configuration: 2500 msg/s target, 4 workers\nActual Throughput: 1,292 msg/s (52% of target)\nLatency P95: 0.0ms (simplified measurement)\nLatency P99: 0.0ms (simplified measurement)\nCPU Usage: 6.78%\nError Rate: 0%\n</code></pre>"},{"location":"performance/benchmarks/#performance-analysis","title":"Performance Analysis","text":""},{"location":"performance/benchmarks/#redis-outstanding-performance","title":"\ud83d\ude80 Redis: Outstanding Performance","text":"<ul> <li>443% improvement over baseline expectations</li> <li>Sub-millisecond P95 latency with high throughput</li> <li>Minimal CPU overhead at 4.2% usage</li> <li>Excellent scaling with worker count</li> </ul>"},{"location":"performance/benchmarks/#kafka-production-ready","title":"\u26a1 Kafka: Production Ready","text":"<ul> <li>Reliable throughput with enterprise-grade durability</li> <li>Consistent performance under load</li> <li>Good resource efficiency for persistent messaging</li> <li>Excellent for event streaming use cases</li> </ul>"},{"location":"performance/benchmarks/#rabbitmq-balanced-performance","title":"\ud83d\udc30 RabbitMQ: Balanced Performance","text":"<ul> <li>Steady throughput with advanced routing</li> <li>Low resource usage for complex topologies</li> <li>Enterprise messaging features with good performance</li> <li>Ideal for complex routing scenarios</li> </ul>"},{"location":"performance/benchmarks/#optimization-insights","title":"Optimization Insights","text":""},{"location":"performance/benchmarks/#redis-optimization","title":"Redis Optimization","text":"<pre><code># Optimal Redis configuration for high throughput\nREDIS_CONFIG = {\n    \"connection_pool_size\": 20,\n    \"socket_keepalive\": True,\n    \"socket_keepalive_options\": {},\n    \"retry_on_timeout\": True,\n    \"health_check_interval\": 30\n}\n</code></pre>"},{"location":"performance/benchmarks/#kafka-optimization","title":"Kafka Optimization","text":"<pre><code># Optimal Kafka producer configuration\nKAFKA_CONFIG = {\n    \"batch_size\": 16384,\n    \"linger_ms\": 5,\n    \"compression_type\": \"gzip\",\n    \"acks\": \"1\",\n    \"retries\": 3\n}\n</code></pre>"},{"location":"performance/benchmarks/#rabbitmq-optimization","title":"RabbitMQ Optimization","text":"<pre><code># Optimal RabbitMQ configuration\nRABBITMQ_CONFIG = {\n    \"prefetch_count\": 100,\n    \"connection_timeout\": 10,\n    \"heartbeat\": 600,\n    \"confirm_publish\": True\n}\n</code></pre>"},{"location":"performance/benchmarks/#scaling-recommendations","title":"Scaling Recommendations","text":""},{"location":"performance/benchmarks/#high-throughput-2000-msgs","title":"High Throughput (&gt;2000 msg/s)","text":"<ul> <li>Primary: Redis with 4+ workers</li> <li>Alternative: Kafka with optimized batching</li> <li>Monitoring: Essential for sustained performance</li> </ul>"},{"location":"performance/benchmarks/#balanced-workloads-500-2000-msgs","title":"Balanced Workloads (500-2000 msg/s)","text":"<ul> <li>Primary: Any broker with 2-4 workers</li> <li>Focus: Match broker features to use case</li> <li>Configuration: Use production-optimized settings</li> </ul>"},{"location":"performance/benchmarks/#low-latency-critical-1ms-p95","title":"Low Latency Critical (&lt;1ms P95)","text":"<ul> <li>Primary: Redis with connection pooling</li> <li>Secondary: RabbitMQ with prefetch optimization</li> <li>Avoid: High-throughput Kafka configurations</li> </ul>"},{"location":"performance/benchmarks/#benchmark-reproducibility","title":"Benchmark Reproducibility","text":"<p>All benchmarks are reproducible using our comprehensive test suite:</p> <pre><code># Run complete benchmark suite\n./benchmarks/run_benchmarks.sh\n\n# Run specific broker benchmarks\npython benchmarks/benchmark_cli.py --broker redis --duration 60\npython benchmarks/benchmark_cli.py --broker kafka --duration 60\npython benchmarks/benchmark_cli.py --broker rabbitmq --duration 60\n</code></pre>"},{"location":"performance/benchmarks/#next-steps","title":"Next Steps","text":"<ul> <li>Optimization Guide - Detailed tuning recommendations</li> <li>Scaling Guide - Multi-instance deployment patterns</li> <li>Monitoring Setup - Production observability</li> </ul>"},{"location":"performance/optimization/","title":"Performance Optimization","text":"<p>This guide covers performance optimization techniques for Pythia workers and messaging systems.</p>"},{"location":"performance/optimization/#worker-optimization","title":"\ud83c\udfaf Worker Optimization","text":""},{"location":"performance/optimization/#message-processing","title":"Message Processing","text":"<pre><code>from pythia import Worker\nfrom pythia.brokers.kafka import KafkaConsumer\n\nclass OptimizedWorker(Worker):\n    source = KafkaConsumer(\n        topic=\"orders\",\n        batch_size=100,  # Process in batches\n        max_poll_records=500,\n        fetch_min_bytes=1024,\n    )\n\n    async def process(self, message):\n        # Minimize processing time\n        await self.handle_message_fast(message)\n\n    async def handle_message_fast(self, message):\n        # Use async operations for I/O\n        async with self.session.begin():\n            await self.save_to_database(message.body)\n</code></pre>"},{"location":"performance/optimization/#batch-processing","title":"Batch Processing","text":"<pre><code>from pythia import BatchWorker\n\nclass BatchOptimizedWorker(BatchWorker):\n    source = KafkaConsumer(topic=\"events\")\n    batch_size = 50\n    batch_timeout = 5.0\n\n    async def process_batch(self, messages):\n        # Process multiple messages at once\n        data = [msg.body for msg in messages]\n        await self.bulk_insert(data)\n</code></pre>"},{"location":"performance/optimization/#broker-optimization","title":"\ud83d\ude80 Broker Optimization","text":""},{"location":"performance/optimization/#kafka-configuration","title":"Kafka Configuration","text":"<pre><code>from pythia.brokers.kafka import KafkaConfig, KafkaConsumer\n\nconfig = KafkaConfig(\n    bootstrap_servers=\"localhost:9092\",\n    # Consumer optimization\n    session_timeout_ms=30000,\n    heartbeat_interval_ms=3000,\n    max_poll_interval_ms=300000,\n    # Performance tuning\n    fetch_min_bytes=50000,\n    fetch_max_wait_ms=500,\n    max_partition_fetch_bytes=1048576,\n    # Connection pooling\n    connections_max_idle_ms=540000,\n    request_timeout_ms=30000,\n)\n\nconsumer = KafkaConsumer(\n    topic=\"high_volume_topic\",\n    config=config,\n    batch_size=1000,\n)\n</code></pre>"},{"location":"performance/optimization/#redis-optimization","title":"Redis Optimization","text":"<pre><code>from pythia.brokers.redis import RedisConfig, RedisStreamsConsumer\n\nconfig = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    # Connection pooling\n    max_connections=20,\n    retry_on_timeout=True,\n    socket_keepalive=True,\n    socket_keepalive_options={},\n    # Performance settings\n    socket_connect_timeout=5,\n    socket_timeout=5,\n)\n\nconsumer = RedisStreamsConsumer(\n    stream=\"orders\",\n    config=config,\n    batch_size=100,\n    block_time=1000,  # 1 second blocking\n)\n</code></pre>"},{"location":"performance/optimization/#rabbitmq-optimization","title":"RabbitMQ Optimization","text":"<pre><code>from pythia.brokers.rabbitmq import RabbitMQConfig, RabbitMQConsumer\n\nconfig = RabbitMQConfig(\n    host=\"localhost\",\n    port=5672,\n    # Connection optimization\n    heartbeat=600,\n    connection_attempts=3,\n    retry_delay=5,\n    # Performance settings\n    socket_timeout=5,\n    stack_timeout=5,\n)\n\nconsumer = RabbitMQConsumer(\n    queue=\"orders\",\n    config=config,\n    prefetch_count=100,  # Process multiple messages\n    auto_ack=False,\n)\n</code></pre>"},{"location":"performance/optimization/#memory-optimization","title":"\ud83d\udcca Memory Optimization","text":""},{"location":"performance/optimization/#message-handling","title":"Message Handling","text":"<pre><code>class MemoryEfficientWorker(Worker):\n    source = KafkaConsumer(topic=\"large_messages\")\n\n    async def process(self, message):\n        # Process message in chunks for large payloads\n        if len(message.body) &gt; 10_000_000:  # 10MB\n            await self.process_large_message(message)\n        else:\n            await self.process_normal_message(message)\n\n    async def process_large_message(self, message):\n        # Stream processing for large messages\n        chunk_size = 1024 * 1024  # 1MB chunks\n        for i in range(0, len(message.body), chunk_size):\n            chunk = message.body[i:i + chunk_size]\n            await self.process_chunk(chunk)\n</code></pre>"},{"location":"performance/optimization/#connection-pooling","title":"Connection Pooling","text":"<pre><code>from pythia import Worker\nimport aiohttp\n\nclass PooledHTTPWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Reuse HTTP connections\n        self.session = None\n\n    async def startup(self):\n        await super().startup()\n        connector = aiohttp.TCPConnector(\n            limit=100,  # Total connection limit\n            limit_per_host=30,  # Per-host connection limit\n            keepalive_timeout=300,\n        )\n        self.session = aiohttp.ClientSession(connector=connector)\n\n    async def shutdown(self):\n        if self.session:\n            await self.session.close()\n        await super().shutdown()\n</code></pre>"},{"location":"performance/optimization/#cpu-optimization","title":"\u26a1 CPU Optimization","text":""},{"location":"performance/optimization/#asyncawait-best-practices","title":"Async/Await Best Practices","text":"<pre><code>import asyncio\n\nclass AsyncOptimizedWorker(Worker):\n    async def process(self, message):\n        # Run CPU-intensive tasks in executor\n        if message.headers.get(\"cpu_intensive\"):\n            result = await asyncio.get_event_loop().run_in_executor(\n                None, self.cpu_intensive_task, message.body\n            )\n        else:\n            result = await self.async_task(message.body)\n\n        return result\n\n    def cpu_intensive_task(self, data):\n        # Synchronous CPU-bound operation\n        return self.heavy_computation(data)\n\n    async def async_task(self, data):\n        # I/O-bound async operation\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\"/api/process\", json=data) as response:\n                return await response.json()\n</code></pre>"},{"location":"performance/optimization/#concurrency-control","title":"Concurrency Control","text":"<pre><code>import asyncio\n\nclass ConcurrencyControlWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Limit concurrent operations\n        self.semaphore = asyncio.Semaphore(10)\n\n    async def process(self, message):\n        async with self.semaphore:\n            # Process with concurrency limit\n            await self.handle_message(message)\n</code></pre>"},{"location":"performance/optimization/#system-level-optimization","title":"\ud83d\udd27 System-Level Optimization","text":""},{"location":"performance/optimization/#worker-configuration","title":"Worker Configuration","text":"<pre><code>from pythia.config import WorkerConfig\n\nconfig = WorkerConfig(\n    # Processing configuration\n    batch_size=100,\n    batch_timeout=5.0,\n    max_retries=3,\n    retry_delay=1.0,\n\n    # Resource limits\n    max_memory_mb=512,\n    max_cpu_percent=80,\n\n    # Monitoring\n    metrics_enabled=True,\n    health_check_interval=30,\n)\n</code></pre>"},{"location":"performance/optimization/#environment-variables","title":"Environment Variables","text":"<pre><code># Python optimizations\nexport PYTHONOPTIMIZE=1\nexport PYTHONDONTWRITEBYTECODE=1\n\n# asyncio optimizations\nexport PYTHONASYNCIODEBUG=0\n\n# Memory settings\nexport MALLOC_ARENA_MAX=2\n\n# Worker settings\nexport PYTHIA_BATCH_SIZE=100\nexport PYTHIA_MAX_WORKERS=4\nexport PYTHIA_PREFETCH_COUNT=200\n</code></pre>"},{"location":"performance/optimization/#monitoring-profiling","title":"\ud83d\udcc8 Monitoring &amp; Profiling","text":""},{"location":"performance/optimization/#built-in-metrics","title":"Built-in Metrics","text":"<pre><code>from pythia import Worker\nfrom pythia.monitoring import MetricsCollector\n\nclass MonitoredWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.metrics = MetricsCollector()\n\n    async def process(self, message):\n        with self.metrics.timer(\"message_processing_time\"):\n            result = await self.handle_message(message)\n            self.metrics.increment(\"messages_processed\")\n            return result\n</code></pre>"},{"location":"performance/optimization/#performance-testing","title":"Performance Testing","text":"<pre><code>import time\nimport asyncio\nfrom pythia.testing import WorkerTestCase\n\nclass PerformanceTest(WorkerTestCase):\n    async def test_throughput(self):\n        worker = self.create_worker()\n\n        # Send 1000 messages\n        messages = [self.create_message(i) for i in range(1000)]\n\n        start_time = time.time()\n        await worker.process_batch(messages)\n        end_time = time.time()\n\n        throughput = len(messages) / (end_time - start_time)\n        self.assertGreater(throughput, 100)  # &gt; 100 msg/sec\n</code></pre>"},{"location":"performance/optimization/#performance-checklist","title":"\ud83c\udfaf Performance Checklist","text":""},{"location":"performance/optimization/#pre-production-checklist","title":"Pre-Production Checklist","text":"<ul> <li> Batch Processing: Use appropriate batch sizes (50-1000)</li> <li> Connection Pooling: Configure broker connection limits</li> <li> Memory Limits: Set max memory per worker</li> <li> CPU Optimization: Use async/await properly</li> <li> Monitoring: Enable metrics collection</li> <li> Resource Limits: Configure system resource limits</li> <li> Error Handling: Implement efficient retry strategies</li> <li> Testing: Run load tests with realistic data</li> </ul>"},{"location":"performance/optimization/#common-performance-issues","title":"Common Performance Issues","text":"<ol> <li>Small Batch Sizes: Use batches of 50-1000 messages</li> <li>Blocking I/O: Always use async operations</li> <li>Memory Leaks: Monitor memory usage over time</li> <li>Connection Exhaustion: Configure connection pooling</li> <li>CPU Bottlenecks: Profile and optimize hot paths</li> </ol>"},{"location":"performance/optimization/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Asyncio Performance Tips</li> <li>Kafka Performance Tuning</li> <li>Redis Performance Optimization</li> <li>RabbitMQ Performance Tuning</li> </ul>"},{"location":"performance/scaling/","title":"Scaling Pythia Workers","text":"<p>This guide covers horizontal and vertical scaling strategies for Pythia-based applications.</p>"},{"location":"performance/scaling/#horizontal-scaling","title":"\ud83d\ude80 Horizontal Scaling","text":""},{"location":"performance/scaling/#multiple-worker-processes","title":"Multiple Worker Processes","text":"<pre><code># worker.py\nfrom pythia import Worker\nfrom pythia.brokers.kafka import KafkaConsumer\n\nclass OrderProcessor(Worker):\n    source = KafkaConsumer(\n        topic=\"orders\",\n        consumer_group=\"order-processors\",  # Scale with multiple consumers\n    )\n\n    async def process(self, message):\n        await self.process_order(message.body)\n</code></pre> <pre><code># Run multiple instances\npython worker.py &amp;  # Process 1\npython worker.py &amp;  # Process 2\npython worker.py &amp;  # Process 3\npython worker.py &amp;  # Process 4\n</code></pre>"},{"location":"performance/scaling/#docker-scaling","title":"Docker Scaling","text":"<pre><code># docker-compose.yml\nversion: '3.8'\nservices:\n  order-processor:\n    build: .\n    command: python worker.py\n    environment:\n      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092\n      - CONSUMER_GROUP=order-processors\n    deploy:\n      replicas: 4  # Run 4 instances\n    depends_on:\n      - kafka\n</code></pre>"},{"location":"performance/scaling/#kubernetes-scaling","title":"Kubernetes Scaling","text":"<pre><code># deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-processor\nspec:\n  replicas: 10  # Start with 10 replicas\n  selector:\n    matchLabels:\n      app: order-processor\n  template:\n    metadata:\n      labels:\n        app: order-processor\n    spec:\n      containers:\n      - name: worker\n        image: my-app/order-processor:latest\n        env:\n        - name: KAFKA_BOOTSTRAP_SERVERS\n          value: \"kafka:9092\"\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n---\n# Auto-scaling\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: order-processor-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: order-processor\n  minReplicas: 5\n  maxReplicas: 50\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"performance/scaling/#vertical-scaling","title":"\u2b06\ufe0f Vertical Scaling","text":""},{"location":"performance/scaling/#multi-threaded-processing","title":"Multi-threaded Processing","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom pythia import Worker\n\nclass VerticalScaledWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Create thread pool for CPU-intensive tasks\n        self.executor = ThreadPoolExecutor(max_workers=4)\n\n    async def process(self, message):\n        # Offload CPU-intensive work to thread pool\n        if message.headers.get(\"cpu_intensive\"):\n            loop = asyncio.get_event_loop()\n            result = await loop.run_in_executor(\n                self.executor,\n                self.cpu_bound_task,\n                message.body\n            )\n        else:\n            result = await self.io_bound_task(message.body)\n\n        return result\n\n    def cpu_bound_task(self, data):\n        # CPU-intensive synchronous processing\n        return self.heavy_computation(data)\n\n    async def io_bound_task(self, data):\n        # Async I/O operations\n        async with aiohttp.ClientSession() as session:\n            return await self.call_api(session, data)\n</code></pre>"},{"location":"performance/scaling/#resource-configuration","title":"Resource Configuration","text":"<pre><code>from pythia.config import WorkerConfig\n\n# High-performance worker configuration\nconfig = WorkerConfig(\n    # Increase batch processing\n    batch_size=500,\n    batch_timeout=2.0,\n\n    # Optimize memory usage\n    max_memory_mb=2048,  # 2GB limit\n\n    # Increase concurrency\n    max_concurrent_tasks=20,\n\n    # Connection pooling\n    max_connections=50,\n)\n</code></pre>"},{"location":"performance/scaling/#load-balancing-strategies","title":"\ud83d\udcca Load Balancing Strategies","text":""},{"location":"performance/scaling/#consumer-groups-kafka","title":"Consumer Groups (Kafka)","text":"<pre><code># Worker instance 1\nworker1 = Worker()\nworker1.source = KafkaConsumer(\n    topic=\"orders\",\n    consumer_group=\"order-processors\",  # Same group\n    partition_assignment_strategy=\"range\"\n)\n\n# Worker instance 2\nworker2 = Worker()\nworker2.source = KafkaConsumer(\n    topic=\"orders\",\n    consumer_group=\"order-processors\",  # Same group\n    partition_assignment_strategy=\"range\"\n)\n</code></pre>"},{"location":"performance/scaling/#queue-distribution-rabbitmq","title":"Queue Distribution (RabbitMQ)","text":"<pre><code>from pythia.brokers.rabbitmq import RabbitMQConsumer\n\n# Multiple workers consuming from same queue\nclass LoadBalancedWorker(Worker):\n    source = RabbitMQConsumer(\n        queue=\"tasks\",\n        prefetch_count=10,  # Process 10 messages at once\n        auto_ack=False,     # Manual acknowledgment\n    )\n</code></pre>"},{"location":"performance/scaling/#redis-streams-consumer-groups","title":"Redis Streams Consumer Groups","text":"<pre><code>from pythia.brokers.redis import RedisStreamsConsumer\n\nclass StreamWorker(Worker):\n    source = RedisStreamsConsumer(\n        stream=\"events\",\n        consumer_group=\"processors\",\n        consumer_name=\"worker-1\",  # Unique name per instance\n    )\n</code></pre>"},{"location":"performance/scaling/#geographic-distribution","title":"\ud83c\udf0d Geographic Distribution","text":""},{"location":"performance/scaling/#multi-region-setup","title":"Multi-Region Setup","text":"<pre><code># US East worker\nclass USEastWorker(Worker):\n    source = KafkaConsumer(\n        topic=\"orders-us-east\",\n        bootstrap_servers=\"kafka-us-east:9092\"\n    )\n\n# EU West worker\nclass EUWestWorker(Worker):\n    source = KafkaConsumer(\n        topic=\"orders-eu-west\",\n        bootstrap_servers=\"kafka-eu-west:9092\"\n    )\n</code></pre>"},{"location":"performance/scaling/#cross-region-replication","title":"Cross-Region Replication","text":"<pre><code># Kafka cross-region replication\nversion: '3.8'\nservices:\n  mirror-maker:\n    image: confluentinc/cp-kafka:latest\n    command: &gt;\n      kafka-mirror-maker\n      --consumer.config /config/consumer.properties\n      --producer.config /config/producer.properties\n      --whitelist=\"orders.*\"\n    volumes:\n      - ./config:/config\n</code></pre>"},{"location":"performance/scaling/#scaling-patterns","title":"\ud83d\udcc8 Scaling Patterns","text":""},{"location":"performance/scaling/#fan-out-pattern","title":"Fan-Out Pattern","text":"<pre><code>from pythia import Worker\nfrom pythia.brokers.kafka import KafkaConsumer, KafkaProducer\n\nclass FanOutWorker(Worker):\n    source = KafkaConsumer(topic=\"incoming\")\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Multiple output topics for scaling\n        self.order_producer = KafkaProducer(topic=\"orders\")\n        self.inventory_producer = KafkaProducer(topic=\"inventory\")\n        self.billing_producer = KafkaProducer(topic=\"billing\")\n\n    async def process(self, message):\n        event = message.body\n\n        # Fan out to multiple specialized workers\n        if event[\"type\"] == \"order\":\n            await self.order_producer.send(event)\n        elif event[\"type\"] == \"inventory\":\n            await self.inventory_producer.send(event)\n        elif event[\"type\"] == \"billing\":\n            await self.billing_producer.send(event)\n</code></pre>"},{"location":"performance/scaling/#pipeline-pattern","title":"Pipeline Pattern","text":"<pre><code># Stage 1: Data ingestion\nclass DataIngestionWorker(Worker):\n    source = KafkaConsumer(topic=\"raw-data\")\n    sink = KafkaProducer(topic=\"validated-data\")\n\n    async def process(self, message):\n        validated = await self.validate_data(message.body)\n        await self.sink.send(validated)\n\n# Stage 2: Data processing\nclass DataProcessingWorker(Worker):\n    source = KafkaConsumer(topic=\"validated-data\")\n    sink = KafkaProducer(topic=\"processed-data\")\n\n    async def process(self, message):\n        processed = await self.process_data(message.body)\n        await self.sink.send(processed)\n\n# Stage 3: Data storage\nclass DataStorageWorker(Worker):\n    source = KafkaConsumer(topic=\"processed-data\")\n\n    async def process(self, message):\n        await self.store_data(message.body)\n</code></pre>"},{"location":"performance/scaling/#auto-scaling-configuration","title":"\ud83d\udd27 Auto-Scaling Configuration","text":""},{"location":"performance/scaling/#cpu-based-scaling","title":"CPU-Based Scaling","text":"<pre><code>import psutil\nfrom pythia import Worker\n\nclass AutoScalingWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.cpu_threshold = 80.0\n        self.scale_check_interval = 60  # seconds\n\n    async def should_scale_up(self):\n        cpu_percent = psutil.cpu_percent(interval=1)\n        return cpu_percent &gt; self.cpu_threshold\n\n    async def should_scale_down(self):\n        cpu_percent = psutil.cpu_percent(interval=1)\n        return cpu_percent &lt; 30.0\n</code></pre>"},{"location":"performance/scaling/#queue-based-scaling","title":"Queue-Based Scaling","text":"<pre><code>from pythia.brokers.kafka import KafkaAdmin\n\nclass QueueBasedScaling(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.admin = KafkaAdmin()\n        self.lag_threshold = 1000\n\n    async def check_consumer_lag(self):\n        lag = await self.admin.get_consumer_lag(\"order-processors\", \"orders\")\n        return lag &gt; self.lag_threshold\n</code></pre>"},{"location":"performance/scaling/#infrastructure-scaling","title":"\ud83c\udfd7\ufe0f Infrastructure Scaling","text":""},{"location":"performance/scaling/#container-orchestration","title":"Container Orchestration","text":"<pre><code># Docker Swarm scaling\nversion: '3.8'\nservices:\n  worker:\n    image: my-app/worker:latest\n    deploy:\n      replicas: 5\n      update_config:\n        parallelism: 2\n        delay: 10s\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n      resources:\n        limits:\n          cpus: '0.5'\n          memory: 512M\n        reservations:\n          cpus: '0.25'\n          memory: 256M\n</code></pre>"},{"location":"performance/scaling/#service-mesh-integration","title":"Service Mesh Integration","text":"<pre><code># Istio service mesh\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: worker-destination\nspec:\n  host: worker-service\n  trafficPolicy:\n    loadBalancer:\n      simple: LEAST_CONN\n  subsets:\n  - name: v1\n    labels:\n      version: v1\n    trafficPolicy:\n      connectionPool:\n        tcp:\n          maxConnections: 10\n        http:\n          http1MaxPendingRequests: 10\n          maxRequestsPerConnection: 2\n</code></pre>"},{"location":"performance/scaling/#monitoring-scaling","title":"\ud83d\udcca Monitoring Scaling","text":""},{"location":"performance/scaling/#metrics-collection","title":"Metrics Collection","text":"<pre><code>from pythia.monitoring import MetricsCollector\nimport prometheus_client\n\nclass ScalingMetricsWorker(Worker):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.metrics = MetricsCollector()\n\n        # Prometheus metrics\n        self.messages_processed = prometheus_client.Counter(\n            'messages_processed_total',\n            'Total messages processed'\n        )\n        self.processing_time = prometheus_client.Histogram(\n            'message_processing_seconds',\n            'Time spent processing messages'\n        )\n        self.queue_size = prometheus_client.Gauge(\n            'queue_size',\n            'Current queue size'\n        )\n\n    async def process(self, message):\n        with self.processing_time.time():\n            result = await self.handle_message(message)\n            self.messages_processed.inc()\n            return result\n</code></pre>"},{"location":"performance/scaling/#alerting-rules","title":"Alerting Rules","text":"<pre><code># Prometheus alerting rules\ngroups:\n- name: pythia-scaling\n  rules:\n  - alert: HighMessageLag\n    expr: kafka_consumer_lag_sum &gt; 10000\n    for: 2m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"High message lag detected\"\n\n  - alert: HighCPUUsage\n    expr: rate(container_cpu_usage_seconds_total[5m]) * 100 &gt; 80\n    for: 5m\n    labels:\n      severity: critical\n    annotations:\n      summary: \"Worker CPU usage is high\"\n\n  - alert: LowThroughput\n    expr: rate(messages_processed_total[5m]) &lt; 10\n    for: 3m\n    labels:\n      severity: warning\n    annotations:\n      summary: \"Message processing throughput is low\"\n</code></pre>"},{"location":"performance/scaling/#scaling-best-practices","title":"\ud83c\udfaf Scaling Best Practices","text":""},{"location":"performance/scaling/#design-principles","title":"Design Principles","text":"<ol> <li>Stateless Workers: Design workers without local state</li> <li>Idempotent Processing: Handle duplicate messages gracefully</li> <li>Circuit Breakers: Implement failure isolation</li> <li>Graceful Shutdown: Handle termination signals properly</li> <li>Health Checks: Implement readiness and liveness probes</li> </ol>"},{"location":"performance/scaling/#configuration-management","title":"Configuration Management","text":"<pre><code>import os\nfrom pythia.config import WorkerConfig\n\ndef get_scaling_config():\n    \"\"\"Get configuration optimized for current environment\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        return WorkerConfig(\n            batch_size=1000,\n            max_concurrent_tasks=50,\n            max_memory_mb=2048,\n        )\n    elif env == \"staging\":\n        return WorkerConfig(\n            batch_size=500,\n            max_concurrent_tasks=25,\n            max_memory_mb=1024,\n        )\n    else:\n        return WorkerConfig(\n            batch_size=100,\n            max_concurrent_tasks=10,\n            max_memory_mb=512,\n        )\n</code></pre>"},{"location":"performance/scaling/#testing-scaling","title":"Testing Scaling","text":"<pre><code>import asyncio\nfrom pythia.testing import LoadTester\n\nasync def test_horizontal_scaling():\n    \"\"\"Test worker performance with multiple instances\"\"\"\n    load_tester = LoadTester(\n        worker_class=OrderProcessor,\n        num_workers=10,\n        messages_per_second=1000,\n        duration_seconds=300,\n    )\n\n    results = await load_tester.run()\n\n    assert results.avg_throughput &gt; 800  # msg/sec\n    assert results.avg_latency &lt; 0.1     # 100ms\n    assert results.error_rate &lt; 0.01     # 1%\n</code></pre>"},{"location":"performance/scaling/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Kafka Scaling Guide</li> <li>Kubernetes HPA Documentation</li> <li>Docker Swarm Scaling</li> <li>Redis Cluster Setup</li> </ul>"},{"location":"user-guide/configuration/","title":"Configuration","text":"<p>Complete guide to configuring Pythia workers for different environments and use cases.</p>"},{"location":"user-guide/configuration/#overview","title":"Overview","text":"<p>Pythia uses Pydantic for configuration management, providing type validation, environment variable support, and clear error messages. Configuration can be set via code, environment variables, or configuration files.</p>"},{"location":"user-guide/configuration/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>from pythia.config import WorkerConfig\n\n# 1. Code-based configuration (highest priority)\nconfig = WorkerConfig(\n    worker_name=\"my-worker\",\n    max_concurrent=10\n)\n\n# 2. Environment variables (medium priority)\n# PYTHIA_WORKER_NAME=my-worker\n# PYTHIA_MAX_CONCURRENT=10\n\n# 3. Configuration files (lowest priority)\n# config.yaml, config.json, or .env files\n</code></pre>"},{"location":"user-guide/configuration/#core-worker-configuration","title":"Core Worker Configuration","text":""},{"location":"user-guide/configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from pythia.config import WorkerConfig\n\nconfig = WorkerConfig(\n    # Worker identification\n    worker_name=\"email-processor\",          # Worker name for logs/metrics\n    worker_id=\"email-processor-001\",        # Unique worker ID (auto-generated if not provided)\n\n    # Processing settings\n    max_retries=3,                          # Maximum retry attempts for failed messages\n    retry_delay=1.0,                        # Delay between retries in seconds\n    batch_size=10,                          # Batch size for processing\n    max_concurrent=20,                      # Maximum concurrent workers\n\n    # Broker configuration\n    broker_type=\"kafka\",                    # Message broker type (kafka/redis/rabbitmq)\n    multi_broker=False,                     # Enable multi-broker support\n\n    # Logging configuration\n    log_level=\"INFO\",                       # Log level (DEBUG, INFO, WARNING, ERROR)\n    log_format=\"json\",                      # Log format (json/text)\n    log_file=\"/var/log/pythia/worker.log\", # Log file path (optional)\n\n    # Health check configuration\n    health_check_interval=30,               # Health check interval in seconds\n    health_check_timeout=10                 # Health check timeout in seconds\n)\n</code></pre>"},{"location":"user-guide/configuration/#environment-variable-configuration","title":"Environment Variable Configuration","text":"<pre><code># Core settings\nexport PYTHIA_WORKER_NAME=\"production-worker\"\nexport PYTHIA_WORKER_ID=\"prod-worker-001\"\nexport PYTHIA_BROKER_TYPE=\"kafka\"\nexport PYTHIA_MAX_CONCURRENT=50\n\n# Retry configuration\nexport PYTHIA_MAX_RETRIES=5\nexport PYTHIA_RETRY_DELAY=2.0\n\n# Logging\nexport PYTHIA_LOG_LEVEL=\"WARNING\"\nexport PYTHIA_LOG_FORMAT=\"json\"\nexport PYTHIA_LOG_FILE=\"/var/log/pythia/production.log\"\n\n# Health checks\nexport PYTHIA_HEALTH_CHECK_INTERVAL=60\nexport PYTHIA_HEALTH_CHECK_TIMEOUT=15\n</code></pre> <pre><code>from pythia.config import WorkerConfig\n\n# Automatically loads from environment variables\nconfig = WorkerConfig()\nprint(f\"Worker: {config.worker_name}\")  # Output: production-worker\n</code></pre>"},{"location":"user-guide/configuration/#broker-specific-configuration","title":"Broker-Specific Configuration","text":""},{"location":"user-guide/configuration/#redis-configuration","title":"Redis Configuration","text":"<pre><code>from pythia.config.redis import RedisConfig\n\n# Basic Redis setup\nredis_config = RedisConfig(\n    host=\"localhost\",\n    port=6379,\n    db=0,\n    password=None,\n\n    # Queue configuration\n    queue=\"task-queue\",                     # List-based queue name\n    batch_size=50,                         # Messages per batch\n    block_timeout_ms=1000,                 # Polling timeout\n\n    # Connection pooling\n    connection_pool_size=20,               # Pool size\n    socket_keepalive=True,                 # Keep connections alive\n    socket_timeout=30,                     # Socket timeout\n    retry_on_timeout=True,                 # Retry on timeout\n\n    # Health monitoring\n    health_check_interval=30               # Health check frequency\n)\n\n# Stream configuration\nredis_stream_config = RedisConfig(\n    host=\"redis-cluster.internal\",\n    port=6379,\n\n    # Stream settings\n    stream=\"events-stream\",                # Stream name\n    consumer_group=\"workers\",              # Consumer group\n    batch_size=100,                       # Larger batches for streams\n    max_stream_length=50000,              # Limit stream size\n\n    # Consumer settings\n    block_timeout_ms=5000,                # 5 second block timeout\n    consumer_name=\"worker-001\"            # Consumer identifier\n)\n\n# Environment variables for Redis\n# REDIS_HOST=redis.example.com\n# REDIS_PORT=6380\n# REDIS_PASSWORD=secure_password\n# REDIS_DB=1\n# REDIS_QUEUE=production-queue\n</code></pre>"},{"location":"user-guide/configuration/#kafka-configuration","title":"Kafka Configuration","text":"<pre><code>from pythia.config.kafka import KafkaConfig\n\n# Basic Kafka setup\nkafka_config = KafkaConfig(\n    bootstrap_servers=\"kafka1:9092,kafka2:9092,kafka3:9092\",\n\n    # Consumer settings\n    group_id=\"email-processors\",\n    topics=[\"emails\", \"notifications\"],\n    auto_offset_reset=\"earliest\",          # Start from beginning\n    enable_auto_commit=False,              # Manual commits for reliability\n\n    # Performance tuning\n    max_poll_records=1000,                 # Messages per poll\n    fetch_min_bytes=50000,                 # Wait for more data\n    fetch_max_wait_ms=500,                 # But don't wait too long\n\n    # Session management\n    session_timeout_ms=30000,              # 30 second session timeout\n    heartbeat_interval_ms=3000,            # Heartbeat every 3 seconds\n    max_poll_interval_ms=600000,           # 10 minutes max processing time\n\n    # Producer settings (for output)\n    acks=\"all\",                           # Wait for all replicas\n    retries=5,                            # Retry failed sends\n    batch_size=32768,                     # 32KB batches\n    linger_ms=10                          # Wait 10ms for batching\n)\n\n# Security configuration\nkafka_secure_config = KafkaConfig(\n    bootstrap_servers=\"secure-kafka:9093\",\n\n    # SASL authentication\n    security_protocol=\"SASL_SSL\",\n    sasl_mechanism=\"PLAIN\",\n    sasl_username=\"worker-user\",\n    sasl_password=\"secure-password\",\n\n    # SSL settings\n    ssl_ca_location=\"/etc/ssl/ca.crt\",\n    ssl_certificate_location=\"/etc/ssl/client.crt\",\n    ssl_key_location=\"/etc/ssl/client.key\",\n    ssl_key_password=\"key-password\"\n)\n\n# Environment variables for Kafka\n# KAFKA_BOOTSTRAP_SERVERS=kafka1:9092,kafka2:9092\n# KAFKA_GROUP_ID=production-group\n# KAFKA_TOPICS=orders,payments,notifications\n# KAFKA_SECURITY_PROTOCOL=SASL_SSL\n# KAFKA_SASL_USERNAME=prod-user\n# KAFKA_SASL_PASSWORD=secure-password\n</code></pre>"},{"location":"user-guide/configuration/#rabbitmq-configuration","title":"RabbitMQ Configuration","text":"<pre><code>from pythia.config.rabbitmq import RabbitMQConfig\n\n# Basic RabbitMQ setup\nrabbitmq_config = RabbitMQConfig(\n    url=\"amqp://user:password@rabbitmq.internal:5672/production\",\n\n    # Queue configuration\n    queue=\"notifications-queue\",           # Queue name\n    exchange=\"notifications-exchange\",     # Exchange name\n    routing_key=\"notification.*\",         # Routing pattern\n\n    # Durability settings\n    durable=True,                         # Survive broker restart\n    auto_ack=False,                       # Manual acknowledgment\n\n    # Performance tuning\n    prefetch_count=100,                   # Messages to prefetch\n\n    # Connection settings\n    heartbeat=600,                        # 10 minute heartbeat\n    connection_attempts=5,                # Retry connection attempts\n    retry_delay=2.0                       # Delay between attempts\n)\n\n# Advanced routing configuration\nrabbitmq_routing_config = RabbitMQConfig(\n    url=\"amqp://rabbitmq-cluster:5672/\",\n\n    # Topic exchange for complex routing\n    queue=\"user-events-queue\",\n    exchange=\"events-topic-exchange\",\n    routing_key=\"user.*.created\",         # Pattern matching\n\n    # Exchange configuration\n    exchange_type=\"topic\",                # topic, direct, fanout, headers\n    exchange_durable=True,\n\n    # Queue arguments for advanced features\n    queue_arguments={\n        \"x-dead-letter-exchange\": \"dlq-exchange\",\n        \"x-dead-letter-routing-key\": \"failed\",\n        \"x-message-ttl\": 300000,          # 5 minute TTL\n        \"x-max-length\": 10000             # Max queue length\n    }\n)\n\n# Environment variables for RabbitMQ\n# RABBITMQ_URL=amqp://user:pass@rabbitmq:5672/vhost\n# RABBITMQ_QUEUE=production-queue\n# RABBITMQ_EXCHANGE=production-exchange\n# RABBITMQ_ROUTING_KEY=task.process\n# RABBITMQ_PREFETCH_COUNT=50\n</code></pre>"},{"location":"user-guide/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"user-guide/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>from pythia.config import LogConfig\n\nlog_config = LogConfig(\n    level=\"INFO\",                         # Log level\n    format=\"json\",                        # Output format (json/text)\n    file=\"/var/log/pythia/worker.log\",    # Log file path\n    rotation=\"100 MB\",                    # Rotate when file reaches size\n    retention=\"30 days\",                  # Keep logs for 30 days\n\n    # Structured logging\n    add_timestamp=True,                   # Include timestamp\n    add_worker_id=True,                   # Include worker ID\n    add_correlation_id=True               # Include correlation ID for tracing\n)\n\n# Advanced logging setup\nclass CustomWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Custom log configuration\n        self.logger.configure(\n            handlers=[\n                {\n                    \"sink\": sys.stdout,\n                    \"format\": \"{time} | {level} | {name} | {message}\",\n                    \"level\": \"INFO\"\n                },\n                {\n                    \"sink\": \"/var/log/pythia/errors.log\",\n                    \"format\": \"{time} | {level} | {name} | {message}\",\n                    \"level\": \"ERROR\",\n                    \"rotation\": \"10 MB\"\n                }\n            ]\n        )\n</code></pre>"},{"location":"user-guide/configuration/#metrics-configuration","title":"Metrics Configuration","text":"<pre><code>from pythia.config import MetricsConfig\n\nmetrics_config = MetricsConfig(\n    enabled=True,                         # Enable metrics collection\n    port=8080,                           # Metrics server port\n    path=\"/metrics\",                     # Metrics endpoint path\n\n    # Prometheus configuration\n    prometheus_enabled=True,\n    prometheus_prefix=\"pythia_worker\",    # Metric name prefix\n\n    # Custom metrics\n    custom_metrics={\n        \"message_processing_duration\": {\n            \"type\": \"histogram\",\n            \"description\": \"Time spent processing messages\",\n            \"buckets\": [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n        },\n        \"queue_depth\": {\n            \"type\": \"gauge\",\n            \"description\": \"Current queue depth\"\n        },\n        \"error_rate\": {\n            \"type\": \"counter\",\n            \"description\": \"Number of processing errors\"\n        }\n    }\n)\n\n# Use in worker\nclass MetricsAwareWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config, metrics_config=metrics_config)\n\n    async def process_message(self, message):\n        # Record processing time\n        with self.metrics.timer(\"message_processing_duration\"):\n            result = await self._process_message(message)\n\n        # Update counters\n        self.metrics.counter(\"messages_processed_total\").inc()\n\n        return result\n</code></pre>"},{"location":"user-guide/configuration/#security-configuration","title":"Security Configuration","text":"<pre><code>from pythia.config import SecurityConfig\n\nsecurity_config = SecurityConfig(\n    # SSL/TLS settings\n    ssl_enabled=True,\n    ssl_cert_file=\"/etc/ssl/certs/worker.crt\",\n    ssl_key_file=\"/etc/ssl/private/worker.key\",\n    ssl_ca_file=\"/etc/ssl/certs/ca.crt\",\n\n    # Authentication\n    auth_enabled=True,\n    auth_method=\"oauth2\",                 # oauth2, basic, api_key\n\n    # Field encryption\n    encryption_enabled=True,\n    encryption_key=\"base64-encoded-key\"   # AES encryption key\n)\n\n# Secure worker example\nclass SecureWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.security_config = security_config\n\n    async def process_message(self, message):\n        # Decrypt sensitive fields if needed\n        if self.security_config.encryption_enabled:\n            message = await self._decrypt_message(message)\n\n        result = await self._process_secure_message(message)\n\n        # Encrypt result if needed\n        if self.security_config.encryption_enabled:\n            result = await self._encrypt_result(result)\n\n        return result\n</code></pre>"},{"location":"user-guide/configuration/#resilience-configuration","title":"Resilience Configuration","text":"<pre><code>from pythia.config import ResilienceConfig\n\nresilience_config = ResilienceConfig(\n    # Retry settings\n    max_retries=5,                        # Maximum retry attempts\n    retry_delay=1.0,                      # Initial delay\n    retry_backoff=2.0,                    # Exponential backoff multiplier\n    retry_max_delay=60.0,                 # Maximum delay between retries\n\n    # Circuit breaker\n    circuit_breaker_enabled=True,\n    circuit_breaker_threshold=10,         # Failures to trigger breaker\n    circuit_breaker_timeout=60,           # Breaker reset timeout\n\n    # Timeouts\n    processing_timeout=300,               # Per-message timeout (5 minutes)\n    connection_timeout=30,                # Connection timeout\n\n    # Rate limiting\n    rate_limit_enabled=True,\n    rate_limit_requests=1000,             # Requests per minute\n    rate_limit_window=60                  # Time window in seconds\n)\n\n# Use resilience config\nconfig = WorkerConfig(\n    worker_name=\"resilient-worker\",\n    resilience=resilience_config\n)\n</code></pre>"},{"location":"user-guide/configuration/#environment-specific-configuration","title":"Environment-Specific Configuration","text":""},{"location":"user-guide/configuration/#development-configuration","title":"Development Configuration","text":"<pre><code># config/development.py\nfrom pythia.config import WorkerConfig\nfrom pythia.config.redis import RedisConfig\n\ndef get_development_config() -&gt; WorkerConfig:\n    return WorkerConfig(\n        worker_name=\"dev-worker\",\n        broker_type=\"redis\",\n        log_level=\"DEBUG\",\n        log_format=\"text\",                # Readable format for dev\n        max_concurrent=5,                 # Lower concurrency\n        health_check_interval=10,         # Frequent health checks\n\n        # Development-friendly settings\n        max_retries=1,                    # Fail fast for debugging\n        retry_delay=0.5                   # Quick retries\n    )\n\ndef get_redis_config() -&gt; RedisConfig:\n    return RedisConfig(\n        host=\"localhost\",\n        port=6379,\n        db=1,                            # Use different DB for dev\n        queue=\"dev-queue\",\n        batch_size=5                     # Small batches for testing\n    )\n</code></pre>"},{"location":"user-guide/configuration/#production-configuration","title":"Production Configuration","text":"<pre><code># config/production.py\nfrom pythia.config import WorkerConfig\nfrom pythia.config.kafka import KafkaConfig\n\ndef get_production_config() -&gt; WorkerConfig:\n    return WorkerConfig(\n        worker_name=\"prod-worker\",\n        broker_type=\"kafka\",\n        log_level=\"INFO\",\n        log_format=\"json\",               # Structured logging\n        log_file=\"/var/log/pythia/production.log\",\n\n        # Production settings\n        max_concurrent=50,               # High concurrency\n        max_retries=5,                   # More retry attempts\n        retry_delay=2.0,\n\n        # Monitoring\n        health_check_interval=60,\n\n        # Security\n        ssl_enabled=True\n    )\n\ndef get_kafka_config() -&gt; KafkaConfig:\n    return KafkaConfig(\n        bootstrap_servers=os.getenv(\"KAFKA_BROKERS\"),\n        group_id=\"production-workers\",\n        topics=[\"orders\", \"payments\", \"notifications\"],\n\n        # Production optimization\n        max_poll_records=2000,\n        fetch_min_bytes=100000,\n        session_timeout_ms=45000,\n\n        # Security\n        security_protocol=\"SASL_SSL\",\n        sasl_mechanism=\"PLAIN\",\n        sasl_username=os.getenv(\"KAFKA_USERNAME\"),\n        sasl_password=os.getenv(\"KAFKA_PASSWORD\")\n    )\n</code></pre>"},{"location":"user-guide/configuration/#testing-configuration","title":"Testing Configuration","text":"<pre><code># config/testing.py\nfrom pythia.config import WorkerConfig\n\ndef get_testing_config() -&gt; WorkerConfig:\n    return WorkerConfig(\n        worker_name=\"test-worker\",\n        broker_type=\"memory\",            # In-memory broker for tests\n        log_level=\"ERROR\",               # Minimal logging\n        max_concurrent=1,                # Single-threaded for tests\n        max_retries=0,                   # No retries in tests\n        health_check_interval=1          # Fast health checks\n    )\n</code></pre>"},{"location":"user-guide/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"user-guide/configuration/#yaml-configuration","title":"YAML Configuration","text":"<pre><code># config.yaml\npythia:\n  worker:\n    name: \"yaml-worker\"\n    max_concurrent: 20\n    log_level: \"INFO\"\n\n  redis:\n    host: \"redis.example.com\"\n    port: 6379\n    queue: \"production-queue\"\n    batch_size: 50\n\n  metrics:\n    enabled: true\n    port: 8080\n    prometheus_enabled: true\n</code></pre> <pre><code>import yaml\nfrom pythia.config import WorkerConfig\n\ndef load_config_from_yaml(file_path: str) -&gt; WorkerConfig:\n    with open(file_path, 'r') as file:\n        config_data = yaml.safe_load(file)\n\n    return WorkerConfig(**config_data['pythia']['worker'])\n</code></pre>"},{"location":"user-guide/configuration/#json-configuration","title":"JSON Configuration","text":"<pre><code>{\n  \"pythia\": {\n    \"worker\": {\n      \"name\": \"json-worker\",\n      \"max_concurrent\": 30,\n      \"log_level\": \"INFO\",\n      \"broker_type\": \"kafka\"\n    },\n    \"kafka\": {\n      \"bootstrap_servers\": \"kafka1:9092,kafka2:9092\",\n      \"group_id\": \"json-workers\",\n      \"topics\": [\"events\"]\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/configuration/#environment-file-env","title":"Environment File (.env)","text":"<pre><code># .env\nPYTHIA_WORKER_NAME=env-worker\nPYTHIA_BROKER_TYPE=rabbitmq\nPYTHIA_MAX_CONCURRENT=25\nPYTHIA_LOG_LEVEL=WARNING\n\nRABBITMQ_URL=amqp://user:pass@rabbitmq:5672/prod\nRABBITMQ_QUEUE=production-tasks\nRABBITMQ_PREFETCH_COUNT=100\n</code></pre>"},{"location":"user-guide/configuration/#configuration-validation","title":"Configuration Validation","text":"<pre><code>from pydantic import ValidationError\nfrom pythia.config import WorkerConfig\n\ndef validate_config():\n    \"\"\"Example of configuration validation\"\"\"\n    try:\n        config = WorkerConfig(\n            worker_name=\"test\",\n            max_concurrent=-1,           # Invalid value\n            log_level=\"INVALID\"          # Invalid level\n        )\n    except ValidationError as e:\n        print(\"Configuration errors:\")\n        for error in e.errors():\n            print(f\"  {error['loc'][0]}: {error['msg']}\")\n\n# Output:\n# Configuration errors:\n#   max_concurrent: ensure this value is greater than 0\n#   log_level: value is not a valid enumeration member\n</code></pre>"},{"location":"user-guide/configuration/#dynamic-configuration-updates","title":"Dynamic Configuration Updates","text":"<pre><code>class ConfigurableWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.config_file_path = \"config.yaml\"\n        self.last_config_check = time.time()\n\n    async def _check_config_updates(self):\n        \"\"\"Check for configuration updates\"\"\"\n        if time.time() - self.last_config_check &gt; 60:  # Check every minute\n            try:\n                new_config = self._load_config_from_file()\n                if new_config != self.config:\n                    self.logger.info(\"Configuration updated, applying changes...\")\n                    await self._apply_config_changes(new_config)\n                    self.config = new_config\n\n            except Exception as e:\n                self.logger.error(f\"Failed to reload configuration: {e}\")\n\n            self.last_config_check = time.time()\n\n    async def _apply_config_changes(self, new_config: WorkerConfig):\n        \"\"\"Apply configuration changes without restart\"\"\"\n        # Update logging level\n        if new_config.log_level != self.config.log_level:\n            self.logger.configure(level=new_config.log_level)\n\n        # Update concurrency\n        if new_config.max_concurrent != self.config.max_concurrent:\n            await self._adjust_concurrency(new_config.max_concurrent)\n</code></pre>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/#1-environment-separation","title":"1. Environment Separation","text":"<pre><code>import os\n\ndef get_config() -&gt; WorkerConfig:\n    \"\"\"Get configuration based on environment\"\"\"\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        return get_production_config()\n    elif env == \"staging\":\n        return get_staging_config()\n    else:\n        return get_development_config()\n</code></pre>"},{"location":"user-guide/configuration/#2-secret-management","title":"2. Secret Management","text":"<pre><code>import os\nfrom typing import Optional\n\nclass SecretManager:\n    \"\"\"Manage secrets from various sources\"\"\"\n\n    @staticmethod\n    def get_secret(key: str) -&gt; Optional[str]:\n        \"\"\"Get secret from environment or secret store\"\"\"\n        # Try environment first\n        value = os.getenv(key)\n        if value:\n            return value\n\n        # Try AWS Secrets Manager, HashiCorp Vault, etc.\n        return SecretManager._get_from_secret_store(key)\n\n    @staticmethod\n    def _get_from_secret_store(key: str) -&gt; Optional[str]:\n        # Implementation depends on your secret store\n        pass\n\n# Usage\nkafka_config = KafkaConfig(\n    bootstrap_servers=os.getenv(\"KAFKA_BROKERS\"),\n    sasl_username=SecretManager.get_secret(\"KAFKA_USERNAME\"),\n    sasl_password=SecretManager.get_secret(\"KAFKA_PASSWORD\")\n)\n</code></pre>"},{"location":"user-guide/configuration/#3-configuration-testing","title":"3. Configuration Testing","text":"<pre><code>import pytest\nfrom pythia.config import WorkerConfig\n\nclass TestConfiguration:\n    def test_default_config(self):\n        \"\"\"Test default configuration values\"\"\"\n        config = WorkerConfig()\n        assert config.worker_name == \"pythia-worker\"\n        assert config.max_retries == 3\n        assert config.log_level == \"INFO\"\n\n    def test_environment_override(self, monkeypatch):\n        \"\"\"Test environment variable overrides\"\"\"\n        monkeypatch.setenv(\"PYTHIA_WORKER_NAME\", \"test-worker\")\n        monkeypatch.setenv(\"PYTHIA_MAX_RETRIES\", \"5\")\n\n        config = WorkerConfig()\n        assert config.worker_name == \"test-worker\"\n        assert config.max_retries == 5\n\n    def test_invalid_config(self):\n        \"\"\"Test configuration validation\"\"\"\n        with pytest.raises(ValidationError):\n            WorkerConfig(max_concurrent=-1)\n</code></pre>"},{"location":"user-guide/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Error Handling - Advanced error handling patterns</li> <li>Performance Optimization - Performance tuning</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/error-handling/","title":"Error Handling","text":"<p>Comprehensive guide to robust error handling patterns in Pythia workers.</p>"},{"location":"user-guide/error-handling/#overview","title":"Overview","text":"<p>Effective error handling is crucial for building reliable workers. Pythia provides multiple strategies for handling different types of errors, from transient network issues to permanent data corruption.</p>"},{"location":"user-guide/error-handling/#error-classification","title":"Error Classification","text":""},{"location":"user-guide/error-handling/#error-types","title":"Error Types","text":"<pre><code>from pythia.exceptions import (\n    RecoverableError,      # Temporary errors that should be retried\n    PermanentError,        # Errors that shouldn't be retried\n    ValidationError,       # Data validation failures\n    ConnectionError,       # Broker connection issues\n    TimeoutError,         # Processing timeout errors\n    CircuitBreakerError   # Circuit breaker triggered\n)\n\nclass EmailProcessor(Worker):\n    async def process_message(self, message: Message) -&gt; Any:\n        try:\n            return await self._send_email(message)\n\n        except aiohttp.ClientConnectionError as e:\n            # Network issue - should retry\n            raise RecoverableError(f\"Network error: {e}\")\n\n        except InvalidEmailAddressError as e:\n            # Bad data - don't retry\n            raise PermanentError(f\"Invalid email: {e}\")\n\n        except EmailServiceQuotaExceeded as e:\n            # Rate limit - retry later\n            raise RecoverableError(f\"Rate limited: {e}\")\n\n        except Exception as e:\n            # Unknown error - be conservative and retry\n            self.logger.error(f\"Unknown error: {e}\")\n            raise RecoverableError(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"user-guide/error-handling/#retry-mechanisms","title":"Retry Mechanisms","text":""},{"location":"user-guide/error-handling/#basic-retry-logic","title":"Basic Retry Logic","text":"<pre><code>from pythia.config import ResilienceConfig\n\nclass RetryableWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Configure retry behavior\n        self.resilience_config = ResilienceConfig(\n            max_retries=5,                    # Maximum retry attempts\n            retry_delay=1.0,                  # Initial delay\n            retry_backoff=2.0,                # Exponential backoff\n            retry_max_delay=60.0,             # Maximum delay cap\n\n            # Jitter to prevent thundering herd\n            retry_jitter=True,\n            retry_jitter_max=0.1              # \u00b110% jitter\n        )\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with automatic retry handling\"\"\"\n        for attempt in range(self.resilience_config.max_retries + 1):\n            try:\n                return await self._attempt_processing(message, attempt)\n\n            except RecoverableError as e:\n                if attempt &gt;= self.resilience_config.max_retries:\n                    # Final attempt failed\n                    self.logger.error(f\"All retry attempts failed: {e}\")\n                    await self._handle_final_failure(message, e)\n                    raise PermanentError(f\"Max retries exceeded: {e}\")\n\n                # Calculate delay for next attempt\n                delay = self._calculate_retry_delay(attempt)\n                self.logger.warning(f\"Attempt {attempt + 1} failed, retrying in {delay}s: {e}\")\n\n                await asyncio.sleep(delay)\n\n            except PermanentError:\n                # Don't retry permanent errors\n                raise\n\n    def _calculate_retry_delay(self, attempt: int) -&gt; float:\n        \"\"\"Calculate delay with exponential backoff and jitter\"\"\"\n        base_delay = self.resilience_config.retry_delay\n        backoff = self.resilience_config.retry_backoff ** attempt\n        delay = min(base_delay * backoff, self.resilience_config.retry_max_delay)\n\n        # Add jitter to prevent thundering herd\n        if self.resilience_config.retry_jitter:\n            jitter_range = delay * self.resilience_config.retry_jitter_max\n            jitter = random.uniform(-jitter_range, jitter_range)\n            delay += jitter\n\n        return max(0.1, delay)  # Minimum delay\n</code></pre>"},{"location":"user-guide/error-handling/#advanced-retry-patterns","title":"Advanced Retry Patterns","text":"<pre><code>from datetime import datetime, timedelta\nfrom typing import Dict, Type, Callable\n\nclass AdvancedRetryWorker(Worker):\n    \"\"\"Worker with sophisticated retry patterns\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Define retry strategies per error type\n        self.retry_strategies = {\n            ConnectionError: self._connection_retry_strategy,\n            TimeoutError: self._timeout_retry_strategy,\n            RateLimitError: self._rate_limit_retry_strategy,\n            ValidationError: self._validation_retry_strategy\n        }\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with error-specific retry strategies\"\"\"\n        last_error = None\n\n        for attempt in range(self.resilience_config.max_retries + 1):\n            try:\n                return await self._attempt_processing(message, attempt)\n\n            except Exception as e:\n                last_error = e\n                error_type = type(e)\n\n                # Get retry strategy for this error type\n                strategy = self.retry_strategies.get(error_type, self._default_retry_strategy)\n\n                # Check if we should retry\n                should_retry, delay = await strategy(e, attempt, message)\n\n                if not should_retry or attempt &gt;= self.resilience_config.max_retries:\n                    break\n\n                self.logger.warning(\n                    f\"Attempt {attempt + 1} failed with {error_type.__name__}, \"\n                    f\"retrying in {delay}s: {e}\"\n                )\n\n                await asyncio.sleep(delay)\n\n        # All retries exhausted\n        await self._handle_final_failure(message, last_error)\n        raise PermanentError(f\"All retry attempts failed: {last_error}\")\n\n    async def _connection_retry_strategy(\n        self, error: Exception, attempt: int, message: Message\n    ) -&gt; tuple[bool, float]:\n        \"\"\"Strategy for connection errors\"\"\"\n        # Retry connection errors with exponential backoff\n        delay = min(2.0 ** attempt, 30.0)  # Cap at 30 seconds\n        return True, delay\n\n    async def _timeout_retry_strategy(\n        self, error: Exception, attempt: int, message: Message\n    ) -&gt; tuple[bool, float]:\n        \"\"\"Strategy for timeout errors\"\"\"\n        # Increase timeout on subsequent attempts\n        if attempt &lt; 3:\n            # Quick retries for first few attempts\n            delay = 1.0 * (attempt + 1)\n            return True, delay\n        else:\n            # Longer delays for later attempts\n            delay = 10.0 * (attempt - 2)\n            return True, min(delay, 60.0)\n\n    async def _rate_limit_retry_strategy(\n        self, error: Exception, attempt: int, message: Message\n    ) -&gt; tuple[bool, float]:\n        \"\"\"Strategy for rate limit errors\"\"\"\n        # Use longer delays for rate limits\n        base_delay = 30.0  # 30 seconds\n        delay = base_delay * (attempt + 1)\n\n        # Check rate limit headers if available\n        if hasattr(error, 'retry_after'):\n            delay = max(delay, error.retry_after)\n\n        return True, min(delay, 300.0)  # Cap at 5 minutes\n\n    async def _validation_retry_strategy(\n        self, error: Exception, attempt: int, message: Message\n    ) -&gt; tuple[bool, float]:\n        \"\"\"Strategy for validation errors\"\"\"\n        # Don't retry validation errors\n        return False, 0.0\n\n    async def _default_retry_strategy(\n        self, error: Exception, attempt: int, message: Message\n    ) -&gt; tuple[bool, float]:\n        \"\"\"Default retry strategy for unknown errors\"\"\"\n        # Conservative exponential backoff\n        delay = min(1.0 * (2 ** attempt), 60.0)\n        return True, delay\n</code></pre>"},{"location":"user-guide/error-handling/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>import asyncio\nfrom enum import Enum\nfrom datetime import datetime, timedelta\n\nclass CircuitBreakerState(Enum):\n    CLOSED = \"closed\"        # Normal operation\n    OPEN = \"open\"           # Failing, reject requests\n    HALF_OPEN = \"half_open\" # Testing if service recovered\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker implementation\"\"\"\n\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        recovery_timeout: int = 60,\n        expected_exception: Type[Exception] = Exception\n    ):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exception = expected_exception\n\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitBreakerState.CLOSED\n\n    async def call(self, func: Callable, *args, **kwargs):\n        \"\"\"Execute function with circuit breaker protection\"\"\"\n        if self.state == CircuitBreakerState.OPEN:\n            if self._should_attempt_reset():\n                self.state = CircuitBreakerState.HALF_OPEN\n            else:\n                raise CircuitBreakerError(\"Circuit breaker is OPEN\")\n\n        try:\n            result = await func(*args, **kwargs)\n            self._on_success()\n            return result\n\n        except self.expected_exception as e:\n            self._on_failure()\n            raise\n\n    def _should_attempt_reset(self) -&gt; bool:\n        \"\"\"Check if enough time has passed to attempt reset\"\"\"\n        return (\n            self.last_failure_time and\n            datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.recovery_timeout)\n        )\n\n    def _on_success(self):\n        \"\"\"Handle successful execution\"\"\"\n        self.failure_count = 0\n        self.state = CircuitBreakerState.CLOSED\n\n    def _on_failure(self):\n        \"\"\"Handle failed execution\"\"\"\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n\n        if self.failure_count &gt;= self.failure_threshold:\n            self.state = CircuitBreakerState.OPEN\n\nclass CircuitBreakerWorker(Worker):\n    \"\"\"Worker with circuit breaker protection\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n\n        # Create circuit breakers for different services\n        self.database_breaker = CircuitBreaker(\n            failure_threshold=5,\n            recovery_timeout=60,\n            expected_exception=DatabaseError\n        )\n\n        self.api_breaker = CircuitBreaker(\n            failure_threshold=3,\n            recovery_timeout=30,\n            expected_exception=APIError\n        )\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with circuit breaker protection\"\"\"\n        try:\n            # Database operation with circuit breaker\n            data = await self.database_breaker.call(\n                self._fetch_from_database, message.body\n            )\n\n            # API call with circuit breaker\n            result = await self.api_breaker.call(\n                self._call_external_api, data\n            )\n\n            return result\n\n        except CircuitBreakerError as e:\n            self.logger.warning(f\"Circuit breaker triggered: {e}\")\n            # Fallback to cached data or default response\n            return await self._fallback_processing(message)\n</code></pre>"},{"location":"user-guide/error-handling/#dead-letter-queue-dlq-handling","title":"Dead Letter Queue (DLQ) Handling","text":"<pre><code>class DLQHandler:\n    \"\"\"Dead Letter Queue handler\"\"\"\n\n    def __init__(self, worker_config: WorkerConfig):\n        self.config = worker_config\n        self.dlq_producer = None\n\n    async def send_to_dlq(\n        self,\n        original_message: Message,\n        final_error: Exception,\n        retry_history: List[Dict[str, Any]] = None\n    ):\n        \"\"\"Send message to dead letter queue\"\"\"\n        dlq_message = {\n            \"original_message\": {\n                \"id\": original_message.message_id,\n                \"body\": original_message.body,\n                \"headers\": original_message.headers,\n                \"timestamp\": original_message.timestamp.isoformat()\n            },\n            \"error_details\": {\n                \"error_type\": type(final_error).__name__,\n                \"error_message\": str(final_error),\n                \"error_traceback\": traceback.format_exc()\n            },\n            \"processing_details\": {\n                \"worker_id\": self.config.worker_id,\n                \"worker_name\": self.config.worker_name,\n                \"failed_at\": datetime.utcnow().isoformat(),\n                \"retry_count\": original_message.retry_count,\n                \"retry_history\": retry_history or []\n            },\n            \"broker_metadata\": {\n                \"topic\": original_message.topic,\n                \"queue\": original_message.queue,\n                \"partition\": original_message.partition,\n                \"offset\": original_message.offset\n            }\n        }\n\n        # Send to appropriate DLQ based on broker type\n        if self.config.broker_type == \"kafka\":\n            await self._send_to_kafka_dlq(dlq_message)\n        elif self.config.broker_type == \"rabbitmq\":\n            await self._send_to_rabbitmq_dlq(dlq_message)\n        else:\n            await self._send_to_generic_dlq(dlq_message)\n\n    async def _send_to_kafka_dlq(self, dlq_message: Dict[str, Any]):\n        \"\"\"Send to Kafka DLQ topic\"\"\"\n        dlq_topic = f\"{dlq_message['broker_metadata']['topic']}-dlq\"\n\n        # Use Kafka producer to send to DLQ topic\n        await self.dlq_producer.send(\n            topic=dlq_topic,\n            value=json.dumps(dlq_message),\n            key=dlq_message[\"original_message\"][\"id\"]\n        )\n\n    async def _send_to_rabbitmq_dlq(self, dlq_message: Dict[str, Any]):\n        \"\"\"Send to RabbitMQ DLQ exchange\"\"\"\n        dlq_exchange = \"dlq-exchange\"\n        dlq_routing_key = f\"dlq.{dlq_message['broker_metadata']['queue']}\"\n\n        # Use RabbitMQ producer to send to DLQ\n        await self.dlq_producer.publish(\n            exchange=dlq_exchange,\n            routing_key=dlq_routing_key,\n            message=json.dumps(dlq_message)\n        )\n\nclass DLQAwareWorker(Worker):\n    \"\"\"Worker with DLQ support\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.dlq_handler = DLQHandler(config)\n        self.retry_history = []\n\n    async def _handle_final_failure(self, message: Message, error: Exception):\n        \"\"\"Handle message that failed all retry attempts\"\"\"\n        # Add final error to retry history\n        self.retry_history.append({\n            \"attempt\": message.retry_count + 1,\n            \"error\": str(error),\n            \"timestamp\": datetime.utcnow().isoformat()\n        })\n\n        # Send to DLQ\n        await self.dlq_handler.send_to_dlq(\n            original_message=message,\n            final_error=error,\n            retry_history=self.retry_history\n        )\n\n        # Update metrics\n        self.metrics.counter(\"messages_sent_to_dlq_total\").inc()\n\n        # Log for monitoring\n        self.logger.error(\n            f\"Message sent to DLQ after {message.retry_count} retries\",\n            extra={\n                \"message_id\": message.message_id,\n                \"error\": str(error),\n                \"retry_count\": message.retry_count\n            }\n        )\n</code></pre>"},{"location":"user-guide/error-handling/#error-monitoring-alerting","title":"Error Monitoring &amp; Alerting","text":"<pre><code>class MonitoredErrorHandler:\n    \"\"\"Error handler with monitoring and alerting\"\"\"\n\n    def __init__(self, worker: Worker):\n        self.worker = worker\n        self.error_counts = defaultdict(int)\n        self.error_rate_threshold = 0.1  # 10% error rate threshold\n        self.alert_cooldown = 300  # 5 minutes\n        self.last_alert_time = {}\n\n    async def handle_error(\n        self,\n        error: Exception,\n        message: Message,\n        context: Dict[str, Any] = None\n    ):\n        \"\"\"Handle error with monitoring\"\"\"\n        error_type = type(error).__name__\n\n        # Update error metrics\n        self.worker.metrics.counter(f\"errors_total\").inc()\n        self.worker.metrics.counter(f\"errors_by_type\").labels(error_type=error_type).inc()\n\n        # Track error rate\n        self.error_counts[error_type] += 1\n\n        # Check if error rate is too high\n        await self._check_error_rate_threshold(error_type)\n\n        # Log structured error\n        self.worker.logger.error(\n            f\"Processing error: {error}\",\n            extra={\n                \"error_type\": error_type,\n                \"message_id\": message.message_id,\n                \"worker_id\": self.worker.config.worker_id,\n                \"context\": context or {},\n                \"traceback\": traceback.format_exc()\n            }\n        )\n\n        # Send to error tracking service (Sentry, Rollbar, etc.)\n        await self._send_to_error_tracking(error, message, context)\n\n    async def _check_error_rate_threshold(self, error_type: str):\n        \"\"\"Check if error rate exceeds threshold\"\"\"\n        total_processed = self.worker.metrics.counter(\"messages_processed_total\")._value\n        error_count = self.error_counts[error_type]\n\n        if total_processed &gt; 100:  # Only check after processing some messages\n            error_rate = error_count / total_processed\n\n            if error_rate &gt; self.error_rate_threshold:\n                await self._send_high_error_rate_alert(error_type, error_rate)\n\n    async def _send_high_error_rate_alert(self, error_type: str, error_rate: float):\n        \"\"\"Send alert for high error rate\"\"\"\n        now = datetime.utcnow()\n        last_alert = self.last_alert_time.get(error_type)\n\n        # Check cooldown\n        if last_alert and (now - last_alert).seconds &lt; self.alert_cooldown:\n            return\n\n        alert_message = (\n            f\"High error rate detected for {error_type}: {error_rate:.2%}\\n\"\n            f\"Worker: {self.worker.config.worker_name}\\n\"\n            f\"Worker ID: {self.worker.config.worker_id}\"\n        )\n\n        # Send alert (implementation depends on your alerting system)\n        await self._send_alert(alert_message, severity=\"high\")\n\n        self.last_alert_time[error_type] = now\n\n    async def _send_to_error_tracking(\n        self,\n        error: Exception,\n        message: Message,\n        context: Dict[str, Any] = None\n    ):\n        \"\"\"Send error to tracking service\"\"\"\n        # Example with Sentry\n        try:\n            import sentry_sdk\n\n            with sentry_sdk.configure_scope() as scope:\n                scope.set_tag(\"worker_name\", self.worker.config.worker_name)\n                scope.set_tag(\"worker_id\", self.worker.config.worker_id)\n                scope.set_tag(\"message_id\", message.message_id)\n\n                if context:\n                    for key, value in context.items():\n                        scope.set_extra(key, value)\n\n                sentry_sdk.capture_exception(error)\n\n        except ImportError:\n            # Sentry not available, skip\n            pass\n</code></pre>"},{"location":"user-guide/error-handling/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>class GracefulWorker(Worker):\n    \"\"\"Worker that degrades gracefully when services fail\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.degraded_mode = False\n        self.service_health = {\n            \"database\": True,\n            \"cache\": True,\n            \"external_api\": True\n        }\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with graceful degradation\"\"\"\n        if self.degraded_mode:\n            return await self._process_degraded_mode(message)\n\n        try:\n            return await self._process_normal_mode(message)\n\n        except ServiceUnavailableError as e:\n            service = e.service_name\n            self.service_health[service] = False\n\n            # Check if we should enter degraded mode\n            if self._should_enter_degraded_mode():\n                self.degraded_mode = True\n                self.logger.warning(f\"Entering degraded mode due to {service} failure\")\n\n            return await self._process_degraded_mode(message)\n\n    def _should_enter_degraded_mode(self) -&gt; bool:\n        \"\"\"Determine if we should enter degraded mode\"\"\"\n        # Enter degraded mode if critical services are down\n        critical_services = [\"database\"]\n        return not all(\n            self.service_health.get(service, False)\n            for service in critical_services\n        )\n\n    async def _process_normal_mode(self, message: Message) -&gt; Any:\n        \"\"\"Full-featured processing\"\"\"\n        # Fetch from database\n        data = await self._fetch_from_database(message.body)\n\n        # Get from cache\n        cached_data = await self._get_from_cache(data.id)\n\n        # Call external API\n        api_result = await self._call_external_api(data)\n\n        return {\n            \"status\": \"success\",\n            \"mode\": \"normal\",\n            \"result\": api_result\n        }\n\n    async def _process_degraded_mode(self, message: Message) -&gt; Any:\n        \"\"\"Degraded processing with fallbacks\"\"\"\n        self.logger.info(\"Processing in degraded mode\")\n\n        try:\n            # Try to use cached data\n            if self.service_health.get(\"cache\", False):\n                cached_result = await self._get_cached_fallback(message.body)\n                if cached_result:\n                    return {\n                        \"status\": \"success\",\n                        \"mode\": \"degraded_cache\",\n                        \"result\": cached_result\n                    }\n\n            # Use default/static response\n            return {\n                \"status\": \"success\",\n                \"mode\": \"degraded_default\",\n                \"result\": self._get_default_response(message.body)\n            }\n\n        except Exception as e:\n            # Last resort: queue for later processing\n            await self._queue_for_later(message)\n            return {\n                \"status\": \"queued\",\n                \"mode\": \"degraded_queued\",\n                \"message\": \"Queued for later processing\"\n            }\n\n    async def _periodic_health_check(self):\n        \"\"\"Periodically check service health\"\"\"\n        while True:\n            for service in self.service_health:\n                try:\n                    if service == \"database\":\n                        await self._check_database_health()\n                    elif service == \"cache\":\n                        await self._check_cache_health()\n                    elif service == \"external_api\":\n                        await self._check_api_health()\n\n                    self.service_health[service] = True\n\n                except Exception:\n                    self.service_health[service] = False\n\n            # Exit degraded mode if all services are healthy\n            if self.degraded_mode and all(self.service_health.values()):\n                self.degraded_mode = False\n                self.logger.info(\"Exiting degraded mode - all services healthy\")\n\n            await asyncio.sleep(30)  # Check every 30 seconds\n</code></pre>"},{"location":"user-guide/error-handling/#testing-error-handling","title":"Testing Error Handling","text":"<pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\n\nclass TestErrorHandling:\n    \"\"\"Test suite for error handling\"\"\"\n\n    @pytest.fixture\n    def worker(self):\n        config = WorkerConfig(max_retries=3)\n        return EmailProcessor(config)\n\n    async def test_recoverable_error_retry(self, worker):\n        \"\"\"Test that recoverable errors are retried\"\"\"\n        message = Message(body=\"test@example.com\")\n\n        # Mock the email service to fail twice then succeed\n        with patch.object(worker, '_send_email') as mock_send:\n            mock_send.side_effect = [\n                RecoverableError(\"Network error\"),\n                RecoverableError(\"Network error\"),\n                {\"status\": \"sent\"}\n            ]\n\n            result = await worker.process_message(message)\n\n            assert result[\"status\"] == \"sent\"\n            assert mock_send.call_count == 3\n\n    async def test_permanent_error_no_retry(self, worker):\n        \"\"\"Test that permanent errors are not retried\"\"\"\n        message = Message(body=\"invalid-email\")\n\n        with patch.object(worker, '_send_email') as mock_send:\n            mock_send.side_effect = PermanentError(\"Invalid email\")\n\n            with pytest.raises(PermanentError):\n                await worker.process_message(message)\n\n            assert mock_send.call_count == 1\n\n    async def test_max_retries_exceeded(self, worker):\n        \"\"\"Test behavior when max retries exceeded\"\"\"\n        message = Message(body=\"test@example.com\")\n\n        with patch.object(worker, '_send_email') as mock_send:\n            mock_send.side_effect = RecoverableError(\"Persistent failure\")\n\n            with pytest.raises(PermanentError) as exc_info:\n                await worker.process_message(message)\n\n            assert \"Max retries exceeded\" in str(exc_info.value)\n            assert mock_send.call_count == 4  # Initial + 3 retries\n\n    async def test_circuit_breaker_opens(self):\n        \"\"\"Test circuit breaker opens after threshold failures\"\"\"\n        breaker = CircuitBreaker(failure_threshold=3)\n\n        async def failing_function():\n            raise Exception(\"Service unavailable\")\n\n        # Trigger failures to open circuit breaker\n        for _ in range(3):\n            with pytest.raises(Exception):\n                await breaker.call(failing_function)\n\n        # Circuit breaker should now be open\n        with pytest.raises(CircuitBreakerError):\n            await breaker.call(failing_function)\n</code></pre>"},{"location":"user-guide/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/error-handling/#1-error-classification","title":"1. Error Classification","text":"<ul> <li>Be specific: Use custom exception types for different error categories</li> <li>Document behavior: Clearly document which errors are recoverable</li> <li>Fail fast: Don't retry validation or configuration errors</li> </ul>"},{"location":"user-guide/error-handling/#2-retry-strategy","title":"2. Retry Strategy","text":"<ul> <li>Exponential backoff: Prevent overwhelming failed services</li> <li>Jitter: Add randomness to prevent thundering herd</li> <li>Max delay: Cap retry delays to reasonable values</li> <li>Circuit breakers: Protect downstream services</li> </ul>"},{"location":"user-guide/error-handling/#3-monitoring","title":"3. Monitoring","text":"<ul> <li>Error rates: Track error rates over time</li> <li>Error types: Monitor specific error patterns</li> <li>DLQ metrics: Track messages sent to dead letter queues</li> <li>Recovery time: Measure how quickly services recover</li> </ul>"},{"location":"user-guide/error-handling/#4-alerting","title":"4. Alerting","text":"<ul> <li>Threshold-based: Alert on error rate thresholds</li> <li>Cooldown periods: Prevent alert spam</li> <li>Context: Include relevant context in alerts</li> <li>Escalation: Define escalation paths for critical errors</li> </ul>"},{"location":"user-guide/error-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Optimization - Performance tuning techniques</li> <li>Monitoring Setup - Production monitoring</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"user-guide/logging/","title":"Logging with Loguru","text":"<p>Pythia uses Loguru as its default logging framework, providing structured, flexible, and high-performance logging out of the box.</p>"},{"location":"user-guide/logging/#why-loguru","title":"\ud83c\udf1f Why Loguru?","text":"<ul> <li>\ud83d\ude80 Zero Configuration: Works perfectly with sensible defaults</li> <li>\ud83d\udcca Structured Logging: JSON output with custom fields</li> <li>\ud83c\udfa8 Beautiful Console Output: Colored, readable logs for development</li> <li>\u26a1 High Performance: Async-compatible with minimal overhead</li> <li>\ud83d\udd27 Highly Configurable: Custom formatters, filters, and handlers</li> </ul>"},{"location":"user-guide/logging/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"user-guide/logging/#basic-usage","title":"Basic Usage","text":"<p>Loguru is automatically configured when you create a Pythia worker:</p> <pre><code>from pythia import Worker\nfrom pythia.brokers.redis import RedisConsumer\nfrom loguru import logger\n\nclass EmailWorker(Worker):\n    source = RedisConsumer(queue_name=\"emails\")\n\n    async def process(self, message):\n        logger.info(\"Processing email\", email=message.body.get(\"email\"))\n\n        try:\n            # Process email logic\n            await self.send_email(message.body)\n            logger.success(\"Email sent successfully\",\n                         email=message.body.get(\"email\"))\n        except Exception as e:\n            logger.error(\"Failed to send email\",\n                        email=message.body.get(\"email\"),\n                        error=str(e))\n            raise\n\n        return {\"status\": \"sent\", \"email\": message.body.get(\"email\")}\n</code></pre>"},{"location":"user-guide/logging/#structured-logging","title":"Structured Logging","text":"<p>Use keyword arguments to add structured data to your logs:</p> <pre><code>logger.info(\"User action\",\n           user_id=123,\n           action=\"login\",\n           ip=\"192.168.1.1\",\n           timestamp=datetime.now().isoformat())\n</code></pre>"},{"location":"user-guide/logging/#configuration","title":"\u2699\ufe0f Configuration","text":""},{"location":"user-guide/logging/#environment-variables","title":"Environment Variables","text":"<p>Configure logging behavior using environment variables:</p> <pre><code># Log level (TRACE, DEBUG, INFO, SUCCESS, WARNING, ERROR, CRITICAL)\nPYTHIA_LOG_LEVEL=INFO\n\n# Log format (console, json, custom)\nPYTHIA_LOG_FORMAT=console\n\n# Log file path (optional)\nPYTHIA_LOG_FILE=/app/logs/worker.log\n\n# Disable console logging\nPYTHIA_LOG_CONSOLE=false\n\n# Enable log sampling for high-volume scenarios\nPYTHIA_LOG_SAMPLING_RATE=0.1  # Log 10% of messages\n</code></pre>"},{"location":"user-guide/logging/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from pythia.logging import setup_logging\nfrom loguru import logger\n\n# Custom configuration\nsetup_logging(\n    level=\"DEBUG\",\n    format_type=\"json\",\n    log_file=\"/app/logs/worker.log\",\n    console_enabled=True,\n    sampling_rate=1.0  # Log everything\n)\n\nclass MyWorker(Worker):\n    async def process(self, message):\n        logger.debug(\"Processing message\",\n                    message_id=message.message_id,\n                    queue=message.metadata.get(\"queue\"))\n</code></pre>"},{"location":"user-guide/logging/#log-formats","title":"\ud83d\udcdd Log Formats","text":""},{"location":"user-guide/logging/#console-format-development","title":"Console Format (Development)","text":"<p>Perfect for local development with colored, readable output:</p> <pre><code>from pythia.logging import setup_logging\n\nsetup_logging(format_type=\"console\", level=\"DEBUG\")\n</code></pre> <p>Output: <pre><code>2025-09-02 10:30:15.123 | INFO     | my_worker:process:15 - Processing email email=user@example.com\n2025-09-02 10:30:15.456 | SUCCESS  | my_worker:process:22 - Email sent successfully email=user@example.com\n</code></pre></p>"},{"location":"user-guide/logging/#json-format-production","title":"JSON Format (Production)","text":"<p>Structured JSON logs for production systems and log aggregation:</p> <pre><code>setup_logging(format_type=\"json\", level=\"INFO\")\n</code></pre> <p>Output: <pre><code>{\n  \"timestamp\": \"2025-09-02T10:30:15.123456Z\",\n  \"level\": \"INFO\",\n  \"module\": \"my_worker\",\n  \"function\": \"process\",\n  \"line\": 15,\n  \"message\": \"Processing email\",\n  \"email\": \"user@example.com\",\n  \"worker_id\": \"worker-123\",\n  \"trace_id\": \"abc123def456\"\n}\n</code></pre></p>"},{"location":"user-guide/logging/#custom-format","title":"Custom Format","text":"<p>Create your own log format:</p> <pre><code>from pythia.logging import setup_logging\n\ncustom_format = (\n    \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | \"\n    \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n    \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n    \"&lt;level&gt;{message}&lt;/level&gt; | \"\n    \"{extra}\"\n)\n\nsetup_logging(\n    format_type=\"custom\",\n    custom_format=custom_format,\n    level=\"INFO\"\n)\n</code></pre>"},{"location":"user-guide/logging/#log-levels-and-filtering","title":"\ud83c\udfaf Log Levels and Filtering","text":""},{"location":"user-guide/logging/#available-log-levels","title":"Available Log Levels","text":"<pre><code>from loguru import logger\n\n# TRACE - Very detailed debugging information\nlogger.trace(\"Entering function\", function=\"process_message\")\n\n# DEBUG - Detailed debugging information\nlogger.debug(\"Processing step\", step=1, data={\"key\": \"value\"})\n\n# INFO - General information about program execution\nlogger.info(\"Worker started\", worker_type=\"EmailWorker\")\n\n# SUCCESS - Successful operation (Loguru-specific)\nlogger.success(\"Message processed successfully\", message_id=\"123\")\n\n# WARNING - Something unexpected happened\nlogger.warning(\"Retry attempt\", attempt=2, max_retries=3)\n\n# ERROR - Serious problem occurred\nlogger.error(\"Processing failed\", error=str(e), message_id=\"123\")\n\n# CRITICAL - Very serious error occurred\nlogger.critical(\"Database connection lost\", database=\"primary\")\n</code></pre>"},{"location":"user-guide/logging/#filtering-by-module","title":"Filtering by Module","text":"<p>Filter logs by specific modules or components:</p> <pre><code>from pythia.logging import setup_logging\n\n# Only log from specific modules\nsetup_logging(\n    level=\"DEBUG\",\n    filter_modules=[\"my_worker\", \"pythia.brokers.kafka\"]\n)\n\n# Exclude specific modules\nsetup_logging(\n    level=\"DEBUG\",\n    exclude_modules=[\"urllib3\", \"asyncio\"]\n)\n</code></pre>"},{"location":"user-guide/logging/#performance-considerations","title":"\ud83d\udcca Performance Considerations","text":""},{"location":"user-guide/logging/#log-sampling","title":"Log Sampling","text":"<p>For high-throughput scenarios, use log sampling to reduce I/O:</p> <pre><code>from pythia.logging import setup_logging\n\n# Log only 10% of messages\nsetup_logging(\n    level=\"INFO\",\n    sampling_rate=0.1,\n    sampling_strategy=\"random\"  # or \"systematic\"\n)\n\nclass HighVolumeWorker(Worker):\n    async def process(self, message):\n        # This will only be logged 10% of the time\n        logger.info(\"Processing message\", message_id=message.message_id)\n</code></pre>"},{"location":"user-guide/logging/#async-logging","title":"Async Logging","text":"<p>Enable async logging for better performance:</p> <pre><code>from pythia.logging import setup_async_logging\n\n# Async logging with buffer\nsetup_async_logging(\n    buffer_size=1000,\n    flush_interval=5.0  # seconds\n)\n</code></pre>"},{"location":"user-guide/logging/#custom-formatters","title":"\ud83d\udd27 Custom Formatters","text":""},{"location":"user-guide/logging/#adding-context-information","title":"Adding Context Information","text":"<p>Automatically add worker context to all logs:</p> <pre><code>from pythia.logging.decorators import with_worker_context\nfrom loguru import logger\n\nclass MyWorker(Worker):\n    def __init__(self):\n        super().__init__()\n        # Add worker context to all logs\n        logger.configure(\n            extra={\n                \"worker_id\": f\"worker-{id(self)}\",\n                \"worker_type\": self.__class__.__name__,\n                \"broker_type\": self.source.__class__.__name__\n            }\n        )\n\n    @with_worker_context\n    async def process(self, message):\n        # All logs will automatically include worker context\n        logger.info(\"Processing message\", message_id=message.message_id)\n</code></pre>"},{"location":"user-guide/logging/#message-correlation","title":"Message Correlation","text":"<p>Track messages across your entire pipeline:</p> <pre><code>import uuid\nfrom loguru import logger\n\nclass TrackableWorker(Worker):\n    async def process(self, message):\n        # Generate or extract correlation ID\n        correlation_id = message.headers.get(\"correlation_id\") or str(uuid.uuid4())\n\n        with logger.contextualize(correlation_id=correlation_id):\n            logger.info(\"Starting message processing\")\n\n            # All nested logs will include correlation_id\n            await self.process_step_1(message)\n            await self.process_step_2(message)\n\n            logger.success(\"Message processing completed\")\n\n    async def process_step_1(self, message):\n        # This log will automatically include correlation_id\n        logger.debug(\"Executing step 1\")\n\n    async def process_step_2(self, message):\n        # This log will automatically include correlation_id\n        logger.debug(\"Executing step 2\")\n</code></pre>"},{"location":"user-guide/logging/#error-handling-and-logging","title":"\ud83d\udee1\ufe0f Error Handling and Logging","text":""},{"location":"user-guide/logging/#exception-logging","title":"Exception Logging","text":"<p>Automatically log exceptions with full context:</p> <pre><code>from pythia.logging.decorators import log_exceptions\nfrom loguru import logger\n\nclass SafeWorker(Worker):\n    @log_exceptions(level=\"ERROR\", reraise=True)\n    async def process(self, message):\n        # Any exception will be automatically logged with context\n        result = await self.risky_operation(message)\n        return result\n\n    async def risky_operation(self, message):\n        try:\n            # Risky code here\n            return await external_api_call(message.body)\n        except ValueError as e:\n            logger.warning(\"Validation error\",\n                          error=str(e),\n                          message_data=message.body)\n            raise\n        except Exception as e:\n            logger.error(\"Unexpected error in risky_operation\",\n                        error=str(e),\n                        error_type=type(e).__name__,\n                        message_id=message.message_id)\n            raise\n</code></pre>"},{"location":"user-guide/logging/#log-analysis-and-monitoring","title":"\ud83d\udd0d Log Analysis and Monitoring","text":""},{"location":"user-guide/logging/#integration-with-observability-stack","title":"Integration with Observability Stack","text":"<p>Configure Loguru to work with your observability tools:</p> <pre><code>from pythia.logging import setup_logging\nimport json\n\n# Configure for Grafana Loki\nsetup_logging(\n    format_type=\"json\",\n    log_file=\"/app/logs/worker.jsonl\",\n    extra_fields={\n        \"service\": \"email-worker\",\n        \"version\": \"1.0.0\",\n        \"environment\": \"production\"\n    }\n)\n\n# Add custom log processor for OpenTelemetry\ndef add_trace_context(record):\n    from opentelemetry import trace\n    span = trace.get_current_span()\n    if span:\n        record[\"extra\"][\"trace_id\"] = format(span.get_span_context().trace_id, \"032x\")\n        record[\"extra\"][\"span_id\"] = format(span.get_span_context().span_id, \"016x\")\n\nlogger.configure(\n    processors=[add_trace_context]\n)\n</code></pre>"},{"location":"user-guide/logging/#health-check-logging","title":"Health Check Logging","text":"<p>Log health check information for monitoring:</p> <pre><code>from pythia.logging import health_logger\n\nclass MonitoredWorker(Worker):\n    async def startup(self):\n        health_logger.info(\"Worker starting up\",\n                          worker_type=self.__class__.__name__)\n\n    async def process(self, message):\n        # Regular processing...\n        pass\n\n    async def health_check(self):\n        try:\n            # Check database connection, etc.\n            health_logger.success(\"Health check passed\")\n            return {\"status\": \"healthy\"}\n        except Exception as e:\n            health_logger.error(\"Health check failed\", error=str(e))\n            return {\"status\": \"unhealthy\", \"error\": str(e)}\n</code></pre>"},{"location":"user-guide/logging/#best-practices","title":"\ud83d\udcc8 Best Practices","text":""},{"location":"user-guide/logging/#1-use-structured-logging","title":"1. Use Structured Logging","text":"<p>Always prefer structured logging over string formatting:</p> <pre><code># \u274c Don't do this\nlogger.info(f\"Processing order {order_id} for user {user_id}\")\n\n# \u2705 Do this instead\nlogger.info(\"Processing order\", order_id=order_id, user_id=user_id)\n</code></pre>"},{"location":"user-guide/logging/#2-log-meaningful-information","title":"2. Log Meaningful Information","text":"<p>Include context that helps with debugging:</p> <pre><code>logger.info(\"Message processed successfully\",\n           message_id=message.message_id,\n           processing_time_ms=processing_time * 1000,\n           queue_depth=queue.depth(),\n           retry_count=message.retry_count)\n</code></pre>"},{"location":"user-guide/logging/#3-use-appropriate-log-levels","title":"3. Use Appropriate Log Levels","text":"<pre><code># Use INFO for important business events\nlogger.info(\"Order created\", order_id=order.id, customer_id=customer.id)\n\n# Use DEBUG for detailed debugging information\nlogger.debug(\"Validation step\", field=\"email\", value=\"user@example.com\", valid=True)\n\n# Use SUCCESS for completed operations (Loguru-specific)\nlogger.success(\"Email sent\", recipient=\"user@example.com\", template=\"welcome\")\n\n# Use WARNING for recoverable issues\nlogger.warning(\"Retry required\", attempt=2, max_attempts=3, reason=\"timeout\")\n</code></pre>"},{"location":"user-guide/logging/#4-handle-sensitive-data","title":"4. Handle Sensitive Data","text":"<p>Never log sensitive information:</p> <pre><code># \u274c Never log sensitive data\nlogger.info(\"User login\", password=password, credit_card=cc_number)\n\n# \u2705 Log safely\nlogger.info(\"User login\",\n           user_id=user.id,\n           login_method=\"password\",\n           password_length=len(password))\n</code></pre>"},{"location":"user-guide/logging/#testing-logging","title":"\ud83e\uddea Testing Logging","text":""},{"location":"user-guide/logging/#testing-log-output","title":"Testing Log Output","text":"<p>Test that your workers log correctly:</p> <pre><code>import pytest\nfrom loguru import logger\nfrom pythia.utils.testing import capture_logs\n\nclass TestWorkerLogging:\n    def test_successful_processing_logs(self):\n        worker = MyWorker()\n        message = create_test_message({\"email\": \"test@example.com\"})\n\n        with capture_logs() as logs:\n            result = await worker.process(message)\n\n        # Assert logs were generated\n        assert len(logs) &gt;= 2\n        assert logs[0].level == \"INFO\"\n        assert \"Processing email\" in logs[0].message\n        assert logs[0].email == \"test@example.com\"\n\n        assert logs[1].level == \"SUCCESS\"\n        assert \"Email sent successfully\" in logs[1].message\n\n    def test_error_logging(self):\n        worker = MyWorker()\n        message = create_test_message({\"invalid\": \"data\"})\n\n        with capture_logs() as logs:\n            with pytest.raises(ValidationError):\n                await worker.process(message)\n\n        # Assert error was logged\n        error_logs = [log for log in logs if log.level == \"ERROR\"]\n        assert len(error_logs) == 1\n        assert \"Failed to send email\" in error_logs[0].message\n</code></pre>"},{"location":"user-guide/logging/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Loguru Documentation: https://loguru.readthedocs.io/</li> <li>Structured Logging Best Practices: [Link to best practices guide]</li> <li>Integration with Grafana: [Link to Grafana integration guide]</li> <li>OpenTelemetry Integration: [Link to OpenTelemetry guide]</li> </ul> <p>Ready to implement robust logging in your Pythia workers? Start with the basic configuration and gradually add more advanced features as your needs grow!</p>"},{"location":"user-guide/message-handling/","title":"Message Handling","text":"<p>Complete guide to processing messages in Pythia workers, including message structure, acknowledgment patterns, and processing strategies.</p>"},{"location":"user-guide/message-handling/#message-structure","title":"Message Structure","text":"<p>Pythia provides a unified message abstraction that works across all brokers:</p> <pre><code>from pythia.core import Message\nfrom datetime import datetime\nfrom typing import Dict, Any\n\n# Universal message structure\n@dataclass\nclass Message:\n    # Core data\n    body: Union[str, bytes, dict, Any]    # Message payload\n    message_id: str                       # Unique identifier\n    timestamp: datetime                   # Message timestamp\n    headers: Dict[str, Any]              # Message headers\n\n    # Broker-specific metadata\n    topic: Optional[str] = None          # Kafka topic\n    queue: Optional[str] = None          # Queue name\n    routing_key: Optional[str] = None    # RabbitMQ routing key\n    partition: Optional[int] = None      # Kafka partition\n    offset: Optional[int] = None         # Kafka offset\n    exchange: Optional[str] = None       # RabbitMQ exchange\n\n    # Redis-specific fields\n    stream_id: Optional[str] = None      # Redis stream ID\n    channel: Optional[str] = None        # Redis channel\n\n    # RabbitMQ-specific fields\n    delivery_tag: Optional[int] = None   # RabbitMQ delivery tag\n\n    # Processing metadata\n    retry_count: int = 0                 # Current retry attempt\n    max_retries: int = 3                # Maximum retries allowed\n</code></pre>"},{"location":"user-guide/message-handling/#basic-message-processing","title":"Basic Message Processing","text":""},{"location":"user-guide/message-handling/#simple-message-handler","title":"Simple Message Handler","text":"<pre><code>import json\nfrom pythia.core import Worker, Message\n\nclass BasicMessageProcessor(Worker):\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process a single message\"\"\"\n        try:\n            # Parse message body if it's JSON\n            if isinstance(message.body, str):\n                data = json.loads(message.body)\n            else:\n                data = message.body\n\n            self.logger.info(\n                f\"Processing message {message.message_id}\",\n                extra={\n                    \"message_id\": message.message_id,\n                    \"timestamp\": message.timestamp,\n                    \"data_type\": type(data).__name__\n                }\n            )\n\n            # Your business logic here\n            result = await self._process_data(data)\n\n            return {\n                \"status\": \"success\",\n                \"message_id\": message.message_id,\n                \"result\": result,\n                \"processed_at\": datetime.utcnow().isoformat()\n            }\n\n        except json.JSONDecodeError as e:\n            self.logger.error(f\"Invalid JSON in message: {e}\")\n            raise ValueError(f\"Invalid message format: {e}\")\n\n        except Exception as e:\n            self.logger.error(f\"Processing failed: {e}\")\n            raise\n\n    async def _process_data(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Business logic implementation\"\"\"\n        # Implement your specific processing logic\n        return {\"processed\": True, \"data\": data}\n</code></pre>"},{"location":"user-guide/message-handling/#message-acknowledgment-patterns","title":"Message Acknowledgment Patterns","text":""},{"location":"user-guide/message-handling/#manual-acknowledgment-recommended","title":"Manual Acknowledgment (Recommended)","text":"<pre><code>class ReliableMessageProcessor(Worker):\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process message with manual acknowledgment\"\"\"\n        try:\n            # Process the message\n            result = await self._process_business_logic(message)\n\n            # Acknowledge only after successful processing\n            await self._acknowledge_message(message)\n\n            return result\n\n        except RecoverableError as e:\n            # Temporary error - reject and requeue\n            self.logger.warning(f\"Recoverable error, rejecting: {e}\")\n            await self._reject_message(message, requeue=True)\n            raise\n\n        except PermanentError as e:\n            # Permanent error - reject without requeue\n            self.logger.error(f\"Permanent error, discarding: {e}\")\n            await self._reject_message(message, requeue=False)\n            raise\n\n    async def _acknowledge_message(self, message: Message):\n        \"\"\"Acknowledge message based on broker type\"\"\"\n        if hasattr(message, 'ack') and callable(message.ack):\n            await message.ack()\n        else:\n            # Framework handles acknowledgment automatically\n            pass\n\n    async def _reject_message(self, message: Message, requeue: bool = True):\n        \"\"\"Reject message based on broker type\"\"\"\n        if hasattr(message, 'nack') and callable(message.nack):\n            await message.nack(requeue=requeue)\n        else:\n            # Framework handles rejection automatically\n            if not requeue:\n                # Move to dead letter queue if available\n                await self._send_to_dlq(message)\n</code></pre>"},{"location":"user-guide/message-handling/#auto-acknowledgment","title":"Auto Acknowledgment","text":"<pre><code>class FastMessageProcessor(Worker):\n    \"\"\"For high-throughput, low-reliability scenarios\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        # Enable auto-acknowledgment in broker config\n        # This acknowledges messages immediately upon receipt\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with auto-acknowledgment\"\"\"\n        # Message is already acknowledged\n        # If processing fails, message is lost\n        result = await self._fast_processing(message)\n        return result\n</code></pre>"},{"location":"user-guide/message-handling/#message-processing-patterns","title":"Message Processing Patterns","text":""},{"location":"user-guide/message-handling/#1-single-message-processing","title":"1. Single Message Processing","text":"<pre><code>class SingleMessageProcessor(Worker):\n    \"\"\"Process one message at a time\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process individual message\"\"\"\n        start_time = time.time()\n\n        try:\n            # Extract and validate data\n            data = await self._extract_data(message)\n            await self._validate_data(data)\n\n            # Process the data\n            result = await self._business_logic(data)\n\n            # Record metrics\n            processing_time = time.time() - start_time\n            self.metrics.histogram(\"message_processing_seconds\").observe(processing_time)\n\n            return result\n\n        except ValidationError as e:\n            self.logger.error(f\"Validation failed: {e}\")\n            # Don't retry validation errors\n            raise PermanentError(str(e))\n\n    async def _extract_data(self, message: Message) -&gt; Dict[str, Any]:\n        \"\"\"Extract data from message\"\"\"\n        if isinstance(message.body, dict):\n            return message.body\n        elif isinstance(message.body, str):\n            return json.loads(message.body)\n        else:\n            raise ValueError(f\"Unsupported message body type: {type(message.body)}\")\n\n    async def _validate_data(self, data: Dict[str, Any]):\n        \"\"\"Validate message data\"\"\"\n        required_fields = [\"id\", \"action\", \"data\"]\n        missing_fields = [field for field in required_fields if field not in data]\n\n        if missing_fields:\n            raise ValidationError(f\"Missing required fields: {missing_fields}\")\n</code></pre>"},{"location":"user-guide/message-handling/#2-batch-processing","title":"2. Batch Processing","text":"<pre><code>from typing import List\n\nclass BatchMessageProcessor(Worker):\n    \"\"\"Process messages in batches for efficiency\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.batch_size = config.batch_size or 10\n        self.batch_timeout = 30  # seconds\n\n    async def process_messages_batch(self, messages: List[Message]) -&gt; List[Any]:\n        \"\"\"Process a batch of messages\"\"\"\n        self.logger.info(f\"Processing batch of {len(messages)} messages\")\n\n        try:\n            # Extract all data\n            batch_data = [await self._extract_data(msg) for msg in messages]\n\n            # Process as batch (more efficient for database operations)\n            results = await self._process_batch(batch_data)\n\n            # Acknowledge all messages on success\n            for message in messages:\n                await self._acknowledge_message(message)\n\n            return results\n\n        except Exception as e:\n            # Handle batch failure\n            await self._handle_batch_error(messages, e)\n            raise\n\n    async def _process_batch(self, batch_data: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Process entire batch efficiently\"\"\"\n        # Example: Bulk database insert\n        async with self.db_pool.acquire() as conn:\n            async with conn.transaction():\n                results = []\n                for data in batch_data:\n                    result = await conn.fetchrow(\n                        \"INSERT INTO processed_data (data) VALUES ($1) RETURNING id\",\n                        json.dumps(data)\n                    )\n                    results.append({\"id\": result[\"id\"], \"status\": \"processed\"})\n                return results\n\n    async def _handle_batch_error(self, messages: List[Message], error: Exception):\n        \"\"\"Handle batch processing error\"\"\"\n        self.logger.error(f\"Batch processing failed: {error}\")\n\n        # Try processing messages individually\n        for message in messages:\n            try:\n                await self.process_message(message)\n            except Exception:\n                await self._reject_message(message, requeue=True)\n</code></pre>"},{"location":"user-guide/message-handling/#3-streaming-processing","title":"3. Streaming Processing","text":"<pre><code>import asyncio\nfrom asyncio import Queue\n\nclass StreamingProcessor(Worker):\n    \"\"\"Process messages in a continuous stream\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.message_queue = Queue(maxsize=1000)\n        self.processing_task = None\n\n    async def on_startup(self):\n        \"\"\"Start streaming processor\"\"\"\n        await super().on_startup()\n        self.processing_task = asyncio.create_task(self._stream_processor())\n\n    async def on_shutdown(self):\n        \"\"\"Stop streaming processor\"\"\"\n        if self.processing_task:\n            self.processing_task.cancel()\n        await super().on_shutdown()\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Add message to stream queue\"\"\"\n        try:\n            await self.message_queue.put(message)\n            return {\"status\": \"queued\"}\n        except asyncio.QueueFull:\n            self.logger.warning(\"Message queue full, rejecting message\")\n            raise TemporaryError(\"Queue full, retry later\")\n\n    async def _stream_processor(self):\n        \"\"\"Continuous stream processor\"\"\"\n        while True:\n            try:\n                # Get message with timeout\n                message = await asyncio.wait_for(\n                    self.message_queue.get(), timeout=1.0\n                )\n\n                # Process in stream\n                await self._process_streaming_message(message)\n\n            except asyncio.TimeoutError:\n                # No message available, continue\n                continue\n            except Exception as e:\n                self.logger.error(f\"Stream processing error: {e}\")\n\n    async def _process_streaming_message(self, message: Message):\n        \"\"\"Process message in streaming context\"\"\"\n        try:\n            # Your streaming logic here\n            result = await self._stream_business_logic(message)\n            await self._acknowledge_message(message)\n\n        except Exception as e:\n            await self._reject_message(message, requeue=True)\n            raise\n</code></pre>"},{"location":"user-guide/message-handling/#message-routing-filtering","title":"Message Routing &amp; Filtering","text":""},{"location":"user-guide/message-handling/#content-based-routing","title":"Content-Based Routing","text":"<pre><code>class RoutingProcessor(Worker):\n    \"\"\"Route messages based on content\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Route message based on content\"\"\"\n        data = json.loads(message.body)\n        message_type = data.get(\"type\")\n\n        # Route based on message type\n        if message_type == \"user_registration\":\n            return await self._handle_user_registration(data)\n        elif message_type == \"order_created\":\n            return await self._handle_order_created(data)\n        elif message_type == \"payment_processed\":\n            return await self._handle_payment_processed(data)\n        else:\n            self.logger.warning(f\"Unknown message type: {message_type}\")\n            raise ValueError(f\"Unsupported message type: {message_type}\")\n\n    async def _handle_user_registration(self, data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Handle user registration message\"\"\"\n        user_id = data[\"user_id\"]\n        email = data[\"email\"]\n\n        # Send welcome email\n        await self._send_welcome_email(email)\n\n        # Update user profile\n        await self._update_user_profile(user_id, data)\n\n        return {\"status\": \"user_registered\", \"user_id\": user_id}\n</code></pre>"},{"location":"user-guide/message-handling/#header-based-filtering","title":"Header-Based Filtering","text":"<pre><code>class FilteredProcessor(Worker):\n    \"\"\"Process messages based on headers\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process only messages matching criteria\"\"\"\n\n        # Check priority header\n        priority = message.headers.get(\"priority\", \"normal\")\n        if priority == \"low\":\n            # Skip low priority messages during high load\n            if await self._is_high_load():\n                self.logger.info(\"Skipping low priority message during high load\")\n                await self._defer_message(message)\n                return {\"status\": \"deferred\"}\n\n        # Check region header\n        region = message.headers.get(\"region\")\n        if region and region not in [\"us-east\", \"us-west\"]:\n            self.logger.info(f\"Skipping message from unsupported region: {region}\")\n            return {\"status\": \"skipped\", \"reason\": \"unsupported_region\"}\n\n        # Process the message\n        return await self._process_filtered_message(message)\n\n    async def _is_high_load(self) -&gt; bool:\n        \"\"\"Check if system is under high load\"\"\"\n        # Check various metrics\n        cpu_usage = await self._get_cpu_usage()\n        queue_depth = await self._get_queue_depth()\n\n        return cpu_usage &gt; 80 or queue_depth &gt; 1000\n</code></pre>"},{"location":"user-guide/message-handling/#error-handling-in-message-processing","title":"Error Handling in Message Processing","text":""},{"location":"user-guide/message-handling/#retry-mechanisms","title":"Retry Mechanisms","text":"<pre><code>class RetryableProcessor(Worker):\n    \"\"\"Processor with sophisticated retry logic\"\"\"\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process with retry logic\"\"\"\n        try:\n            return await self._attempt_processing(message)\n\n        except RecoverableError as e:\n            if message.retry_count &lt; message.max_retries:\n                await self._schedule_retry(message, e)\n                return {\"status\": \"retry_scheduled\"}\n            else:\n                await self._handle_max_retries_exceeded(message, e)\n                raise PermanentError(f\"Max retries exceeded: {e}\")\n\n    async def _schedule_retry(self, message: Message, error: Exception):\n        \"\"\"Schedule message retry with backoff\"\"\"\n        retry_delay = self._calculate_retry_delay(message.retry_count)\n\n        self.logger.info(\n            f\"Scheduling retry {message.retry_count + 1} in {retry_delay}s: {error}\"\n        )\n\n        # Update retry count\n        message.retry_count += 1\n        message.headers[\"retry_count\"] = message.retry_count\n        message.headers[\"last_error\"] = str(error)\n        message.headers[\"retry_scheduled_at\"] = datetime.utcnow().isoformat()\n\n        # Schedule for later processing\n        await self._publish_delayed_message(message, retry_delay)\n\n    def _calculate_retry_delay(self, retry_count: int) -&gt; int:\n        \"\"\"Calculate exponential backoff delay\"\"\"\n        base_delay = 2\n        max_delay = 300  # 5 minutes\n        delay = min(base_delay ** retry_count, max_delay)\n        return delay\n</code></pre>"},{"location":"user-guide/message-handling/#dead-letter-queue-handling","title":"Dead Letter Queue Handling","text":"<pre><code>class DLQProcessor(Worker):\n    \"\"\"Processor with dead letter queue handling\"\"\"\n\n    async def _handle_max_retries_exceeded(self, message: Message, error: Exception):\n        \"\"\"Handle message that exceeded max retries\"\"\"\n        dlq_message = {\n            \"original_message\": message.to_dict(),\n            \"final_error\": str(error),\n            \"retry_history\": message.headers.get(\"retry_history\", []),\n            \"failed_at\": datetime.utcnow().isoformat(),\n            \"worker_id\": self.config.worker_id\n        }\n\n        # Send to dead letter queue\n        await self._send_to_dlq(dlq_message)\n\n        # Alert monitoring\n        self.metrics.counter(\"messages_sent_to_dlq_total\").inc()\n\n        # Log for investigation\n        self.logger.error(\n            f\"Message sent to DLQ after {message.max_retries} retries: {error}\",\n            extra={\n                \"message_id\": message.message_id,\n                \"final_error\": str(error)\n            }\n        )\n\n    async def _send_to_dlq(self, dlq_message: Dict[str, Any]):\n        \"\"\"Send message to dead letter queue\"\"\"\n        # Implementation depends on broker type\n        if self.config.broker_type == \"rabbitmq\":\n            await self._send_to_rabbitmq_dlq(dlq_message)\n        elif self.config.broker_type == \"kafka\":\n            await self._send_to_kafka_dlq(dlq_message)\n        else:\n            # Generic DLQ (could be database, file, etc.)\n            await self._send_to_generic_dlq(dlq_message)\n</code></pre>"},{"location":"user-guide/message-handling/#performance-optimization","title":"Performance Optimization","text":""},{"location":"user-guide/message-handling/#message-prefetching","title":"Message Prefetching","text":"<pre><code>class OptimizedProcessor(Worker):\n    \"\"\"Processor optimized for high throughput\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        # Configure prefetch for better performance\n        self.prefetch_count = config.prefetch_count or 100\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Optimized message processing\"\"\"\n        # Use connection pooling\n        async with self.connection_pool.acquire() as conn:\n            # Batch operations when possible\n            result = await self._optimized_processing(message, conn)\n\n        return result\n</code></pre>"},{"location":"user-guide/message-handling/#parallel-processing","title":"Parallel Processing","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass ParallelProcessor(Worker):\n    \"\"\"Process messages in parallel\"\"\"\n\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.executor = ThreadPoolExecutor(max_workers=10)\n\n    async def process_message(self, message: Message) -&gt; Any:\n        \"\"\"Process message in parallel\"\"\"\n        # CPU-bound work in thread pool\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            self.executor,\n            self._cpu_intensive_work,\n            message.body\n        )\n\n        return result\n\n    def _cpu_intensive_work(self, data) -&gt; Dict[str, Any]:\n        \"\"\"CPU-intensive processing in thread\"\"\"\n        # Heavy computation here\n        import time\n        time.sleep(0.1)  # Simulate work\n        return {\"processed\": True, \"data\": data}\n</code></pre>"},{"location":"user-guide/message-handling/#testing-message-handling","title":"Testing Message Handling","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\n\nclass TestMessageHandling(WorkerTestCase):\n    async def test_successful_processing(self):\n        \"\"\"Test successful message processing\"\"\"\n        test_data = {\"id\": \"123\", \"action\": \"create\", \"data\": {\"name\": \"test\"}}\n        message = self.create_test_message(json.dumps(test_data))\n\n        result = await self.worker.process_message(message)\n\n        assert result[\"status\"] == \"success\"\n        assert result[\"message_id\"] == message.message_id\n\n    async def test_invalid_message_format(self):\n        \"\"\"Test handling of invalid message format\"\"\"\n        invalid_message = self.create_test_message(\"invalid json\")\n\n        with pytest.raises(ValueError):\n            await self.worker.process_message(invalid_message)\n\n    async def test_retry_logic(self):\n        \"\"\"Test message retry logic\"\"\"\n        # Create message that will fail\n        message = self.create_test_message('{\"should_fail\": true}')\n\n        # Mock failure on first attempt\n        with pytest.raises(RecoverableError):\n            await self.worker.process_message(message)\n\n        # Verify retry was scheduled\n        assert message.retry_count == 1\n</code></pre>"},{"location":"user-guide/message-handling/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration - Complete configuration guide</li> <li>Error Handling - Advanced error handling patterns</li> <li>Performance Optimization - Performance tuning guide</li> </ul>"},{"location":"user-guide/observability/","title":"Observability with Grafana, Tempo &amp; OpenTelemetry","text":"<p>Pythia provides comprehensive observability through metrics, logs, and distributed tracing using modern open-source tools: Grafana, Tempo, and OpenTelemetry.</p>"},{"location":"user-guide/observability/#overview","title":"\ud83c\udf1f Overview","text":"<p>The complete observability stack includes:</p> <ul> <li>\ud83d\udcca Grafana - Dashboards and alerting for metrics and logs</li> <li>\ud83d\udcc8 Prometheus - Metrics collection and storage</li> <li>\ud83d\udd0d Tempo - Distributed tracing storage and querying</li> <li>\ud83d\udd04 OpenTelemetry - Unified observability data collection</li> <li>\ud83d\udcdd Loki - Log aggregation and querying (optional)</li> </ul>"},{"location":"user-guide/observability/#quick-setup","title":"\ud83d\ude80 Quick Setup","text":""},{"location":"user-guide/observability/#docker-compose-stack","title":"Docker Compose Stack","text":"<p>Create a complete observability stack with Docker:</p> <pre><code># docker-compose.observability.yml\nversion: '3.8'\n\nservices:\n  # Grafana Dashboard\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3000:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor\n    volumes:\n      - grafana-data:/var/lib/grafana\n      - ./grafana/provisioning:/etc/grafana/provisioning\n      - ./grafana/dashboards:/var/lib/grafana/dashboards\n    networks:\n      - observability\n\n  # Prometheus Metrics\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--storage.tsdb.retention.time=15d'\n      - '--web.enable-lifecycle'\n      - '--web.enable-admin-api'\n    volumes:\n      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus-data:/prometheus\n    networks:\n      - observability\n\n  # Tempo Tracing\n  tempo:\n    image: grafana/tempo:latest\n    ports:\n      - \"3200:3200\"   # Tempo\n      - \"14268:14268\" # Jaeger ingest\n      - \"9411:9411\"   # Zipkin\n    command: [ \"-config.file=/etc/tempo.yaml\" ]\n    volumes:\n      - ./tempo/tempo.yaml:/etc/tempo.yaml\n      - tempo-data:/tmp/tempo\n    networks:\n      - observability\n\n  # Loki Logs (Optional)\n  loki:\n    image: grafana/loki:latest\n    ports:\n      - \"3100:3100\"\n    command: -config.file=/etc/loki/local-config.yaml\n    volumes:\n      - loki-data:/loki\n    networks:\n      - observability\n\n  # OpenTelemetry Collector\n  otel-collector:\n    image: otel/opentelemetry-collector-contrib:latest\n    command: [\"--config=/etc/otel-collector-config.yaml\"]\n    volumes:\n      - ./otel/otel-collector-config.yaml:/etc/otel-collector-config.yaml\n    ports:\n      - \"4317:4317\"   # OTLP gRPC receiver\n      - \"4318:4318\"   # OTLP HTTP receiver\n      - \"8888:8888\"   # Prometheus metrics\n    networks:\n      - observability\n    depends_on:\n      - tempo\n      - prometheus\n\nvolumes:\n  grafana-data:\n  prometheus-data:\n  tempo-data:\n  loki-data:\n\nnetworks:\n  observability:\n    driver: bridge\n</code></pre>"},{"location":"user-guide/observability/#configuration-files","title":"Configuration Files","text":"<p>Create the necessary configuration files:</p> <pre><code>mkdir -p grafana/provisioning/{datasources,dashboards}\nmkdir -p grafana/dashboards\nmkdir -p prometheus\nmkdir -p tempo\nmkdir -p otel\n</code></pre>"},{"location":"user-guide/observability/#prometheus-configuration","title":"Prometheus Configuration","text":"<pre><code># prometheus/prometheus.yml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'pythia-workers'\n    static_configs:\n      - targets: ['host.docker.internal:8000']  # Your worker metrics endpoint\n    scrape_interval: 5s\n    metrics_path: '/metrics'\n\n  - job_name: 'otel-collector'\n    static_configs:\n      - targets: ['otel-collector:8888']\n\nrule_files:\n  - \"pythia_alerts.yml\"\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets:\n          # - alertmanager:9093\n</code></pre>"},{"location":"user-guide/observability/#tempo-configuration","title":"Tempo Configuration","text":"<pre><code># tempo/tempo.yaml\nserver:\n  http_listen_port: 3200\n\ndistributor:\n  receivers:\n    jaeger:\n      protocols:\n        thrift_http:\n          endpoint: 0.0.0.0:14268\n        grpc:\n          endpoint: 0.0.0.0:14250\n    zipkin:\n      endpoint: 0.0.0.0:9411\n    otlp:\n      protocols:\n        http:\n          endpoint: 0.0.0.0:4318\n        grpc:\n          endpoint: 0.0.0.0:4317\n\ningester:\n  trace_idle_period: 10s\n  max_block_bytes: 1_000_000\n  max_block_duration: 5m\n\ncompactor:\n  compaction:\n    compacted_block_retention: 1h\n\nstorage:\n  trace:\n    backend: local\n    local:\n      path: /tmp/tempo/traces\n</code></pre>"},{"location":"user-guide/observability/#opentelemetry-collector-configuration","title":"OpenTelemetry Collector Configuration","text":"<pre><code># otel/otel-collector-config.yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc:\n        endpoint: 0.0.0.0:4317\n      http:\n        endpoint: 0.0.0.0:4318\n\n  prometheus:\n    config:\n      scrape_configs:\n        - job_name: 'pythia-workers'\n          static_configs:\n            - targets: ['host.docker.internal:8000']\n\nprocessors:\n  batch:\n\nexporters:\n  # Send traces to Tempo\n  otlp/tempo:\n    endpoint: tempo:4317\n    tls:\n      insecure: true\n\n  # Send metrics to Prometheus\n  prometheus:\n    endpoint: \"0.0.0.0:8888\"\n\n  # Debug exporter for development\n  logging:\n    loglevel: debug\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [otlp/tempo, logging]\n\n    metrics:\n      receivers: [otlp, prometheus]\n      processors: [batch]\n      exporters: [prometheus, logging]\n</code></pre>"},{"location":"user-guide/observability/#grafana-data-sources","title":"Grafana Data Sources","text":"<pre><code># grafana/provisioning/datasources/datasources.yml\napiVersion: 1\n\ndatasources:\n  - name: Prometheus\n    type: prometheus\n    url: http://prometheus:9090\n    access: proxy\n    isDefault: true\n\n  - name: Tempo\n    type: tempo\n    url: http://tempo:3200\n    access: proxy\n\n  - name: Loki\n    type: loki\n    url: http://loki:3100\n    access: proxy\n</code></pre>"},{"location":"user-guide/observability/#pythia-worker-integration","title":"\ud83d\udcca Pythia Worker Integration","text":""},{"location":"user-guide/observability/#enable-metrics-and-tracing","title":"Enable Metrics and Tracing","text":"<pre><code>from pythia import Worker\nfrom pythia.brokers.redis import RedisConsumer\nfrom pythia.monitoring import setup_observability\nfrom opentelemetry import trace\nfrom loguru import logger\nimport time\n\n# Setup observability\nsetup_observability(\n    service_name=\"email-worker\",\n    service_version=\"1.0.0\",\n    otlp_endpoint=\"http://localhost:4317\",\n    metrics_enabled=True,\n    tracing_enabled=True,\n    logs_enabled=True\n)\n\ntracer = trace.get_tracer(__name__)\n\nclass ObservableEmailWorker(Worker):\n    source = RedisConsumer(queue_name=\"emails\")\n\n    async def process(self, message):\n        with tracer.start_as_current_span(\"process_email\") as span:\n            # Add span attributes\n            span.set_attribute(\"email.recipient\", message.body.get(\"email\"))\n            span.set_attribute(\"worker.type\", \"email\")\n\n            try:\n                start_time = time.time()\n\n                # Process email\n                result = await self._send_email(message.body)\n\n                processing_time = time.time() - start_time\n                span.set_attribute(\"processing.duration_ms\", processing_time * 1000)\n                span.set_attribute(\"email.status\", \"sent\")\n\n                logger.info(\"Email processed successfully\",\n                           email=message.body.get(\"email\"),\n                           processing_time_ms=processing_time * 1000)\n\n                return result\n\n            except Exception as e:\n                span.record_exception(e)\n                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n\n                logger.error(\"Email processing failed\",\n                           email=message.body.get(\"email\"),\n                           error=str(e))\n                raise\n\n    async def _send_email(self, data):\n        with tracer.start_as_current_span(\"send_email\") as span:\n            span.set_attribute(\"email.provider\", \"smtp\")\n\n            # Simulate email sending\n            await asyncio.sleep(0.1)\n\n            return {\"status\": \"sent\", \"message_id\": \"msg_123\"}\n\n# Run worker\nif __name__ == \"__main__\":\n    worker = ObservableEmailWorker()\n    worker.run_sync()\n</code></pre>"},{"location":"user-guide/observability/#custom-metrics","title":"Custom Metrics","text":"<pre><code>from pythia.monitoring.metrics import Counter, Histogram, Gauge\nfrom prometheus_client import start_http_server\n\nclass MetricsEnabledWorker(Worker):\n    def __init__(self):\n        super().__init__()\n\n        # Custom metrics\n        self.messages_processed = Counter(\n            'pythia_messages_processed_total',\n            'Total messages processed',\n            ['worker_type', 'status']\n        )\n\n        self.processing_duration = Histogram(\n            'pythia_message_processing_seconds',\n            'Message processing duration',\n            ['worker_type']\n        )\n\n        self.queue_size = Gauge(\n            'pythia_queue_size',\n            'Current queue size',\n            ['queue_name']\n        )\n\n        # Start metrics server\n        start_http_server(8000)\n\n    async def process(self, message):\n        start_time = time.time()\n\n        try:\n            result = await self._process_message(message)\n\n            # Record success metrics\n            self.messages_processed.labels(\n                worker_type=self.__class__.__name__,\n                status='success'\n            ).inc()\n\n            duration = time.time() - start_time\n            self.processing_duration.labels(\n                worker_type=self.__class__.__name__\n            ).observe(duration)\n\n            return result\n\n        except Exception as e:\n            # Record error metrics\n            self.messages_processed.labels(\n                worker_type=self.__class__.__name__,\n                status='error'\n            ).inc()\n            raise\n\n    async def health_check(self):\n        # Update queue size metric\n        queue_size = await self.source.get_queue_size()\n        self.queue_size.labels(\n            queue_name=self.source.queue_name\n        ).set(queue_size)\n\n        return {\"status\": \"healthy\", \"queue_size\": queue_size}\n</code></pre>"},{"location":"user-guide/observability/#pre-built-grafana-dashboards","title":"\ud83d\udcc8 Pre-built Grafana Dashboards","text":""},{"location":"user-guide/observability/#worker-overview-dashboard","title":"Worker Overview Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"id\": null,\n    \"title\": \"Pythia Workers Overview\",\n    \"description\": \"Overview of all Pythia workers performance and health\",\n    \"panels\": [\n      {\n        \"title\": \"Messages Processed/sec\",\n        \"type\": \"stat\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(pythia_messages_processed_total[5m])\",\n            \"legendFormat\": \"{{worker_type}} - {{status}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Processing Duration\",\n        \"type\": \"timeseries\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, rate(pythia_message_processing_seconds_bucket[5m]))\",\n            \"legendFormat\": \"P95 - {{worker_type}}\"\n          },\n          {\n            \"expr\": \"histogram_quantile(0.50, rate(pythia_message_processing_seconds_bucket[5m]))\",\n            \"legendFormat\": \"P50 - {{worker_type}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Error Rate\",\n        \"type\": \"timeseries\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(pythia_messages_processed_total{status=\\\"error\\\"}[5m]) / rate(pythia_messages_processed_total[5m]) * 100\",\n            \"legendFormat\": \"Error Rate % - {{worker_type}}\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Queue Sizes\",\n        \"type\": \"timeseries\",\n        \"targets\": [\n          {\n            \"expr\": \"pythia_queue_size\",\n            \"legendFormat\": \"{{queue_name}}\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"user-guide/observability/#tracing-dashboard","title":"Tracing Dashboard","text":"<p>Create a tracing dashboard to view distributed traces:</p> <pre><code>{\n  \"dashboard\": {\n    \"title\": \"Pythia Distributed Tracing\",\n    \"panels\": [\n      {\n        \"title\": \"Trace Search\",\n        \"type\": \"traces\",\n        \"datasource\": \"Tempo\",\n        \"targets\": [\n          {\n            \"query\": \"{ service.name=\\\"email-worker\\\" }\",\n            \"queryType\": \"\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Service Map\",\n        \"type\": \"nodeGraph\",\n        \"datasource\": \"Tempo\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"user-guide/observability/#alerting-rules","title":"\ud83d\udea8 Alerting Rules","text":"<p>Create alerting rules for common issues:</p> <pre><code># prometheus/pythia_alerts.yml\ngroups:\n  - name: pythia_workers\n    rules:\n      - alert: HighErrorRate\n        expr: rate(pythia_messages_processed_total{status=\"error\"}[5m]) / rate(pythia_messages_processed_total[5m]) * 100 &gt; 5\n        for: 2m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High error rate in Pythia worker\"\n          description: \"Worker {{ $labels.worker_type }} has an error rate of {{ $value }}%\"\n\n      - alert: HighProcessingLatency\n        expr: histogram_quantile(0.95, rate(pythia_message_processing_seconds_bucket[5m])) &gt; 5\n        for: 1m\n        labels:\n          severity: warning\n        annotations:\n          summary: \"High processing latency in Pythia worker\"\n          description: \"Worker {{ $labels.worker_type }} P95 latency is {{ $value }}s\"\n\n      - alert: QueueBacklog\n        expr: pythia_queue_size &gt; 1000\n        for: 5m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Large queue backlog\"\n          description: \"Queue {{ $labels.queue_name }} has {{ $value }} pending messages\"\n\n      - alert: WorkerDown\n        expr: up{job=\"pythia-workers\"} == 0\n        for: 1m\n        labels:\n          severity: critical\n        annotations:\n          summary: \"Pythia worker is down\"\n          description: \"Worker instance {{ $labels.instance }} is not responding\"\n</code></pre>"},{"location":"user-guide/observability/#running-the-stack","title":"\ud83c\udfc3 Running the Stack","text":""},{"location":"user-guide/observability/#start-observability-stack","title":"Start Observability Stack","text":"<pre><code># Start all services\ndocker-compose -f docker-compose.observability.yml up -d\n\n# Check services are running\ndocker-compose -f docker-compose.observability.yml ps\n\n# View logs\ndocker-compose -f docker-compose.observability.yml logs -f grafana\n</code></pre>"},{"location":"user-guide/observability/#access-dashboards","title":"Access Dashboards","text":"<ul> <li>Grafana: http://localhost:3000 (admin/admin)</li> <li>Prometheus: http://localhost:9090</li> <li>Tempo: http://localhost:3200</li> </ul>"},{"location":"user-guide/observability/#import-dashboards","title":"Import Dashboards","text":"<ol> <li>Open Grafana at http://localhost:3000</li> <li>Go to \"+\" \u2192 \"Import\"</li> <li>Upload the JSON dashboard files</li> <li>Configure data sources if needed</li> </ol>"},{"location":"user-guide/observability/#production-configuration","title":"\ud83d\udd27 Production Configuration","text":""},{"location":"user-guide/observability/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenTelemetry\nOTEL_SERVICE_NAME=email-worker\nOTEL_SERVICE_VERSION=1.0.0\nOTEL_EXPORTER_OTLP_ENDPOINT=https://your-otel-collector:4317\nOTEL_EXPORTER_OTLP_HEADERS=\"authorization=Bearer your-token\"\n\n# Metrics\nPYTHIA_METRICS_ENABLED=true\nPYTHIA_METRICS_PORT=8000\nPYTHIA_METRICS_ENDPOINT=/metrics\n\n# Tracing\nPYTHIA_TRACING_ENABLED=true\nPYTHIA_TRACING_SAMPLE_RATE=0.1  # Sample 10% of traces\n\n# Logging\nPYTHIA_LOG_LEVEL=INFO\nPYTHIA_LOG_FORMAT=json\n</code></pre>"},{"location":"user-guide/observability/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># kubernetes/observability-stack.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: grafana-datasources\ndata:\n  datasources.yaml: |\n    apiVersion: 1\n    datasources:\n      - name: Prometheus\n        type: prometheus\n        url: http://prometheus:9090\n        access: proxy\n      - name: Tempo\n        type: tempo\n        url: http://tempo:3200\n        access: proxy\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pythia-worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: pythia-worker\n  template:\n    metadata:\n      labels:\n        app: pythia-worker\n      annotations:\n        prometheus.io/scrape: \"true\"\n        prometheus.io/port: \"8000\"\n        prometheus.io/path: \"/metrics\"\n    spec:\n      containers:\n      - name: worker\n        image: your-registry/pythia-worker:latest\n        ports:\n        - containerPort: 8000\n          name: metrics\n        env:\n        - name: OTEL_EXPORTER_OTLP_ENDPOINT\n          value: \"http://otel-collector:4317\"\n        - name: PYTHIA_METRICS_ENABLED\n          value: \"true\"\n        - name: PYTHIA_TRACING_ENABLED\n          value: \"true\"\n</code></pre>"},{"location":"user-guide/observability/#best-practices","title":"\ud83d\udcda Best Practices","text":""},{"location":"user-guide/observability/#1-metric-naming","title":"1. Metric Naming","text":"<p>Follow Prometheus naming conventions:</p> <pre><code># \u2705 Good metric names\nmessages_processed_total\nmessage_processing_duration_seconds\nqueue_size_current\nworker_health_status\n\n# \u274c Avoid these patterns\nmessagesProcessed\nprocess_time_ms\nqueueLen\n</code></pre>"},{"location":"user-guide/observability/#2-trace-attributes","title":"2. Trace Attributes","text":"<p>Add meaningful attributes to spans:</p> <pre><code>span.set_attribute(\"user.id\", user_id)\nspan.set_attribute(\"message.type\", message_type)\nspan.set_attribute(\"queue.name\", queue_name)\nspan.set_attribute(\"worker.version\", \"1.0.0\")\n</code></pre>"},{"location":"user-guide/observability/#3-sampling-strategy","title":"3. Sampling Strategy","text":"<p>Use appropriate sampling for production:</p> <pre><code># High-volume services - sample less\nsetup_observability(tracing_sample_rate=0.01)  # 1%\n\n# Critical services - sample more\nsetup_observability(tracing_sample_rate=0.1)   # 10%\n\n# Development - sample everything\nsetup_observability(tracing_sample_rate=1.0)   # 100%\n</code></pre>"},{"location":"user-guide/observability/#4-dashboard-organization","title":"4. Dashboard Organization","text":"<p>Structure your dashboards:</p> <ul> <li>Overview Dashboard - High-level metrics across all workers</li> <li>Service Dashboards - Detailed metrics per worker type</li> <li>SLA Dashboards - SLA/SLO tracking</li> <li>Troubleshooting Dashboards - Error analysis and debugging</li> </ul>"},{"location":"user-guide/observability/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Set up the observability stack using Docker Compose</li> <li>Instrument your workers with metrics and tracing</li> <li>Import the pre-built dashboards in Grafana</li> <li>Configure alerting rules for your SLAs</li> <li>Test the complete flow from worker to dashboard</li> </ol> <p>Ready to achieve full observability of your Pythia workers!</p>"},{"location":"user-guide/testing-observability/","title":"Testing Observability Stack","text":"<p>This guide shows how to test your Pythia workers' observability features including metrics, tracing, and logging integration.</p>"},{"location":"user-guide/testing-observability/#testing-strategy","title":"\ud83e\uddea Testing Strategy","text":""},{"location":"user-guide/testing-observability/#test-categories","title":"Test Categories","text":"<ol> <li>Unit Tests - Individual component testing</li> <li>Integration Tests - End-to-end observability flow</li> <li>Contract Tests - OpenTelemetry spec compliance</li> <li>Performance Tests - Observability overhead measurement</li> </ol>"},{"location":"user-guide/testing-observability/#test-setup","title":"\ud83d\udd27 Test Setup","text":""},{"location":"user-guide/testing-observability/#test-dependencies","title":"Test Dependencies","text":"<pre><code># pyproject.toml\n[project.optional-dependencies]\ntest-observability = [\n    \"pytest&gt;=7.0.0\",\n    \"pytest-asyncio&gt;=0.21.0\",\n    \"pytest-mock&gt;=3.10.0\",\n    \"testcontainers[redis,kafka]&gt;=3.7.0\",\n    \"opentelemetry-test-utils&gt;=1.20.0\",\n    \"prometheus-client[test]&gt;=0.17.0\",\n    \"responses&gt;=0.23.0\",\n    \"fakeredis&gt;=2.18.0\",\n]\n</code></pre>"},{"location":"user-guide/testing-observability/#test-configuration","title":"Test Configuration","text":"<pre><code># tests/conftest.py\nimport pytest\nimport asyncio\nfrom unittest.mock import Mock, patch\nfrom testcontainers.redis import RedisContainer\nfrom testcontainers.compose import DockerCompose\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.test.test_utils import TraceRecorder, MetricsRecorder\nfrom pythia.monitoring import setup_observability\nfrom pythia.logging import setup_logging\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    \"\"\"Create an instance of the default event loop for the test session.\"\"\"\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture\ndef observability_stack():\n    \"\"\"Start observability stack with Docker Compose for integration tests\"\"\"\n    with DockerCompose(\"../examples/observability\", compose_file_name=\"docker-compose.observability.yml\") as compose:\n        # Wait for services to be ready\n        compose.wait_for(\"http://localhost:3000/api/health\")  # Grafana\n        compose.wait_for(\"http://localhost:9090/-/ready\")     # Prometheus\n        compose.wait_for(\"http://localhost:3200/ready\")       # Tempo\n        compose.wait_for(\"http://localhost:4317\")             # OTEL Collector\n        yield compose\n\n@pytest.fixture\ndef trace_recorder():\n    \"\"\"Mock trace recorder for capturing spans\"\"\"\n    recorder = TraceRecorder()\n    with patch(\"opentelemetry.trace.get_tracer_provider\") as mock_provider:\n        mock_provider.return_value = recorder.tracer_provider\n        yield recorder\n\n@pytest.fixture\ndef metrics_recorder():\n    \"\"\"Mock metrics recorder for capturing metrics\"\"\"\n    recorder = MetricsRecorder()\n    with patch(\"opentelemetry.metrics.get_meter_provider\") as mock_provider:\n        mock_provider.return_value = recorder.meter_provider\n        yield recorder\n\n@pytest.fixture\ndef redis_container():\n    \"\"\"Redis container for testing broker integration\"\"\"\n    with RedisContainer(\"redis:7-alpine\") as redis:\n        yield redis\n\n@pytest.fixture\nasync def setup_test_observability():\n    \"\"\"Setup observability for testing with mocked exporters\"\"\"\n    setup_observability(\n        service_name=\"test-worker\",\n        service_version=\"test\",\n        otlp_endpoint=\"http://localhost:4317\",\n        metrics_enabled=True,\n        tracing_enabled=True,\n        logs_enabled=True,\n        metrics_port=8001  # Different port for testing\n    )\n    yield\n    # Cleanup after test\n    trace._set_tracer_provider(None)\n    metrics._set_meter_provider(None)\n</code></pre>"},{"location":"user-guide/testing-observability/#testing-metrics","title":"\ud83d\udcca Testing Metrics","text":""},{"location":"user-guide/testing-observability/#unit-tests-for-metrics","title":"Unit Tests for Metrics","text":"<pre><code># tests/unit/test_metrics.py\nimport pytest\nimport asyncio\nimport time\nfrom unittest.mock import Mock, patch\nfrom prometheus_client import REGISTRY, CollectorRegistry\nfrom pythia import Worker\nfrom pythia.brokers.redis import RedisConsumer\nfrom pythia.monitoring import ObservabilityMixin, setup_observability\nfrom pythia.core.message import Message\n\nclass TestObservabilityMixin:\n\n    @pytest.fixture\n    def worker_class(self):\n        class TestWorker(ObservabilityMixin, Worker):\n            source = RedisConsumer(queue_name=\"test\")\n\n            async def process(self, message):\n                with self.start_span(\"process_message\") as span:\n                    span.set_attribute(\"message.type\", \"test\")\n                    await asyncio.sleep(0.01)  # Simulate processing\n                    return {\"status\": \"processed\"}\n\n        return TestWorker\n\n    def test_observability_mixin_initialization(self, worker_class, metrics_recorder):\n        \"\"\"Test that ObservabilityMixin initializes metrics correctly\"\"\"\n        worker = worker_class()\n\n        # Verify tracer is created\n        assert worker.tracer is not None\n        assert worker.meter is not None\n\n        # Verify metrics are created\n        assert worker.messages_processed is not None\n        assert worker.processing_duration is not None\n        assert worker.active_messages is not None\n\n    @pytest.mark.asyncio\n    async def test_message_processing_metrics(self, worker_class, metrics_recorder):\n        \"\"\"Test that message processing records metrics correctly\"\"\"\n        worker = worker_class()\n        message = Message(body={\"test\": \"data\"}, message_id=\"test-123\")\n\n        # Record start time\n        start_time = time.time()\n\n        # Process message\n        result = await worker.process(message)\n\n        duration = time.time() - start_time\n\n        # Verify result\n        assert result[\"status\"] == \"processed\"\n\n        # Check that metrics would be recorded\n        # (In real implementation, verify metrics are actually recorded)\n\n    def test_custom_metric_recording(self, worker_class):\n        \"\"\"Test custom metric recording methods\"\"\"\n        worker = worker_class()\n\n        # Test success metric\n        worker.record_message_processed(status=\"success\", queue=\"test\")\n\n        # Test error metric\n        worker.record_message_processed(status=\"error\", error_type=\"ValidationError\")\n\n        # Test duration metric\n        worker.record_processing_duration(0.5, message_type=\"email\")\n\n        # Test active messages\n        worker.increment_active_messages(worker_type=\"TestWorker\")\n        worker.decrement_active_messages(worker_type=\"TestWorker\")\n\nclass TestMetricsIntegration:\n\n    @pytest.mark.asyncio\n    async def test_prometheus_metrics_endpoint(self, setup_test_observability):\n        \"\"\"Test that Prometheus metrics endpoint is accessible\"\"\"\n        import requests\n\n        # Process some test data to generate metrics\n        worker = self.create_test_worker()\n        await worker.process(Message(body={\"test\": \"data\"}))\n\n        # Check metrics endpoint\n        response = requests.get(\"http://localhost:8001/metrics\")\n        assert response.status_code == 200\n\n        # Verify Pythia metrics are present\n        content = response.text\n        assert \"pythia_messages_processed_total\" in content\n        assert \"pythia_message_processing_duration_seconds\" in content\n        assert \"pythia_active_messages\" in content\n\n    def create_test_worker(self):\n        class TestWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                self.record_message_processed(\"success\")\n                self.record_processing_duration(0.1)\n                return {\"status\": \"processed\"}\n\n        return TestWorker()\n</code></pre>"},{"location":"user-guide/testing-observability/#testing-distributed-tracing","title":"\ud83d\udd0d Testing Distributed Tracing","text":""},{"location":"user-guide/testing-observability/#unit-tests-for-tracing","title":"Unit Tests for Tracing","text":"<pre><code># tests/unit/test_tracing.py\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom opentelemetry import trace\nfrom opentelemetry.test.test_utils import TraceRecorder\nfrom pythia.monitoring import ObservabilityMixin, create_pythia_tracer\nfrom pythia.core.message import Message\n\nclass TestDistributedTracing:\n\n    def test_tracer_creation(self):\n        \"\"\"Test Pythia tracer creation\"\"\"\n        tracer = create_pythia_tracer(\"test-worker\")\n        assert tracer is not None\n\n    @pytest.mark.asyncio\n    async def test_span_creation_and_attributes(self, trace_recorder):\n        \"\"\"Test span creation with proper attributes\"\"\"\n\n        class TestWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                with self.start_span(\"process_message\", message_type=\"email\") as span:\n                    span.set_attribute(\"user.id\", \"123\")\n                    span.set_attribute(\"processing.step\", \"validation\")\n                    return {\"status\": \"processed\"}\n\n        worker = TestWorker()\n        message = Message(body={\"email\": \"test@example.com\"}, message_id=\"msg-123\")\n\n        result = await worker.process(message)\n\n        # Verify span was created\n        spans = trace_recorder.get_finished_spans()\n        assert len(spans) == 1\n\n        span = spans[0]\n        assert span.name == \"process_message\"\n\n        # Verify attributes\n        attributes = span.attributes\n        assert attributes[\"worker.type\"] == \"TestWorker\"\n        assert attributes[\"pythia.version\"] == \"0.1.0\"\n        assert attributes[\"message_type\"] == \"email\"\n        assert attributes[\"user.id\"] == \"123\"\n        assert attributes[\"processing.step\"] == \"validation\"\n\n    @pytest.mark.asyncio\n    async def test_span_error_handling(self, trace_recorder):\n        \"\"\"Test span error recording\"\"\"\n\n        class ErrorWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                with self.start_span(\"process_message\") as span:\n                    raise ValueError(\"Test error\")\n\n        worker = ErrorWorker()\n        message = Message(body={}, message_id=\"msg-123\")\n\n        with pytest.raises(ValueError):\n            await worker.process(message)\n\n        # Verify error was recorded in span\n        spans = trace_recorder.get_finished_spans()\n        assert len(spans) == 1\n\n        span = spans[0]\n        assert span.status.status_code == trace.StatusCode.ERROR\n\n        # Verify exception event\n        events = span.events\n        assert len(events) == 1\n        assert events[0].name == \"exception\"\n\n    @pytest.mark.asyncio\n    async def test_nested_spans(self, trace_recorder):\n        \"\"\"Test nested span creation\"\"\"\n\n        class NestedWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                with self.start_span(\"process_message\") as parent_span:\n                    await self._validate_message(message)\n                    await self._send_notification(message)\n                    return {\"status\": \"processed\"}\n\n            async def _validate_message(self, message):\n                with self.start_span(\"validate_message\") as span:\n                    span.set_attribute(\"validation.result\", \"passed\")\n\n            async def _send_notification(self, message):\n                with self.start_span(\"send_notification\") as span:\n                    span.set_attribute(\"notification.type\", \"email\")\n\n        worker = NestedWorker()\n        message = Message(body={}, message_id=\"msg-123\")\n\n        result = await worker.process(message)\n\n        # Verify all spans were created\n        spans = trace_recorder.get_finished_spans()\n        assert len(spans) == 3\n\n        # Verify span hierarchy\n        span_names = [span.name for span in spans]\n        assert \"process_message\" in span_names\n        assert \"validate_message\" in span_names\n        assert \"send_notification\" in span_names\n</code></pre>"},{"location":"user-guide/testing-observability/#testing-logging-integration","title":"\ud83d\udcdd Testing Logging Integration","text":""},{"location":"user-guide/testing-observability/#unit-tests-for-logging","title":"Unit Tests for Logging","text":"<pre><code># tests/unit/test_logging_integration.py\nimport pytest\nfrom unittest.mock import Mock, patch\nfrom loguru import logger\nfrom pythia.logging import setup_logging\nfrom pythia.monitoring.observability import _setup_logging_integration\n\nclass TestLoggingIntegration:\n\n    def test_logging_setup(self, caplog):\n        \"\"\"Test logging setup with observability\"\"\"\n        _setup_logging_integration(\"test-service\", \"1.0.0\")\n\n        # Test that logging includes service information\n        logger.info(\"Test message\", user_id=\"123\")\n\n        # Verify log structure (this would need custom log capture)\n        # In real implementation, verify structured logging format\n\n    @patch('pythia.monitoring.observability.trace.get_current_span')\n    def test_trace_context_in_logs(self, mock_get_span, caplog):\n        \"\"\"Test that trace context is added to logs\"\"\"\n\n        # Mock active span\n        mock_span = Mock()\n        mock_span.is_recording.return_value = True\n        mock_span_context = Mock()\n        mock_span_context.trace_id = 123456789\n        mock_span_context.span_id = 987654321\n        mock_span.get_span_context.return_value = mock_span_context\n        mock_get_span.return_value = mock_span\n\n        _setup_logging_integration(\"test-service\", \"1.0.0\")\n\n        # Log a message\n        logger.info(\"Test with trace context\")\n\n        # Verify trace context is included\n        # (Implementation would verify log output contains trace_id and span_id)\n\nclass TestLogCapture:\n    \"\"\"Utility class for capturing and verifying logs in tests\"\"\"\n\n    def __init__(self):\n        self.logs = []\n\n    def capture_logs(self):\n        \"\"\"Context manager to capture logs\"\"\"\n        return self\n\n    def __enter__(self):\n        # Setup log capture\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Cleanup log capture\n        pass\n\n    def get_logs(self, level=None):\n        \"\"\"Get captured logs, optionally filtered by level\"\"\"\n        if level:\n            return [log for log in self.logs if log.level == level]\n        return self.logs\n</code></pre>"},{"location":"user-guide/testing-observability/#integration-testing","title":"\ud83c\udfd7\ufe0f Integration Testing","text":""},{"location":"user-guide/testing-observability/#end-to-end-observability-tests","title":"End-to-End Observability Tests","text":"<pre><code># tests/integration/test_observability_e2e.py\nimport pytest\nimport asyncio\nimport requests\nimport time\nfrom testcontainers.redis import RedisContainer\nfrom pythia import Worker\nfrom pythia.brokers.redis import RedisConsumer\nfrom pythia.monitoring import setup_observability, ObservabilityMixin\n\nclass TestObservabilityE2E:\n\n    @pytest.mark.asyncio\n    @pytest.mark.integration\n    async def test_complete_observability_flow(self, observability_stack, redis_container):\n        \"\"\"Test complete observability flow from worker to dashboards\"\"\"\n\n        # Setup observability with real endpoints\n        setup_observability(\n            service_name=\"test-e2e-worker\",\n            service_version=\"1.0.0\",\n            otlp_endpoint=\"http://localhost:4317\",\n            metrics_enabled=True,\n            tracing_enabled=True,\n            logs_enabled=True,\n            metrics_port=8002\n        )\n\n        # Create worker with observability\n        class E2EWorker(ObservabilityMixin, Worker):\n            source = RedisConsumer(\n                host=redis_container.get_container_host_ip(),\n                port=redis_container.get_exposed_port(6379),\n                queue_name=\"test-e2e\"\n            )\n\n            async def process(self, message):\n                with self.start_span(\"e2e_process\", test_run=True) as span:\n                    # Simulate processing with various outcomes\n                    if message.body.get(\"fail\"):\n                        span.set_attribute(\"error\", True)\n                        raise ValueError(\"Simulated error\")\n\n                    # Add processing delay\n                    await asyncio.sleep(0.1)\n\n                    span.set_attribute(\"success\", True)\n                    return {\"processed\": True, \"message_id\": message.message_id}\n\n        worker = E2EWorker()\n\n        # Send test messages\n        await self._send_test_messages(worker, redis_container)\n\n        # Wait for metrics to be exported\n        await asyncio.sleep(5)\n\n        # Verify metrics in Prometheus\n        await self._verify_prometheus_metrics()\n\n        # Verify traces in Tempo\n        await self._verify_tempo_traces()\n\n        # Verify Grafana can query data\n        await self._verify_grafana_data()\n\n    async def _send_test_messages(self, worker, redis_container):\n        \"\"\"Send variety of test messages\"\"\"\n        import redis\n\n        r = redis.Redis(\n            host=redis_container.get_container_host_ip(),\n            port=redis_container.get_exposed_port(6379)\n        )\n\n        # Send successful messages\n        for i in range(10):\n            r.lpush(\"test-e2e\", f'{{\"id\": {i}, \"type\": \"success\"}}')\n\n        # Send failing messages\n        for i in range(2):\n            r.lpush(\"test-e2e\", f'{{\"id\": {i}, \"type\": \"error\", \"fail\": true}}')\n\n        # Process messages\n        for _ in range(12):  # 10 success + 2 errors\n            try:\n                await worker._process_single_message()\n            except ValueError:\n                pass  # Expected for error messages\n\n    async def _verify_prometheus_metrics(self):\n        \"\"\"Verify metrics are available in Prometheus\"\"\"\n\n        # Wait for metrics to be scraped\n        await asyncio.sleep(10)\n\n        # Query Prometheus API\n        response = requests.get(\n            \"http://localhost:9090/api/v1/query\",\n            params={\"query\": \"pythia_messages_processed_total\"}\n        )\n\n        assert response.status_code == 200\n        data = response.json()\n\n        # Verify we have metrics data\n        assert data[\"status\"] == \"success\"\n        assert len(data[\"data\"][\"result\"]) &gt; 0\n\n        # Verify we have both success and error metrics\n        results = data[\"data\"][\"result\"]\n        statuses = [result[\"metric\"][\"status\"] for result in results]\n        assert \"success\" in statuses\n        assert \"error\" in statuses\n\n    async def _verify_tempo_traces(self):\n        \"\"\"Verify traces are stored in Tempo\"\"\"\n\n        # Query Tempo API for traces\n        response = requests.get(\n            \"http://localhost:3200/api/search\",\n            params={\n                \"tags\": \"service.name=test-e2e-worker\",\n                \"limit\": 20\n            }\n        )\n\n        assert response.status_code == 200\n        data = response.json()\n\n        # Verify traces exist\n        assert \"traces\" in data\n        assert len(data[\"traces\"]) &gt; 0\n\n        # Get a specific trace\n        trace_id = data[\"traces\"][0][\"traceID\"]\n\n        response = requests.get(f\"http://localhost:3200/api/traces/{trace_id}\")\n        assert response.status_code == 200\n\n        trace_data = response.json()\n\n        # Verify trace structure\n        assert \"data\" in trace_data\n        assert len(trace_data[\"data\"]) &gt; 0\n\n        # Verify span attributes\n        spans = trace_data[\"data\"][0][\"spans\"]\n        assert len(spans) &gt; 0\n\n        root_span = spans[0]\n        assert \"e2e_process\" in root_span[\"operationName\"]\n\n    async def _verify_grafana_data(self):\n        \"\"\"Verify Grafana can query data from data sources\"\"\"\n\n        # Test Prometheus data source\n        response = requests.get(\n            \"http://localhost:3000/api/datasources/proxy/1/api/v1/query\",\n            headers={\"Authorization\": \"Basic YWRtaW46YWRtaW4=\"},  # admin:admin\n            params={\"query\": \"pythia_messages_processed_total\"}\n        )\n\n        assert response.status_code == 200\n\n        # Test Tempo data source\n        response = requests.get(\n            \"http://localhost:3000/api/datasources/proxy/2/api/search\",\n            headers={\"Authorization\": \"Basic YWRtaW46YWRtaW4=\"},\n            params={\"tags\": \"service.name=test-e2e-worker\"}\n        )\n\n        assert response.status_code == 200\n</code></pre>"},{"location":"user-guide/testing-observability/#performance-testing","title":"\ud83d\ude80 Performance Testing","text":""},{"location":"user-guide/testing-observability/#observability-overhead-tests","title":"Observability Overhead Tests","text":"<pre><code># tests/performance/test_observability_overhead.py\nimport pytest\nimport asyncio\nimport time\nimport statistics\nfrom unittest.mock import Mock\nfrom pythia import Worker\nfrom pythia.monitoring import setup_observability, ObservabilityMixin\n\nclass TestObservabilityPerformance:\n\n    @pytest.mark.performance\n    @pytest.mark.asyncio\n    async def test_tracing_overhead(self):\n        \"\"\"Test performance impact of distributed tracing\"\"\"\n\n        # Worker without tracing\n        class BaseWorker(Worker):\n            source = Mock()\n\n            async def process(self, message):\n                await asyncio.sleep(0.001)  # Minimal processing\n                return {\"status\": \"processed\"}\n\n        # Worker with tracing\n        class TracedWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                with self.start_span(\"process\") as span:\n                    span.set_attribute(\"test\", \"true\")\n                    await asyncio.sleep(0.001)  # Same processing\n                    return {\"status\": \"processed\"}\n\n        # Benchmark both workers\n        base_times = await self._benchmark_worker(BaseWorker(), iterations=1000)\n        traced_times = await self._benchmark_worker(TracedWorker(), iterations=1000)\n\n        # Calculate overhead\n        base_avg = statistics.mean(base_times)\n        traced_avg = statistics.mean(traced_times)\n\n        overhead_percent = ((traced_avg - base_avg) / base_avg) * 100\n\n        print(f\"Base worker average: {base_avg:.4f}s\")\n        print(f\"Traced worker average: {traced_avg:.4f}s\")\n        print(f\"Overhead: {overhead_percent:.2f}%\")\n\n        # Assert overhead is acceptable (&lt; 10%)\n        assert overhead_percent &lt; 10.0\n\n    @pytest.mark.performance\n    @pytest.mark.asyncio\n    async def test_metrics_overhead(self):\n        \"\"\"Test performance impact of metrics collection\"\"\"\n\n        class BaseWorker(Worker):\n            source = Mock()\n\n            async def process(self, message):\n                await asyncio.sleep(0.001)\n                return {\"status\": \"processed\"}\n\n        class MetricsWorker(ObservabilityMixin, Worker):\n            source = Mock()\n\n            async def process(self, message):\n                start_time = time.time()\n\n                self.increment_active_messages()\n                result = await asyncio.sleep(0.001)\n                self.decrement_active_messages()\n\n                duration = time.time() - start_time\n                self.record_processing_duration(duration)\n                self.record_message_processed(\"success\")\n\n                return {\"status\": \"processed\"}\n\n        # Benchmark both workers\n        base_times = await self._benchmark_worker(BaseWorker(), iterations=1000)\n        metrics_times = await self._benchmark_worker(MetricsWorker(), iterations=1000)\n\n        # Calculate metrics overhead\n        base_avg = statistics.mean(base_times)\n        metrics_avg = statistics.mean(metrics_times)\n\n        overhead_percent = ((metrics_avg - base_avg) / base_avg) * 100\n\n        print(f\"Metrics overhead: {overhead_percent:.2f}%\")\n\n        # Assert metrics overhead is minimal (&lt; 5%)\n        assert overhead_percent &lt; 5.0\n\n    async def _benchmark_worker(self, worker, iterations=100):\n        \"\"\"Benchmark worker processing time\"\"\"\n        times = []\n        message = Mock()\n        message.body = {\"test\": \"data\"}\n        message.message_id = \"test-123\"\n\n        # Warmup\n        for _ in range(10):\n            await worker.process(message)\n\n        # Actual benchmark\n        for _ in range(iterations):\n            start_time = time.perf_counter()\n            await worker.process(message)\n            end_time = time.perf_counter()\n            times.append(end_time - start_time)\n\n        return times\n\n    @pytest.mark.performance\n    def test_sampling_effectiveness(self):\n        \"\"\"Test that sampling reduces overhead effectively\"\"\"\n\n        # Test different sampling rates\n        sample_rates = [1.0, 0.5, 0.1, 0.01]\n        overheads = []\n\n        for rate in sample_rates:\n            # Setup observability with different sampling rates\n            setup_observability(\n                service_name=\"perf-test\",\n                tracing_sample_rate=rate,\n                metrics_port=8003\n            )\n\n            # Measure overhead (simplified test)\n            overhead = self._measure_sampling_overhead(rate)\n            overheads.append(overhead)\n\n        # Verify that lower sampling rates have lower overhead\n        assert overheads[3] &lt; overheads[2] &lt; overheads[1] &lt; overheads[0]\n\n    def _measure_sampling_overhead(self, sample_rate):\n        \"\"\"Simplified overhead measurement for sampling\"\"\"\n        # This would contain actual timing measurements\n        # For now, return mock values that demonstrate the concept\n        base_overhead = 0.1  # 10% base overhead\n        return base_overhead * sample_rate\n</code></pre>"},{"location":"user-guide/testing-observability/#contract-testing","title":"\ud83d\udcc8 Contract Testing","text":""},{"location":"user-guide/testing-observability/#opentelemetry-compliance-tests","title":"OpenTelemetry Compliance Tests","text":"<pre><code># tests/contract/test_otel_compliance.py\nimport pytest\nfrom opentelemetry import trace, metrics\nfrom opentelemetry.test.test_utils import TraceRecorder\nfrom pythia.monitoring import create_pythia_tracer, create_pythia_meter\n\nclass TestOpenTelemetryCompliance:\n\n    def test_tracer_compliance(self):\n        \"\"\"Test that Pythia tracer follows OpenTelemetry spec\"\"\"\n        tracer = create_pythia_tracer(\"test-service\")\n\n        # Verify tracer implements required interface\n        assert hasattr(tracer, 'start_span')\n        assert hasattr(tracer, 'start_as_current_span')\n\n        # Test span creation\n        with tracer.start_as_current_span(\"test-span\") as span:\n            assert span is not None\n            assert hasattr(span, 'set_attribute')\n            assert hasattr(span, 'set_status')\n            assert hasattr(span, 'record_exception')\n\n    def test_meter_compliance(self):\n        \"\"\"Test that Pythia meter follows OpenTelemetry spec\"\"\"\n        meter = create_pythia_meter(\"test-service\")\n\n        # Test metric creation\n        counter = meter.create_counter(\"test_counter\")\n        histogram = meter.create_histogram(\"test_histogram\")\n        gauge = meter.create_up_down_counter(\"test_gauge\")\n\n        # Verify instruments have required methods\n        assert hasattr(counter, 'add')\n        assert hasattr(histogram, 'record')\n        assert hasattr(gauge, 'add')\n\n    def test_resource_attributes(self, trace_recorder):\n        \"\"\"Test that resource attributes follow semantic conventions\"\"\"\n        from pythia.monitoring.observability import setup_observability\n\n        setup_observability(\n            service_name=\"test-service\",\n            service_version=\"1.0.0\",\n            environment=\"test\"\n        )\n\n        # Create a span to trigger resource creation\n        tracer = trace.get_tracer(\"test\")\n        with tracer.start_as_current_span(\"test\") as span:\n            pass\n\n        spans = trace_recorder.get_finished_spans()\n        if spans:\n            span = spans[0]\n            resource = span.resource\n\n            # Verify required resource attributes\n            assert \"service.name\" in resource.attributes\n            assert \"service.version\" in resource.attributes\n            assert \"deployment.environment\" in resource.attributes\n\n            # Verify Pythia-specific attributes\n            assert resource.attributes.get(\"pythia.framework\") == \"true\"\n</code></pre>"},{"location":"user-guide/testing-observability/#running-tests","title":"\ud83c\udfc3 Running Tests","text":""},{"location":"user-guide/testing-observability/#test-commands","title":"Test Commands","text":"<pre><code># Run all observability tests\npytest tests/ -k observability -v\n\n# Run unit tests only\npytest tests/unit/test_*observability* -v\n\n# Run integration tests (requires Docker)\npytest tests/integration/ -m integration -v\n\n# Run performance tests\npytest tests/performance/ -m performance -v --benchmark-json=benchmark.json\n\n# Run with coverage\npytest tests/ -k observability --cov=pythia.monitoring --cov-report=html\n\n# Run contract tests\npytest tests/contract/ -v\n</code></pre>"},{"location":"user-guide/testing-observability/#continuous-integration","title":"Continuous Integration","text":"<pre><code># .github/workflows/observability-tests.yml\nname: Observability Tests\n\non: [push, pull_request]\n\njobs:\n  test-observability:\n    runs-on: ubuntu-latest\n\n    services:\n      redis:\n        image: redis:7-alpine\n        ports:\n          - 6379:6379\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n\n    - name: Install dependencies\n      run: |\n        pip install -e .[test-observability]\n\n    - name: Start observability stack\n      run: |\n        docker-compose -f examples/observability/docker-compose.observability.yml up -d\n        sleep 30  # Wait for services to start\n\n    - name: Run unit tests\n      run: pytest tests/unit/ -k observability -v\n\n    - name: Run integration tests\n      run: pytest tests/integration/ -m integration -v\n\n    - name: Run performance tests\n      run: pytest tests/performance/ -m performance -v\n\n    - name: Cleanup\n      if: always()\n      run: |\n        docker-compose -f examples/observability/docker-compose.observability.yml down -v\n</code></pre>"},{"location":"user-guide/testing-observability/#best-practices-for-testing-observability","title":"\ud83d\udcda Best Practices for Testing Observability","text":""},{"location":"user-guide/testing-observability/#1-test-isolation","title":"1. Test Isolation","text":"<pre><code># Ensure each test has isolated metrics/traces\n@pytest.fixture(autouse=True)\ndef reset_global_state():\n    \"\"\"Reset OpenTelemetry global state before each test\"\"\"\n    trace._set_tracer_provider(None)\n    metrics._set_meter_provider(None)\n    yield\n    # Cleanup after test\n</code></pre>"},{"location":"user-guide/testing-observability/#2-mock-external-services","title":"2. Mock External Services","text":"<pre><code># Mock OTLP exporters to avoid real network calls in unit tests\n@pytest.fixture\ndef mock_otlp_exporter():\n    with patch('opentelemetry.exporter.otlp.proto.grpc.trace_exporter.OTLPSpanExporter') as mock:\n        yield mock\n</code></pre>"},{"location":"user-guide/testing-observability/#3-verify-business-metrics","title":"3. Verify Business Metrics","text":"<pre><code># Test that business-relevant metrics are captured\ndef test_email_processing_metrics():\n    assert \"email_sent_total\" in captured_metrics\n    assert \"email_delivery_duration_seconds\" in captured_metrics\n</code></pre>"},{"location":"user-guide/testing-observability/#4-test-error-scenarios","title":"4. Test Error Scenarios","text":"<pre><code># Verify error handling doesn't break observability\n@pytest.mark.asyncio\nasync def test_observability_during_errors():\n    # Ensure spans are properly closed even during exceptions\n    # Verify error metrics are recorded\n    # Check that traces contain error information\n</code></pre> <p>Ready to test your observability stack thoroughly! \ud83c\udfaf</p>"},{"location":"user-guide/worker-lifecycle/","title":"Worker Lifecycle","text":"<p>Understanding the complete lifecycle of Pythia workers, from initialization to shutdown.</p>"},{"location":"user-guide/worker-lifecycle/#overview","title":"Overview","text":"<p>Pythia workers follow a well-defined lifecycle with clear states and hooks for customization. This ensures reliable startup, graceful shutdown, and proper resource management.</p>"},{"location":"user-guide/worker-lifecycle/#worker-states","title":"Worker States","text":"<pre><code>from pythia.core.lifecycle import WorkerState\n\n# Available worker states\nWorkerState.INITIALIZING  # Setting up configuration and connections\nWorkerState.STARTING      # Running startup hooks\nWorkerState.RUNNING       # Processing messages\nWorkerState.STOPPING      # Graceful shutdown in progress\nWorkerState.STOPPED       # Worker has stopped\nWorkerState.ERROR         # Worker encountered an error\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#lifecycle-flow","title":"Lifecycle Flow","text":"<pre><code>graph TD\n    A[INITIALIZING] --&gt; B[STARTING]\n    B --&gt; C[RUNNING]\n    C --&gt; D[STOPPING]\n    D --&gt; E[STOPPED]\n    C --&gt; F[ERROR]\n    F --&gt; D\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#basic-worker-lifecycle","title":"Basic Worker Lifecycle","text":"<pre><code>import asyncio\nfrom pythia.core import Worker\nfrom pythia.config import WorkerConfig\n\nclass BasicWorker(Worker):\n    async def process_message(self, message):\n        \"\"\"Process a single message\"\"\"\n        return {\"status\": \"processed\", \"data\": message.body}\n\n    async def on_startup(self):\n        \"\"\"Called during worker startup\"\"\"\n        self.logger.info(\"Worker starting up...\")\n        # Initialize resources, connections, etc.\n\n    async def on_shutdown(self):\n        \"\"\"Called during graceful shutdown\"\"\"\n        self.logger.info(\"Worker shutting down...\")\n        # Cleanup resources, close connections, etc.\n\n# Run the worker\nconfig = WorkerConfig(worker_name=\"basic-worker\")\nworker = BasicWorker(config=config)\n\n# This will go through the complete lifecycle\nasyncio.run(worker.start())\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#detailed-lifecycle-hooks","title":"Detailed Lifecycle Hooks","text":""},{"location":"user-guide/worker-lifecycle/#startup-hooks","title":"Startup Hooks","text":"<pre><code>class DatabaseWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.db_pool = None\n        self.cache_client = None\n\n    async def on_startup(self):\n        \"\"\"Comprehensive startup sequence\"\"\"\n        self.logger.info(\"Starting database worker...\")\n\n        # 1. Initialize database connection pool\n        await self._setup_database()\n\n        # 2. Setup cache connections\n        await self._setup_cache()\n\n        # 3. Verify external services\n        await self._health_check()\n\n        # 4. Setup monitoring\n        await self._setup_monitoring()\n\n        self.logger.info(\"Database worker startup complete\")\n\n    async def _setup_database(self):\n        \"\"\"Initialize database connection pool\"\"\"\n        from asyncpg import create_pool\n\n        self.db_pool = await create_pool(\n            host=\"localhost\",\n            port=5432,\n            user=\"worker\",\n            database=\"production\",\n            min_size=5,\n            max_size=20\n        )\n        self.logger.info(\"Database pool initialized\")\n\n    async def _setup_cache(self):\n        \"\"\"Initialize cache client\"\"\"\n        import aioredis\n\n        self.cache_client = await aioredis.from_url(\n            \"redis://localhost:6379\",\n            encoding=\"utf-8\",\n            decode_responses=True\n        )\n        self.logger.info(\"Cache client initialized\")\n\n    async def _health_check(self):\n        \"\"\"Verify all services are healthy\"\"\"\n        # Check database\n        async with self.db_pool.acquire() as conn:\n            await conn.fetchval(\"SELECT 1\")\n\n        # Check cache\n        await self.cache_client.ping()\n\n        self.logger.info(\"Health check passed\")\n\n    async def _setup_monitoring(self):\n        \"\"\"Setup custom metrics\"\"\"\n        self.metrics.counter(\"database_queries_total\")\n        self.metrics.histogram(\"query_duration_seconds\")\n        self.logger.info(\"Monitoring setup complete\")\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#shutdown-hooks","title":"Shutdown Hooks","text":"<pre><code>class DatabaseWorker(Worker):\n    async def on_shutdown(self):\n        \"\"\"Comprehensive shutdown sequence\"\"\"\n        self.logger.info(\"Starting graceful shutdown...\")\n\n        # 1. Stop accepting new messages (handled by framework)\n        # 2. Finish processing current messages\n        await self._finish_current_work()\n\n        # 3. Close external connections\n        await self._cleanup_connections()\n\n        # 4. Flush metrics and logs\n        await self._flush_metrics()\n\n        self.logger.info(\"Graceful shutdown complete\")\n\n    async def _finish_current_work(self):\n        \"\"\"Wait for current work to complete\"\"\"\n        # Framework handles this automatically\n        # This is for any additional cleanup\n        current_tasks = len(asyncio.all_tasks())\n        if current_tasks &gt; 1:\n            self.logger.info(f\"Waiting for {current_tasks-1} tasks to complete\")\n\n    async def _cleanup_connections(self):\n        \"\"\"Close all external connections\"\"\"\n        if self.db_pool:\n            await self.db_pool.close()\n            self.logger.info(\"Database pool closed\")\n\n        if self.cache_client:\n            await self.cache_client.close()\n            self.logger.info(\"Cache client closed\")\n\n    async def _flush_metrics(self):\n        \"\"\"Flush any pending metrics\"\"\"\n        if hasattr(self, 'metrics'):\n            # Prometheus metrics are automatically handled\n            pass\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#signal-handling","title":"Signal Handling","text":"<p>Pythia workers handle system signals gracefully:</p> <pre><code>import signal\n\nclass SignalAwareWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.setup_signal_handlers()\n\n    def setup_signal_handlers(self):\n        \"\"\"Setup custom signal handlers\"\"\"\n        # SIGTERM - Graceful shutdown (default behavior)\n        signal.signal(signal.SIGTERM, self._handle_shutdown_signal)\n\n        # SIGINT - Keyboard interrupt (Ctrl+C)\n        signal.signal(signal.SIGINT, self._handle_shutdown_signal)\n\n        # SIGUSR1 - Custom signal for reloading config\n        signal.signal(signal.SIGUSR1, self._handle_reload_signal)\n\n    def _handle_shutdown_signal(self, signum, frame):\n        \"\"\"Handle shutdown signals\"\"\"\n        self.logger.info(f\"Received signal {signum}, initiating shutdown...\")\n        # Framework will handle the actual shutdown\n        asyncio.create_task(self.stop())\n\n    def _handle_reload_signal(self, signum, frame):\n        \"\"\"Handle reload signals\"\"\"\n        self.logger.info(\"Received reload signal, reloading configuration...\")\n        asyncio.create_task(self._reload_config())\n\n    async def _reload_config(self):\n        \"\"\"Reload configuration without restart\"\"\"\n        # Implement hot config reloading\n        new_config = WorkerConfig()\n        if new_config != self.config:\n            self.logger.info(\"Configuration changed, applying updates...\")\n            self.config = new_config\n            # Apply configuration changes\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#health-checks","title":"Health Checks","text":"<pre><code>class HealthAwareWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.healthy = True\n        self.last_health_check = None\n\n    async def on_startup(self):\n        \"\"\"Setup health checking\"\"\"\n        await super().on_startup()\n\n        # Start periodic health checks\n        asyncio.create_task(self._periodic_health_check())\n\n    async def health_check(self) -&gt; bool:\n        \"\"\"Check if worker is healthy\"\"\"\n        try:\n            # Check database connectivity\n            if hasattr(self, 'db_pool') and self.db_pool:\n                async with self.db_pool.acquire() as conn:\n                    await conn.fetchval(\"SELECT 1\")\n\n            # Check message broker connectivity\n            # This is handled by the framework\n\n            # Check external services\n            await self._check_external_services()\n\n            self.healthy = True\n            self.last_health_check = datetime.now()\n            return True\n\n        except Exception as e:\n            self.logger.error(f\"Health check failed: {e}\")\n            self.healthy = False\n            return False\n\n    async def _check_external_services(self):\n        \"\"\"Check external service dependencies\"\"\"\n        # Example: Check API endpoint\n        async with aiohttp.ClientSession() as session:\n            async with session.get(\"https://api.example.com/health\") as resp:\n                if resp.status != 200:\n                    raise Exception(f\"API health check failed: {resp.status}\")\n\n    async def _periodic_health_check(self):\n        \"\"\"Run health checks periodically\"\"\"\n        while self.state == WorkerState.RUNNING:\n            await self.health_check()\n            await asyncio.sleep(self.config.health_check_interval)\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#error-recovery","title":"Error Recovery","text":"<pre><code>class ResilientWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.error_count = 0\n        self.last_error = None\n\n    async def process_message(self, message):\n        \"\"\"Process message with error recovery\"\"\"\n        try:\n            result = await self._process_business_logic(message)\n\n            # Reset error count on success\n            if self.error_count &gt; 0:\n                self.logger.info(\"Error recovery successful\")\n                self.error_count = 0\n\n            return result\n\n        except Exception as e:\n            self.error_count += 1\n            self.last_error = e\n\n            await self._handle_processing_error(e)\n            raise\n\n    async def _handle_processing_error(self, error: Exception):\n        \"\"\"Handle processing errors with recovery strategies\"\"\"\n        self.logger.error(f\"Processing error #{self.error_count}: {error}\")\n\n        # Circuit breaker pattern\n        if self.error_count &gt;= 5:\n            self.logger.warning(\"Too many errors, entering recovery mode\")\n            await self._enter_recovery_mode()\n\n    async def _enter_recovery_mode(self):\n        \"\"\"Enter recovery mode to prevent cascade failures\"\"\"\n        self.logger.info(\"Entering recovery mode...\")\n\n        # Pause processing for a short time\n        await asyncio.sleep(10)\n\n        # Run health checks\n        if await self.health_check():\n            self.logger.info(\"Health check passed, resuming normal operation\")\n            self.error_count = 0\n        else:\n            self.logger.error(\"Health check failed, extending recovery period\")\n            await asyncio.sleep(30)\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#monitoring-worker-state","title":"Monitoring Worker State","text":"<pre><code>class MonitoredWorker(Worker):\n    async def on_startup(self):\n        \"\"\"Setup state monitoring\"\"\"\n        await super().on_startup()\n\n        # Record startup metrics\n        self.metrics.counter(\"worker_startups_total\").inc()\n        self.metrics.gauge(\"worker_startup_timestamp\").set(time.time())\n\n        # Start state monitoring\n        asyncio.create_task(self._monitor_state())\n\n    async def _monitor_state(self):\n        \"\"\"Monitor and report worker state\"\"\"\n        while True:\n            # Update state metrics\n            self.metrics.gauge(\"worker_state\").set(self.state.value)\n            self.metrics.gauge(\"worker_uptime_seconds\").set(\n                (datetime.now() - self._startup_time).total_seconds()\n            )\n\n            # Log periodic status\n            if self.state == WorkerState.RUNNING:\n                processed = self.metrics.counter(\"messages_processed_total\")._value\n                self.logger.info(f\"Worker healthy, processed {processed} messages\")\n\n            await asyncio.sleep(60)  # Report every minute\n\n    async def on_shutdown(self):\n        \"\"\"Record shutdown metrics\"\"\"\n        uptime = (datetime.now() - self._startup_time).total_seconds()\n\n        self.metrics.histogram(\"worker_uptime_seconds\").observe(uptime)\n        self.metrics.counter(\"worker_shutdowns_total\").inc()\n\n        await super().on_shutdown()\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#advanced-lifecycle-management","title":"Advanced Lifecycle Management","text":""},{"location":"user-guide/worker-lifecycle/#custom-lifecycle-manager","title":"Custom Lifecycle Manager","text":"<pre><code>from pythia.core.lifecycle import LifecycleManager, WorkerState\n\nclass CustomLifecycleManager(LifecycleManager):\n    \"\"\"Custom lifecycle manager with additional features\"\"\"\n\n    async def start_worker(self, worker):\n        \"\"\"Custom startup sequence\"\"\"\n        self.logger.info(\"Starting custom worker lifecycle...\")\n\n        # Pre-startup phase\n        await self._pre_startup_phase(worker)\n\n        # Standard startup\n        await super().start_worker(worker)\n\n        # Post-startup phase\n        await self._post_startup_phase(worker)\n\n    async def _pre_startup_phase(self, worker):\n        \"\"\"Pre-startup custom logic\"\"\"\n        # Validate environment\n        await self._validate_environment()\n\n        # Setup monitoring\n        await self._setup_advanced_monitoring(worker)\n\n    async def _post_startup_phase(self, worker):\n        \"\"\"Post-startup custom logic\"\"\"\n        # Register with service discovery\n        await self._register_with_discovery()\n\n        # Start background tasks\n        await self._start_background_tasks(worker)\n\n# Use custom lifecycle manager\nclass AdvancedWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.lifecycle_manager = CustomLifecycleManager(config)\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/worker-lifecycle/#1-resource-management","title":"1. Resource Management","text":"<pre><code>class ResourceManagedWorker(Worker):\n    async def on_startup(self):\n        \"\"\"Proper resource initialization\"\"\"\n        try:\n            # Initialize resources in order\n            await self._init_database()\n            await self._init_cache()\n            await self._init_external_clients()\n\n        except Exception as e:\n            # Cleanup on startup failure\n            await self.on_shutdown()\n            raise\n\n    async def on_shutdown(self):\n        \"\"\"Proper resource cleanup\"\"\"\n        # Cleanup in reverse order\n        if hasattr(self, 'external_clients'):\n            await self._cleanup_external_clients()\n\n        if hasattr(self, 'cache'):\n            await self._cleanup_cache()\n\n        if hasattr(self, 'db'):\n            await self._cleanup_database()\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#2-graceful-degradation","title":"2. Graceful Degradation","text":"<pre><code>class GracefulWorker(Worker):\n    def __init__(self, config: WorkerConfig):\n        super().__init__(config)\n        self.degraded_mode = False\n\n    async def process_message(self, message):\n        \"\"\"Process with graceful degradation\"\"\"\n        if self.degraded_mode:\n            return await self._process_degraded(message)\n\n        try:\n            return await self._process_normal(message)\n        except ExternalServiceError:\n            # Enter degraded mode\n            self.degraded_mode = True\n            self.logger.warning(\"Entering degraded mode\")\n            return await self._process_degraded(message)\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#3-configuration-reload","title":"3. Configuration Reload","text":"<pre><code>class ReloadableWorker(Worker):\n    async def reload_config(self):\n        \"\"\"Reload configuration without restart\"\"\"\n        old_config = self.config\n        new_config = WorkerConfig()\n\n        if new_config != old_config:\n            self.logger.info(\"Configuration changed, reloading...\")\n\n            # Apply new configuration\n            self.config = new_config\n\n            # Reinitialize components if needed\n            await self._apply_config_changes(old_config, new_config)\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#testing-worker-lifecycle","title":"Testing Worker Lifecycle","text":"<pre><code>import pytest\nfrom pythia.utils.testing import WorkerTestCase\n\nclass TestWorkerLifecycle(WorkerTestCase):\n    async def test_startup_sequence(self):\n        \"\"\"Test worker startup\"\"\"\n        # Worker should start successfully\n        await self.worker.start()\n        assert self.worker.state == WorkerState.RUNNING\n\n    async def test_shutdown_sequence(self):\n        \"\"\"Test graceful shutdown\"\"\"\n        await self.worker.start()\n        await self.worker.stop()\n        assert self.worker.state == WorkerState.STOPPED\n\n    async def test_error_recovery(self):\n        \"\"\"Test error handling and recovery\"\"\"\n        # Simulate error condition\n        with pytest.raises(Exception):\n            await self.worker._handle_error(Exception(\"Test error\"))\n\n        # Worker should recover\n        assert self.worker.state != WorkerState.ERROR\n</code></pre>"},{"location":"user-guide/worker-lifecycle/#next-steps","title":"Next Steps","text":"<ul> <li>Message Handling - Deep dive into message processing</li> <li>Configuration - Complete configuration guide</li> <li>Error Handling - Advanced error handling patterns</li> </ul>"}]}